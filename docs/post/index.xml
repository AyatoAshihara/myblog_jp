<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>https://ayatoashihara.github.io/myblog_jp/post/</link>
      <atom:link href="https://ayatoashihara.github.io/myblog_jp/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>ja</language><lastBuildDate>Sun, 26 May 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ayatoashihara.github.io/myblog_jp/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/</link>
    </image>
    
    <item>
      <title>GPLVMでマルチファクターモデルを構築してみた</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post8/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post8/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;おはこんばんにちは。
ずいぶん前にGianonne et al (2008)のマルチファクターモデルで四半期GDPの予想を行いました。
結果としては、ある程度は予測精度が出ていたものの彼らの論文ほどは満足のいくものではありませんでした。原因としてはクロスセクショナルなデータ不足が大きいと思われ、現在収集方法についてもEXCELを用いて改修中です。しかし一方で、マルチファクターモデルの改善も考えたいと思っています。前回は月次経済統計を主成分分析（実際にはカルマンフィルタ）を用いて次元削減を行い、主成分得点を説明変数としてGDPに回帰しました。今回はこの主成分分析のド発展版である&lt;code&gt;Gaussian Process Latent Variable Model&lt;/code&gt;(GPLVM)を用いてファクターを計算し、それをGDPに回帰したいと思います。&lt;/p&gt;
&lt;div id=&#34;gplvmとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. GPLVMとは&lt;/h2&gt;
&lt;p&gt;GPLVMとは、&lt;code&gt;Gaussian Process&lt;/code&gt; Modelの一種です。以前、&lt;code&gt;Gaussian Process Regression&lt;/code&gt;の記事を書きました。&lt;/p&gt;
&lt;p&gt;最も基本的な&lt;code&gt;Gaussian Process&lt;/code&gt; Modelは上の記事のようなモデルで、非説明変数&lt;span class=&#34;math inline&#34;&gt;\(Y=(y_{1},y_{2},...,y_{n})\)&lt;/span&gt;と説明変数&lt;span class=&#34;math inline&#34;&gt;\(X=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{n})\)&lt;/span&gt;があり、以下のような関係式で表される際にそのモデルを直接推定することなしに新たな説明変数&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;の入力に対し、非説明変数&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;の予測値をはじき出すというものでした。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(\textbf{x}_{i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の説明変数ベクトル、&lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt;は非線形関数、 &lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}^{T}\)&lt;/span&gt;は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\)&lt;/span&gt;を想定しています（&lt;span class=&#34;math inline&#34;&gt;\(x_{1,i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の入力データ&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{i}\)&lt;/span&gt;の１番目の変数）。詳しくは過去記事を参照してください。&lt;/p&gt;
&lt;p&gt;今回やるGPLVMは説明変数ベクトルが観測できない潜在変数（Latent Variable）であるところが特徴です。以下のスライドが非常にわかりやすいですが、GP-LVMは確率的主成分分析（PPCA）の非線形版という位置付けになっています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.slideshare.net/antiplastics/pcagplvm&#34;&gt;資料&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では具体的な説明に移ります。GPLVMは主成分分析の発展版ですので、主に次元削減のために行われることを想定しています。つまり、データセットがあったとして、サンプルサイズ&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;よりも変数の次元&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;が大きいような場合を想定しています。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;最もprimitiveなgp-lvm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. &lt;a href=&#34;http://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf&#34;&gt;最もPrimitiveなGP-LVM&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;先述したようにGPLVMはPPCAの非線形版です。なので、GPLVMを説明するスタートはPPCAになります。観測可能な&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;次元データセットを&lt;span class=&#34;math inline&#34;&gt;\(\{\textbf{y}_{n}\}_{n=1}^{N}\)&lt;/span&gt;とします。そして、潜在変数を&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;とおきます。今、データセットと潜在変数の間には以下のような関係があるとします。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{y}_{n} = \textbf{W}\textbf{x}_{n} + \epsilon_{n}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;はウェイト行列、&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{n}\)&lt;/span&gt;はかく乱項で&lt;span class=&#34;math inline&#34;&gt;\(N(0,\beta^{-1}\textbf{I})\)&lt;/span&gt;に従います（被説明変数が多次元になることに注意）。また、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N(0,\textbf{I})\)&lt;/span&gt;に従います。このとき、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{y}_{n}\)&lt;/span&gt;の尤度を&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;を周辺化することで表現すると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\displaystyle p(\textbf{y}_{n}|\textbf{W},\beta) &amp;amp;=&amp;amp; \int p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)N(0,\textbf{I})d\textbf{x}_{n} \\
\displaystyle &amp;amp;=&amp;amp; \int N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})N(0,\textbf{I})d\textbf{x}_{n} \\
&amp;amp;=&amp;amp; N(0,\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}) \\
\displaystyle &amp;amp;=&amp;amp; \frac{1}{(2\pi)^{DN/2}|\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}|^{N/2}}\exp(\frac{1}{2}\textbf{tr}( (\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{YY}^{T}))
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)=N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})\)&lt;/span&gt;です。平均と分散は以下から求めました。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
E(\textbf{y}_{n}|\textbf{W},\beta) &amp;amp;=&amp;amp; E(\textbf{W}\textbf{x}_{n} + \epsilon_{n}) \\
&amp;amp;=&amp;amp; E(\textbf{W}\textbf{x}_{n}) + E(\epsilon_{n}) \\
&amp;amp;=&amp;amp; \textbf{W}E(\textbf{x}_{n}) + E(\epsilon_{n}) = 0
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
E[(\textbf{y}_{n}|\textbf{W},\beta)(\textbf{y}_{n}|\textbf{W},\beta)^{T}] &amp;amp;=&amp;amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)(\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)^{T} ] \\
&amp;amp;=&amp;amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n})(\textbf{W}\textbf{x}_{n} + \epsilon_{n})^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \textbf{W}\textbf{x}_{n}\epsilon_{n}^{T} + \epsilon_{n}\textbf{W}\textbf{x}_{n}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T}] + E[\epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; \textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を求めるためには&lt;span class=&#34;math inline&#34;&gt;\(\textbf{y}_{n}\)&lt;/span&gt;がi.i.d.と仮定し、以下のようなデータセット全体の尤度を最大化すれば良いことになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{Y}|\textbf{W},\beta) = \prod_{n=1}^{N}p(\textbf{y}_{n}|\textbf{W},\beta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N×D\)&lt;/span&gt;の計画行列です。このように、PPCAでは&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;を周辺化し、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を最適化します。逆に、Lawrence(2004)では&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を周辺化し、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;します（理由は後述）。&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を周辺化するために、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;に事前分布を与えましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{W}) = \prod_{i=1}^{D}N(\textbf{w}_{i}|0,\alpha^{-1}\textbf{I})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}_{i}\)&lt;/span&gt;はウェイト行列&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;の&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の列です。では、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を周辺化して&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;の尤度関数を導出してみます。やり方はさっきとほぼ同じなので省略します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{Y}|\textbf{X},\beta) = \frac{1}{(2\pi)^{DN/2}|K|^{D/2}}\exp(\frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T}))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}=\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{Y}|\textbf{X},\beta)\)&lt;/span&gt;の分散共分散行列で、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{N})^{T}\)&lt;/span&gt;は入力ベクトルです。対数尤度は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle L = - \frac{DN}{2}\ln{2\pi} - \frac{1}{2}\ln{|\textbf{K}|} - \frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;周辺化のおかげでウェイト&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;が消えたのでこれを&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;で微分してみましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle\frac{\partial L}{ \partial \textbf{X}} = \alpha^2 \textbf{K}^{-1}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} - \alpha^2 D\textbf{K}^{-1}\textbf{X}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここから、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} = \textbf{X}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、特異値分解を用いると&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{X} = \textbf{ULV}^{T}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(\textbf{U} = (\textbf{u}_{1},\textbf{u}_{2},...,\textbf{u}_{q})\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N×q\)&lt;/span&gt;直交行列、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{L} = diag(l_{1},l_{2},..., l_{q})\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(q×q\)&lt;/span&gt;の特異値を対角成分に並べた行列、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{V}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(q×q\)&lt;/span&gt;直交行列です。これを先ほどの式に代入すると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\textbf{K}^{-1}\textbf{X} &amp;amp;=&amp;amp; (\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{X} \\
&amp;amp;=&amp;amp; \textbf{X}(\alpha^2\textbf{X}^{T}\textbf{X} + \beta^{-1}\textbf{I})^{-1} \\
&amp;amp;=&amp;amp; \textbf{ULV}^{T}(\alpha^2\textbf{VLU}^{T}\textbf{ULV}^{T} + \beta^{-1}\textbf{I})^{-1} \\
&amp;amp;=&amp;amp; \textbf{ULV}^{T}\textbf{V}(\alpha^2\textbf{LU}^{T}\textbf{UL} + \beta^{-1}\textbf{I}^{-1})\textbf{V}^{T} \\
&amp;amp;=&amp;amp; \textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{V}^{T}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1})\textbf{V}^{T} &amp;amp;=&amp;amp; \textbf{ULV}^{T}\\
\displaystyle \textbf{Y}\textbf{Y}^{T}\textbf{UL} &amp;amp;=&amp;amp; D\textbf{U}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{L} \\
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(l_{j}\)&lt;/span&gt;が0でなければ、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\textbf{u}_{j} = D(\alpha^2 l_{j}^{2} + \beta^{-1})\textbf{u}_{j}\)&lt;/span&gt;となり、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{U}\)&lt;/span&gt;のそれぞれの列は&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有ベクトルであり、対応する固有値&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(D(\alpha^2 l_{j}^{2} + \beta^{-1})\)&lt;/span&gt;となります。つまり、未知であった&lt;span class=&#34;math inline&#34;&gt;\(X=ULV\)&lt;/span&gt;が実は&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有値問題から求めることが出来るというわけです。&lt;span class=&#34;math inline&#34;&gt;\(l_{j}\)&lt;/span&gt;は上式を利用して、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle l_{j} = (\frac{\lambda_{j}}{D\alpha^2} - \frac{1}{\beta\alpha^2})^{1/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有値&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{j}\)&lt;/span&gt;とパラメータから求められることがわかります。よって、&lt;span class=&#34;math inline&#34;&gt;\(X=ULV\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{X} = \textbf{U}_{q}\textbf{L}\textbf{V}^{T}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{U}_{q}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有ベクトルを&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;個取り出したものです。&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が限りなく大きければ（=観測誤差が限りなく小さければ）通常のPCAと一致します。&lt;/p&gt;
&lt;p&gt;以上がPPCAです。GPLVMはPPCAで確率モデルとして想定していた以下のモデルを拡張します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{y}_{n} = \textbf{W}^{T}\textbf{x}_{n} + \epsilon_{n}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;具体的には、通常のガウス過程と同様、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \textbf{y}_{n}  = \textbf{W}^{T}\phi(\textbf{x}_{n})+ \epsilon_{n}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;という風に基底関数&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_{n})\)&lt;/span&gt;をかませて拡張します。&lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt;は平均&lt;span class=&#34;math inline&#34;&gt;\(\textbf{0}\)&lt;/span&gt;、分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{\textbf{x}}\)&lt;/span&gt;のガウス過程と仮定します。分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{\textbf{x}}\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{K}_{\textbf{x}} = \alpha^2\phi(\textbf{x})\phi(\textbf{x})^T
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;であり、入力ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{・})\)&lt;/span&gt;で非線形変換した特徴量&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x})\)&lt;/span&gt;が近いほど、出力値&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;も近くなりやすいという性質があることになります。GPLVMではこの性質を逆に利用しています。つまり、出力値&lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(Y_j\)&lt;/span&gt;が近い→&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_i)\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_j)\)&lt;/span&gt;が近い（内積が大きい）→&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{x,ij}\)&lt;/span&gt;が大きい→観測不可能なデータ&lt;span class=&#34;math inline&#34;&gt;\(X_{i}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(X_{j}\)&lt;/span&gt;は近い値（or同じようなパターン）をとる。
この議論からもわかるように、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{\textbf{x}}\)&lt;/span&gt;は入力ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;それぞれの距離を表したものになります。分散共分散行列の計算には入力ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;を基底関数&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{・})\)&lt;/span&gt;で非線形変換した後、内積を求めるといったことをする必要はなく、カーネル関数を計算するのみでOKです。今回は王道中の王道RBFカーネルを使用していますので、これを例説明します。&lt;/p&gt;
&lt;p&gt;RBFカーネル（スカラーに対する）
&lt;span class=&#34;math display&#34;&gt;\[
\theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x^T)^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;このRBFカーネルは以下の基底関数と対応しています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\phi(x)_h = \tau\exp(-\frac{1}{r}(x-h)^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;例えば、この基底関数で入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;を変換したものを&lt;span class=&#34;math inline&#34;&gt;\(2H^2+1\)&lt;/span&gt;個並べた関数を&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = (\phi(x)_{-H^2}, ..., \phi(x)_{0},...,\phi(x)_{H^2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;の特徴量だとすると&lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;との共分散&lt;span class=&#34;math inline&#34;&gt;\(K_{x}(x,x&amp;#39;)\)&lt;/span&gt;は内積の和なので&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
K_{x}(x,x&amp;#39;) = \sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&amp;#39;)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(H \to \infty\)&lt;/span&gt;とし、グリッドを極限まで細かくしてみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
K_{x}(x,x&amp;#39;) &amp;amp;=&amp;amp; \lim_{H \to \infty}\sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&amp;#39;) \\
&amp;amp;\to&amp;amp;\int_{-\infty}^{\infty}\tau\exp(-\frac{1}{r}(x-h)^2)\tau\exp(-\frac{1}{r}(x&amp;#39;-h)^2)dh \\
&amp;amp;=&amp;amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{(x-h)^2+(x&amp;#39;-h)^2\})dh \\
&amp;amp;=&amp;amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{2(h-\frac{x+x&amp;#39;}{2})^2+\frac{1}{2}(x-x&amp;#39;)^2\})dh \\
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;に関係のない部分を積分の外に出します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
&amp;amp;=&amp;amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&amp;#39;}{2})^2)dh\exp(-\frac{1}{2r}(x-x&amp;#39;)^2) \\
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;残った積分を見ると、正規分布の正規化定数と等しいことがわかります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\int_{-\infty}^{\infty}\exp(-\frac{1}{2\sigma}(h-\frac{x+x&amp;#39;}{2})^2)dh &amp;amp;=&amp;amp;  \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&amp;#39;}{2})^2)dh\\
\sigma &amp;amp;=&amp;amp; \frac{r}{4}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となるので、ガウス積分の公式を用いて&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
&amp;amp;=&amp;amp; \tau^2 \sqrt{\frac{\pi r}{2}}\exp(-\frac{1}{2r}(x-x&amp;#39;)^2)　\\
&amp;amp;=&amp;amp; \theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x’)^2)
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となり、RBFカーネルと等しくなることがわかります。よって、RBFカーネルで計算した共分散は上述した基底関数で入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;を無限次元へ拡張した特徴量ベクトルの内積から計算した共分散と同値になることがわかります。つまり、入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;のスカラーの計算のみで&lt;span class=&#34;math inline&#34;&gt;\(K_{x}(x,x&amp;#39;)\)&lt;/span&gt;ができてしまうという夢のような計算効率化が可能になるわけです。無限次元特徴量ベクトルの回帰問題なんて普通計算できませんからね。。。カーネル関数は偉大です。
前の記事にも載せましたが、RBFカーネルで分散共分散行列を計算したガウス過程のサンプルパスは以下通りです（&lt;span class=&#34;math inline&#34;&gt;\(\theta_1=1,\theta_2=0.5\)&lt;/span&gt;）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define Kernel function
Kernel_Mat &amp;lt;- function(X,sigma,beta){
  N &amp;lt;- NROW(X)
  K &amp;lt;- matrix(0,N,N)
  for (i in 1:N) {
    for (k in 1:N) {
      if(i==k) kdelta = 1 else kdelta = 0
      K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta
    }
  }
  return(K)
}

N &amp;lt;- 10 # max value of X
M &amp;lt;- 1000 # sample size
X &amp;lt;- matrix(seq(1,N,length=M),M,1) # create X
testK &amp;lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix

library(MASS)

P &amp;lt;- 6 # num of sample path
Y &amp;lt;- matrix(0,M,P) # define Y

for(i in 1:P){
  Y[,i] &amp;lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y
}

# Plot
matplot(x=X,y=Y,type = &amp;quot;l&amp;quot;,lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;非常に滑らかな関数となっていることがわかります。RBFのほかにもカーネル関数は存在します。カーネル関数を変えると基底関数が変わりますから、サンプルパスは大きく変わることになります。&lt;/p&gt;
&lt;p&gt;GPLVMの推定方法に話を進めましょう。PPCAの時と同じく、以下の尤度関数を最大化する観測不能な入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;を推定値とします。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle L = - \frac{DN}{2}\ln{2\pi} - \frac{1}{2}\ln{|\textbf{K}|} - \frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ただ、PPCAとは異なり、その値は解析的に求めることができません。尤度関数の導関数は今や複雑な関数であり、展開することができないからです。よって、共役勾配法を用いて数値的に計算するのが主流なようです（自分は準ニュートン法で実装）。解析的、数値的のどちらにせよ導関数を求めておくことは必要なので、導関数を求めてみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial \textbf{K}_x} = \frac{1}{2}(\textbf{K}_x^{-1}\textbf{Y}\textbf{Y}^T\textbf{K}_x^{-1}-D\textbf{K}_x^{-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なので、チェーンルールから&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial \textbf{x}} = \frac{\partial L}{\partial \textbf{K}_x}\frac{\partial \textbf{K}_x}{\partial \textbf{x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なので、カーネル関数を決め、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}\)&lt;/span&gt;に初期値を与えてやれば勾配法によって尤度&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;が最大となる点を探索することができます。今回使用するRBFカーネルで&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial \textbf{K}_x}{\partial x_{nj}}\)&lt;/span&gt;を計算してみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\frac{\partial \textbf{K}_x(\textbf{x}_n,\textbf{x}_n&amp;#39;)}{x_{nj}}&amp;amp;=&amp;amp;\frac{\partial\theta_{1}\exp(-\frac{|\textbf{x}_n-\textbf{x}_n&amp;#39;|^2}{\theta_{2}})}{\partial x_{nk}} \\
&amp;amp;=&amp;amp; \frac{\partial\theta_{1}\exp(-\frac{(\textbf{x}_n-\textbf{x}_n&amp;#39;)^T(\textbf{x}_n-\textbf{x}_n&amp;#39;)}{\theta_{2}})}{\partial x_{nk}} \\
&amp;amp;=&amp;amp; \frac{\partial\theta_{1}\exp(-\frac{-(\textbf{x}_n^T\textbf{x}_n-2\textbf{x}_n&amp;#39;^T\textbf{x}_n+\textbf{x}_n&amp;#39;^T\textbf{x}_n&amp;#39;)}{\theta_{2}})}{\partial x_{nk}} \\
&amp;amp;=&amp;amp; -2\textbf{K}_x(\textbf{x}_n\textbf{x}_n&amp;#39;)\frac{(x_{nj}-x_{n&amp;#39;j})}{\theta_2}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;番目の潜在変数の&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;番目のサンプルそれぞれに導関数を計算し、それを分散共分散行列と同じ行列に整理したものと&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial L}{\partial \textbf{K}_x}\)&lt;/span&gt;との要素ごとの積を足し合わせたものが勾配となります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rでの実装&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Rでの実装&lt;/h2&gt;
&lt;p&gt;GPLVMを&lt;code&gt;R&lt;/code&gt;で実装します。使用するデータは以前giannoneの記事で使用したものと同じものです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ESTIMATE_GPLVM &amp;lt;- function(Y,P,sigma){
  # 1. Set initial value
  Y &amp;lt;- as.matrix(Y)
  eigenvector &amp;lt;- eigen(cov(Y))$vectors
  X &amp;lt;- Y%*%eigenvector[,1:P] # initial value
  N &amp;lt;- NROW(Y) # Sample Size
  D &amp;lt;- NCOL(Y) # Dimention of dataset
  X0 &amp;lt;- c(as.vector(X))
  sigma &amp;lt;- var(matrix(Y,dim(Y)[1]*dim(Y)[2],1))
  
  # 2. Define log likelihood function
  loglik &amp;lt;- function(X0,Y,N,P,D,beta,sigma){
    X &amp;lt;- matrix(X0,N,P)
    K &amp;lt;- matrix(0,N,N)
    scale &amp;lt;- diag(sqrt(3/((apply(X, 2, max) -apply(X, 2, min))^2)))
    X &amp;lt;- X%*%scale
    for (i in 1:N) {
      for (k in 1:N) {
         if(i==k) kdelta = 1 else kdelta = 0
         K[i,k] &amp;lt;- K[k,i] &amp;lt;- sigma*exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)
       }
     }
 
    L &amp;lt;- - D*N/2*log(2*pi) - D/2*log(det(K)) - 1/2*sum(diag(ginv(K)%*%Y%*%t(Y))) #loglikelihood

    return(L)
  }
  
  # 3. Define derivatives of log likelihood function
  dloglik &amp;lt;- function(X0,P,D,N,Y,beta,sigma){
    X &amp;lt;- matrix(X0,N,P)
    K &amp;lt;- matrix(0,N,N)
    for (i in 1:N) {
      for (k in 1:N) {
        if(i==k) kdelta = 1 else kdelta = 0
        K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)
      }
    }
    invK &amp;lt;- ginv(K)
    dLdK &amp;lt;- invK%*%Y%*%t(Y)%*%invK - D*invK
    dLdx &amp;lt;- matrix(0,N,P)
    
    for (j in 1:P){
      for(i in 1:N){
        dKdx &amp;lt;- matrix(0,N,N)
        for (k in 1:N){
          dKdx[i,k] &amp;lt;- dKdx[k,i] &amp;lt;- -exp(-(t(X[i,]-X[k,])%*%(X[i,]-X[k,]))*0.5)*((X[i,j]-X[k,j])*0.5)
        }
        dLdx[i,j] &amp;lt;- sum(dLdK*dKdx)
      }
    }
    
    return(dLdx)
  }
  
  # 4. Optimization
  res &amp;lt;- optim(X0, loglik, dloglik, Y = Y, N=N, P=P, D=D, beta = exp(2), sigma = sigma,
               method = &amp;quot;BFGS&amp;quot;, control = list(fnscale = -1,trace=1000,maxit=10000))
  output &amp;lt;- matrix(res$par,N,P)
  result &amp;lt;- list(output,res,P)
  names(result) &amp;lt;- c(&amp;quot;output&amp;quot;,&amp;quot;res&amp;quot;,&amp;quot;P&amp;quot;)
  return(result)
}

GPLVM_SELECT &amp;lt;- function(Y){
  D &amp;lt;- NCOL(Y)
  library(stringr)
  for (i in 1:D){
    if (i == 1){
      result &amp;lt;- ESTIMATE_GPLVM(Y,i)
      P &amp;lt;- 2
      print(str_c(&amp;quot;STEP&amp;quot;, i, &amp;quot; loglikelihood &amp;quot;, as.numeric(result$res$value)))
    }else{
      temp &amp;lt;- ESTIMATE_GPLVM(Y,i)
      print(str_c(&amp;quot;STEP&amp;quot;, i, &amp;quot; loglikelihood &amp;quot;, as.numeric(temp$res$value)))
      if (result$res$value &amp;lt; temp$res$value){
        result &amp;lt;- temp
        P &amp;lt;- i
      }
    }
  }
  print(str_c(&amp;quot;The optimal number of X is &amp;quot;, P))
  print(str_c(&amp;quot;loglikelihood &amp;quot;, as.numeric(result$res$value)))
  return(result)
}

result &amp;lt;- ESTIMATE_GPLVM(scale(Y),5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## initial  value 613.864608 
## final  value 609.080329 
## converged&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

ggplot(gather(as.data.frame(result$output),key = name,value = value),
       aes(x=rep(dataset1$publication,5),y=value,colour=name)) + 
  geom_line(size=1) +
  xlab(&amp;quot;Date&amp;quot;) +
  ggtitle(&amp;quot;5 economic factors&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(xts)
X.xts &amp;lt;- xts(result$output,order.by = dataset1$publication)
X.q.xts &amp;lt;- apply.quarterly(X.xts,mean)
X.3m.xts &amp;lt;- X.xts[endpoints(X.xts,on=&amp;quot;quarters&amp;quot;),]

if (months(index(X.q.xts)[NROW(X.q.xts)]) %in% c(&amp;quot;3月&amp;quot;,&amp;quot;6月&amp;quot;,&amp;quot;9月&amp;quot;,&amp;quot;12月&amp;quot;)){
} else X.q.xts &amp;lt;- X.q.xts[-NROW(X.q.xts),]
if (months(index(X.3m.xts)[NROW(X.3m.xts)]) %in% c(&amp;quot;3月&amp;quot;,&amp;quot;6月&amp;quot;,&amp;quot;9月&amp;quot;,&amp;quot;12月&amp;quot;)){
} else X.3m.xts &amp;lt;- X.3m.xts[-NROW(X.3m.xts),]

colnames(X.xts) &amp;lt;- c(&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;,&amp;quot;factor4&amp;quot;,&amp;quot;factor5&amp;quot;) 

GDP$publication &amp;lt;- GDP$publication + months(2)
GDP.q &amp;lt;- GDP[GDP$publication&amp;gt;=index(X.q.xts)[1] &amp;amp; GDP$publication&amp;lt;=index(X.q.xts)[NROW(X.q.xts)],]

rg &amp;lt;- lm(scale(GDP.q$GDP)~X.q.xts[-54])
rg2 &amp;lt;- lm(scale(GDP.q$GDP)~X.3m.xts[-54])

summary(rg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.q.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.31774 -0.06927 -0.03224  0.06690  0.31062 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)      0.0445000  0.0317777   1.400    0.177    
## X.q.xts[-54]X.1 -0.3181108  0.0106335 -29.916  &amp;lt; 2e-16 ***
## X.q.xts[-54]X.2  0.0004621  0.0175913   0.026    0.979    
## X.q.xts[-54]X.3  0.1737612  0.0249186   6.973 9.09e-07 ***
## X.q.xts[-54]X.4 -0.0060035  0.0376324  -0.160    0.875    
## X.q.xts[-54]X.5  0.0646009  0.0430678   1.500    0.149    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1617 on 20 degrees of freedom
## Multiple R-squared:  0.9791, Adjusted R-squared:  0.9739 
## F-statistic: 187.3 on 5 and 20 DF,  p-value: 4.402e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(rg2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.3m.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35613 -0.11430  0.00258  0.15171  0.36506 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    -0.003798   0.041789  -0.091    0.928    
## X.3m.xts[-54]1 -0.312411   0.014008 -22.303 1.34e-15 ***
## X.3m.xts[-54]2  0.022873   0.025672   0.891    0.384    
## X.3m.xts[-54]3  0.152350   0.031080   4.902 8.62e-05 ***
## X.3m.xts[-54]4 -0.019237   0.047809  -0.402    0.692    
## X.3m.xts[-54]5 -0.032419   0.055662  -0.582    0.567    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.21 on 20 degrees of freedom
## Multiple R-squared:  0.9647, Adjusted R-squared:  0.9559 
## F-statistic: 109.3 on 5 and 20 DF,  p-value: 8.102e-14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;決定係数が大幅に改善しました。
3ヶ月の平均をとったファクターの方がパフォーマンスが良さそうなので、こちらで実際のGDPと予測値のプロットを行ってみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gather(data.frame(fit=rg$fitted.values,actual=scale(GDP.q$GDP),Date=GDP.q$publication),key,value,-Date),aes(y=value,x=Date,colour=key)) +
  geom_line(size=1) +
  ggtitle(&amp;quot;fit v.s. actual GDP&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;いかがでしょうか。個人的にはかなりフィッティングできている印象があります（もはや経済理論など不要なのでしょうか）。ただ、最も新しい値を除いては未来の値が情報量として加味された上で推計されていることになりますから、フェアではありません。正しく予測能力を検証するためには四半期ごとに逐次的に回帰を行う必要があります。&lt;/p&gt;
&lt;p&gt;というわけで、アウトサンプルの予測力がどれほどあるのかをテストしてみたいと思います。まず、2005年4月から2007年3月までの月次統計データでファクターを計算し、データ頻度を四半期に集約します。そして、2007年1Qのデータを除いて、GDPに回帰します。回帰したモデルの係数を用いて、2007年1Qのファクターデータで同時点のGDPの予測値を計算し、それを実績値と比較します。次は2005年4月から2007年6月までのデータを用いて･･･という感じでアウトサンプルの予測を行ってみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)
test_df &amp;lt;- data.frame()
for (i in as.list(seq(as.Date(&amp;quot;2015-04-01&amp;quot;),as.Date(&amp;quot;2019-03-01&amp;quot;),by=&amp;quot;quarter&amp;quot;))){
  day(i) &amp;lt;- days_in_month(i)
  traindata &amp;lt;- dataset1[dataset1$publication&amp;lt;=i,]
  X_train &amp;lt;- ESTIMATE_GPLVM(scale(traindata[,-2]),5)
  X_train.xts &amp;lt;- xts(X_train$output,order.by = traindata$publication)
  X_train.q.xts &amp;lt;- apply.quarterly(X_train.xts,mean)
  
  if (months(index(X_train.q.xts)[NROW(X_train.q.xts)]) %in% c(&amp;quot;3月&amp;quot;,&amp;quot;6月&amp;quot;,&amp;quot;9月&amp;quot;,&amp;quot;12月&amp;quot;)){
  } else X_train.q.xts &amp;lt;- X_train.q.xts[-NROW(X_train.q.xts),]
  
  colnames(X_train.q.xts) &amp;lt;- c(&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;,&amp;quot;factor4&amp;quot;,&amp;quot;factor5&amp;quot;) 

  GDP_train.q &amp;lt;- scale(GDP[GDP$publication&amp;gt;=index(X_train.q.xts)[1] &amp;amp; GDP$publication&amp;lt;=index(X_train.q.xts)[NROW(X_train.q.xts)],2])

  rg_train &amp;lt;- lm(GDP_train.q[-NROW(GDP_train.q)]~.,data=X_train.q.xts[-NROW(X_train.q.xts)])
  summary(rg_train)
  test_df &amp;lt;- rbind(test_df,data.frame(predict(rg_train,X_train.q.xts[NROW(X_train.q.xts)],interval = &amp;quot;prediction&amp;quot;,level=0.90),GDP=GDP_train.q[NROW(GDP_train.q)]))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;計算できました。グラフにしてみましょう。先ほどのグラフよりは精度が悪くなりました。特にリーマンの後は予測値の信頼区間（90%）が大きく拡大しており、不確実性が増大していることもわかります。2010年以降に関しては実績値は信頼区間にほど入っており、予測モデルとしての性能はまあまあなのかなと思います。ただ、リーマンのような金融危機もズバッと当てるところにロマンがあると思うので元データの改善を図りたいと思います。この記事はいったんここで終了です。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gather(data.frame(test_df,Date=as.Date(rownames(test_df))),,,-c(lwr,upr,Date)),aes(y=value,x=Date,colour=key)) +
  geom_ribbon(aes(ymax=upr,ymin=lwr,fill=&amp;quot;band&amp;quot;),alpha=0.1,linetype=&amp;quot;blank&amp;quot;) +
  geom_line(size=1) +
  ggtitle(&amp;quot;out-sample test&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Asset Allocation ModelをRで組んでみた。</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post2/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post2/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;h2&gt;目次&lt;/h2&gt;

&lt;/p&gt;
&lt;p&gt;おはこんばんにちは。勤め先で、アセットアロケーションに関するワークショップに参加したので、この分野は完全なる専門外ですがシミュレーションをしてみたいと思います。今回は、最小分散ポートフォリオ(minimum variance portfolio)を基本ポートフォリオとしたうえで、その分散共分散行列（予測値）をどのように推計するのかという点について先行研究を参考にエクササイズしていきたいと思います。先行研究は以下の論文です（オペレーションリサーチのジャーナルでした）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2947643&#34;&gt;Asset Allocation with Correlation: A Composite Trade-Off&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;最小分散ポートフォリオ&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. 最小分散ポートフォリオ&lt;/h2&gt;
&lt;p&gt;最小分散ポートフォリオの詳しい説明はここでは割愛しますが、要は各資産（内株、外株、内債、外債、オルタナ）のリターンの平均と分散を計算し、それらを縦軸平均値、横軸分散の二次平面にプロットしたうえで、投資可能範囲を計算し、その集合の中で最も分散が小さくなるポートフォリオの事らしいです（下図参照）。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Minimum_variance_flontier_of_MPT.svg/1280px-Minimum_variance_flontier_of_MPT.svg.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;minimum variance portfolio&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;先行研究のCarroll et. al. (2017)では、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;this paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;と記載されており、現状でも期待リターンの推計は難しく、それを必要としない最小分散ポートフォリオは有益で実践的な手法であるといえます。最小分散ポートフォリオの目的関数は、その名の通り「分散を最小化すること」です。今、各資産のリターンを集めたベクトルを[tex:r]、各資産の保有ウェイトを&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;、ポートフォリオリターンを&lt;span class=&#34;math inline&#34;&gt;\(R_{p}\)&lt;/span&gt;で表すことにすると、ポートフォリオ全体の分散&lt;span class=&#34;math inline&#34;&gt;\(var(R_{p})\)&lt;/span&gt;は以下のように記述できます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
var(R_{p}) = var(r^{T}\theta) = E( (r^{T}\theta)(r^{T}\theta)^{T}) = \theta^{T}\Sigma\theta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで&lt;span class=&#34;math inline&#34;&gt;\(Sigma\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;の分散共分散行列です。よって、最小化問題は以下になります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\theta}(\theta^{T}\Sigma\theta) \\
s.t 1^{T}\theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここでは、フルインベストメントを制約条件に加えています。ラグランジュ未定乗数法を用いてこの問題を解いてみましょう。ラグランジュ関数&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;は以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L = \theta^{T}\Sigma\theta + \lambda(1^{T}\theta - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1階の条件は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle\frac{\partial L}{\partial \theta} = 2\Sigma\theta + 1\lambda = 0 \\
\displaystyle \frac{\partial L}{\partial \lambda} = 1^{T} \theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1本目の式を&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;について解くと、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \Sigma^{-1}1\lambda^{*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}=-1/2\lambda\)&lt;/span&gt;です。これを2本目の式に代入し、&lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt;について解きます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
1^{T}\Sigma1\lambda^{*} = 1 \\
\displaystyle \lambda^{*} = \frac{1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta = \Sigma^{-1}1\lambda^{*}\)&lt;/span&gt;だったので、&lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt;を消去すると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \theta_{gmv} = \frac{\Sigma^{-1}1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となり、最適なウェイトを求めることができました。とりあえず、これをRで実装しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gmv &amp;lt;- function(r_dat,r_cov){
  library(MASS)
  i &amp;lt;- matrix(1,NCOL(r_dat),1)
  r_weight &amp;lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i)
  wr_dat &amp;lt;- r_dat*as.numeric(r_weight)
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;) 
  return(result)
}

nlgmv &amp;lt;- function(r_dat,r_cov){
  qp.out &amp;lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))),
           bvec=c(1,rep(0,NCOL(r_dat))),meq=1)
  r_weight &amp;lt;- qp.out$solution
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;入力は各資産のリターンと分散共分散行列になっています。出力はウェイト、リターン、リスクです。&lt;code&gt;nlgmv&lt;/code&gt;は最小分散ポートフォリオの空売り制約バージョンです。解析的な解は得られないので、数値的に買いを求めています。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;分散共分散行列をどのように求めるか&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 分散共分散行列をどのように求めるか&lt;/h2&gt;
&lt;p&gt;最小分散ポートフォリオの計算式は求めることができました。次は、その入力である分散共分散行列をどうやって求めるのかについて分析したいと思います。一番原始的な方法はその時点以前に利用可能なリターンデータを標本として分散共分散行列を求め、その値を固定して最小分散ポートフォリオを求めるというヒストリカルなアプローチかと思います（つまりウェイトも固定）。ただ、これはあくまで過去の平均値を将来の予想値に使用するため、いろいろ問題が出てくるかと思います。専門外の私が思いつくものとしては、前日ある資産Aのリターンが大きく下落したという場面で明日もこの資産の分散は大きくなることが予想されるにも関わらず、平均値を使用するため昨日の効果が薄められてしまうことでしょうか。それに、ウェイトを最初から変更しないというのも時間がたつにつれ、最適点から離れていく気がします。ただ、ではどう推計するのかついてはこの分野でも試行錯誤が行われているようです。Carroll et. al. (2017)でも、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;と述べられていました。この論文では以下のようなモデルを用いて推計が行われています。いずれも、分散共分散行列を時変としているところに特徴があります。&lt;/p&gt;
&lt;div id=&#34;a.-constant-conditional-correlation-ccc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A. Constant conditional correlation (CCC) model&lt;/h3&gt;
&lt;p&gt;元論文は&lt;a href=&#34;https://www.jstor.org/stable/2109358?read-now=1&amp;amp;seq=3#page_scan_tab_contents&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;まず、分散共分散行列と相関行列の関係性から、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t} = D_{t}R_{t}D_{t}\)&lt;/span&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は相関行列、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(diag(\sigma_{1,t},...,\sigma_{N,t})\)&lt;/span&gt;で各資産&lt;span class=&#34;math inline&#34;&gt;\(tt\)&lt;/span&gt;期の標準偏差&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{i,t}\)&lt;/span&gt;を対角成分に並べた行列です。ここから、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;を分けて推計していきます。まず、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;ですが、こちらは以下のような多変量GARCHモデル(1,1)で推計します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t} = \mu + u_{t} \\
u_{t} = \sigma_{t}\epsilon \\
\sigma_{t}^{2} = \alpha_{0} + \alpha_{1}u_{t-1}^{2} + \alpha_{2}\sigma_{t-1}^{2} \\
\epsilon_{t} = NID(0,1) \\
E(u_{t}|u_{t-1}) = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;はリターンの標本平均です。&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{i}\)&lt;/span&gt;は推定すべきパラメータ。&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;をGARCHで推計しているので、リターンの分布が正規分布より裾野の厚い分布に従い、またリターンの変化は一定ではなく前日の分散に依存する関係をモデル化しているといえるのではないでしょうか。とりあえずこれで&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;の推計はできたということにします。次に&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の推計ですが、このモデルではリターンを標本として求めるヒストリカルなアプローチを取ります。つまり、&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は定数です。よって、リターン変動の大きさは時間によって変化するが、各資産の相対的な関係性は不変であるという仮定を置いていることになります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-dynamic-conditional-correlation-dcc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B. Dynamic Conditional Correlation (DCC) model&lt;/h3&gt;
&lt;p&gt;元論文は&lt;a href=&#34;http://www.cass.city.ac.uk/__data/assets/pdf_file/0003/78960/Week7Engle_2002.pdf&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;こちらのモデルでは、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;を求めるところまでは①と同じですが、[&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、&lt;span class=&#34;math inline&#34;&gt;\(tex:t\)&lt;/span&gt;期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。ここで、&lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期での条件付分散共分散行列で以下のように定式化されます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q_{t} = \bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\epsilon_{i,t-1}\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\bar{Q}\)&lt;/span&gt;はヒストリカルな方法で計算した分散共分散行列であり、&lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt;はパラメータです。この方法では、先ほどとは異なり、リターン変動の大きさが時間によって変化するだけでなく、各資産の相対的な関係性も通時的に変化していくという仮定を置いていることになります。金融危機時には全資産のリターンが下落し、各資産の相関が正になる事象も観測されていることから、この定式化は魅力的であるということができるのではないでしょうか。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-dynamic-equicorrelation-deco-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;C. Dynamic Equicorrelation (DECO) model&lt;/h3&gt;
&lt;p&gt;元論文は&lt;a href=&#34;https://faculty.chicagobooth.edu/bryan.kelly/research/pdf/deco.pdf&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;この論文はまだきっちり読めていないのですが、相関行列&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の定義から&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = (1-\rho_{t})I_{N} + \rho_{t}1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となるようです。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt;はスカラーでequicorrelationの程度を表す係数です。equicorrelationとは平均的なペアワイズ相関の事であると理解しています。つまりは欠損値がなければ普通の相関と変わりないんじゃないかと。ただ、資産が増えればそのような問題にも対処する必要があるのでその点ではよい推定量のようです。&lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt;は以下のように求めることができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \rho_{t} = \frac{1}{N(N-1)}(\iota^{T}R_{t}^{DCC}\iota - N) = \frac{2}{N(N-1)}\sum_{i&amp;gt;j}\frac{q_{ij,t}}{\sqrt{q_{ii,t} q_{jj,t}}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\iota\)&lt;/span&gt;はN×1ベクトルで要素は全て1です。また、&lt;span class=&#34;math inline&#34;&gt;\(q_{ij,t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt;のi,j要素です。&lt;/p&gt;
&lt;p&gt;さて、分散共分散行列のモデル化ができたところで、ここまでを&lt;code&gt;R&lt;/code&gt;で実装しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carroll &amp;lt;- function(r_dat,FLG){
  
  library(rmgarch)
  
  if(FLG == &amp;quot;benchmark&amp;quot;){
    H &amp;lt;- cov(r_dat)
  }else{
    #1. define variables
    N &amp;lt;- NCOL(r_dat) # the number of assets
    
    #2. estimate covariance matrix
    basic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = &amp;#39;sGARCH&amp;#39;), distribution.model = &amp;#39;norm&amp;#39;)
    multi_garch = multispec(replicate(N, basic_garch))
    dcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = &amp;quot;mvnorm&amp;quot;,model = &amp;quot;DCC&amp;quot;)
    fit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE))
    forecast_dcc_garch &amp;lt;- dccforecast(fit_dcc_garch)
    if (FLG == &amp;quot;CCC&amp;quot;){
      #Constant conditional correlation (CCC) model
      D &amp;lt;- sigma(forecast_dcc_garch)
      R_ccc &amp;lt;- cor(r_dat)
      H &amp;lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1])
      colnames(H) &amp;lt;- colnames(r_dat)
      rownames(H) &amp;lt;- colnames(r_dat)
    }
    else{
      #Dynamic Conditional Correlation (DCC) model
      H &amp;lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1])
      if (FLG == &amp;quot;DECO&amp;quot;){
        #Dynamic Equicorrelation (DECO) model
        one &amp;lt;- matrix(1,N,N)
        iota &amp;lt;- rep(1,N)
        Q_dcc &amp;lt;- rcor(forecast_dcc_garch,type=&amp;quot;Q&amp;quot;)[[1]][,,1]
        rho &amp;lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N))
        D &amp;lt;- sigma(forecast_dcc_garch)
        R_deco &amp;lt;- (1-rho)*diag(1,N,N) + rho*one
        H &amp;lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1])
        colnames(H) &amp;lt;- colnames(r_dat)
        rownames(H) &amp;lt;- colnames(r_dat)
      }
    }
  }
  return(H)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;本来であれば、パッケージを使用するべきではないのですが、今日はエクササイズなので推計結果だけを追い求めたいと思います。GARCHについては再来週ぐらいに記事を書く予定です。
これで準備ができました。この関数にリターンデータを入れて、分散共分散行列を計算し、それを用いて最小分散ポートフォリオを計算することができるようになりました。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;テスト用データの収集&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. テスト用データの収集&lt;/h2&gt;
&lt;p&gt;データは以下の記事を参考にしました。&lt;/p&gt;
&lt;p&gt;(Introduction to Asset Allocation)[&lt;a href=&#34;https://www.r-bloggers.com/introduction-to-asset-allocation/&#34; class=&#34;uri&#34;&gt;https://www.r-bloggers.com/introduction-to-asset-allocation/&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;使用したのは、以下のインデックスに連動するETF(iShares)の基準価額データです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S&amp;amp;P500&lt;/li&gt;
&lt;li&gt;NASDAQ100&lt;/li&gt;
&lt;li&gt;MSCI Emerging Markets&lt;/li&gt;
&lt;li&gt;Russell 2000&lt;/li&gt;
&lt;li&gt;MSCI EAFE&lt;/li&gt;
&lt;li&gt;US 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index)&lt;/li&gt;
&lt;li&gt;U.S. Real Estate(the Dow Jones US Real Estate Index)&lt;/li&gt;
&lt;li&gt;gold bullion market&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;まず、データ集めです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantmod)

#**************************
# ★8 ASSETS SIMULATION
# SPY - S&amp;amp;P 500 
# QQQ - Nasdaq 100
# EEM - Emerging Markets
# IWM - Russell 2000
# EFA - EAFE
# TLT - 20 Year Treasury
# IYR - U.S. Real Estate
# GLD - Gold
#**************************

# load historical prices from Yahoo Finance
symbol.names = c(&amp;quot;S&amp;amp;P 500&amp;quot;,&amp;quot;Nasdaq 100&amp;quot;,&amp;quot;Emerging Markets&amp;quot;,&amp;quot;Russell 2000&amp;quot;,&amp;quot;EAFE&amp;quot;,&amp;quot;20 Year Treasury&amp;quot;,&amp;quot;U.S. Real Estate&amp;quot;,&amp;quot;Gold&amp;quot;)
symbols = c(&amp;quot;SPY&amp;quot;,&amp;quot;QQQ&amp;quot;,&amp;quot;EEM&amp;quot;,&amp;quot;IWM&amp;quot;,&amp;quot;EFA&amp;quot;,&amp;quot;TLT&amp;quot;,&amp;quot;IYR&amp;quot;,&amp;quot;GLD&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, auto.assign = TRUE)

#gn dates for all symbols &amp;amp; convert to monthly
hist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)
month.ends = endpoints(hist.prices, &amp;#39;day&amp;#39;)
hist.prices = Cl(hist.prices)[month.ends, ]
colnames(hist.prices) = symbols

# remove any missing data
hist.prices = na.omit(hist.prices[&amp;#39;1995::&amp;#39;])

# compute simple returns
hist.returns = na.omit( ROC(hist.prices, type = &amp;#39;discrete&amp;#39;) )

# compute historical returns, risk, and correlation
ia = list()
ia$expected.return = apply(hist.returns, 2, mean, na.rm = T)
ia$risk = apply(hist.returns, 2, sd, na.rm = T)
ia$correlation = cor(hist.returns, use = &amp;#39;complete.obs&amp;#39;, method = &amp;#39;pearson&amp;#39;)

ia$symbols = symbols
ia$symbol.names = symbol.names
ia$n = length(symbols)
ia$hist.returns = hist.returns

# convert to annual, year = 12 months
annual.factor = 12
ia$expected.return = annual.factor * ia$expected.return
ia$risk = sqrt(annual.factor) * ia$risk

rm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;リターンをプロットするとこんな感じです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = &amp;quot;パフォーマンスサマリー&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;次に、バックテストのコーディングを行います。一気にコードを公開します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BACK TEST
backtest &amp;lt;- function(r_dat,FLG,start_date,span,learning_term,port){
  #-----------------------------------------
  # BACKTEST
  # r_dat - return data(xts object) 
  # FLG - flag(CCC,DCC,DECO)
  # start_date - start date for backtest
  # span - rebalance frequency
  # learning_term - learning term (days)
  # port - method of portfolio optimization
  #-----------------------------------------
  
  library(stringi)

  initial_dat &amp;lt;- r_dat[stri_c(as.Date(start_date)-learning_term,&amp;quot;::&amp;quot;,as.Date(start_date))]
  for (i in NROW(initial_dat):NROW(r_dat)) {
    if (i == NROW(initial_dat)){
      H &amp;lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG)
      if (port == &amp;quot;nlgmv&amp;quot;){
        result &amp;lt;- nlgmv(initial_dat,H)
      }else if (port == &amp;quot;risk parity&amp;quot;){
        result &amp;lt;- risk_parity(initial_dat,H)
      }
      weight &amp;lt;- t(result$weight)
      colnames(weight) &amp;lt;- colnames(initial_dat)
      p_return &amp;lt;- initial_dat[NROW(initial_dat),]*result$weight
    } else {
      if (i %in% endpoints(r_dat,span)){
        H &amp;lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG)
        if (port == &amp;quot;nlgmv&amp;quot;){
          result &amp;lt;- nlgmv(test_dat,H)
        }else if (port == &amp;quot;risk parity&amp;quot;){
          result &amp;lt;- risk_parity(test_dat,H)
        }
        
      }
      weight &amp;lt;- rbind(weight,t(result$weight))
      p_return &amp;lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight)
    }
    if (i != NROW(r_dat)){
      term &amp;lt;- stri_c(index(r_dat[i+1,])-learning_term,&amp;quot;::&amp;quot;,index(r_dat[i+1,])) 
      test_dat &amp;lt;- r_dat[term]
    }
  }
  p_return$portfolio &amp;lt;- xts(apply(p_return,1,sum),order.by = index(p_return))
  weight.xts &amp;lt;- xts(weight,order.by = index(p_return))

  result &amp;lt;- list(p_return,weight.xts)
  names(result) &amp;lt;- c(&amp;quot;return&amp;quot;,&amp;quot;weight&amp;quot;)
  return(result)
}

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;計算結果をグラフにしてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result,main = &amp;quot;BACKTEST&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;空売り制約を課したので、上で定義した最小分散ポートフォリオは使用していません。どうやらこれだけで解析的に解くのは難しいらしく、数値的に解くことにしています。リバランス期間を週次にしたので、自前のPCでは計算に時間がかかりましたが、結果が計算できました。&lt;/p&gt;
&lt;p&gt;リーマン以降はどうやらベンチマークである等ウェイトポートフォリオよりをアウトパフォームしているようです。特に、DECOはいい感じです。そもそもDECOとDCCはほぼ変わらないパフォーマンスであると思っていたのですが、どうやら自分の理解が足らないらしく、論文の読み返す必要があるようです。Equicorrelationの意味をもう一度考えてみたいと思います。それぞれの組入比率の推移は以下のようになりました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot allocation weighting
d_allocation &amp;lt;- function(ggweight,title){
  #install.packages(&amp;quot;tidyverse&amp;quot;)
  library(tidyverse)
  ggweight &amp;lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method)
  ggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) +
    geom_area(colour=&amp;quot;black&amp;quot;,size=.1) +
    scale_y_continuous(limits = c(0,1)) +
    labs(title=title) + facet_grid(method~.)
}

gmv_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)))

# plot allocation weighting
d_allocation(gmv_weight,&amp;quot;GMV Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;リーマンの際にTLT、つまり米国債への比率を増やしているようです。CCCとDCCはそれ以外の部分でも米国債への比率が高く、よく挙げられる最小分散ポートフォリオの問題点がここでも発生しているようです。一方、DECOがやはり個性的な組入比率の推移をしており、ここらを考えてももう一度論文を読み返してみる必要がありそうです。&lt;/p&gt;
&lt;p&gt;追記（2019/3/3）
これまでは、最小分散ポートフォリオで分析をしていましたが、リスクパリティの結果も見たいなと言うことで、そのコードも書いてみました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;risk_parity &amp;lt;- function(r_dat,r_cov){
  fn &amp;lt;- function(weight, r_cov) {
    N &amp;lt;- NROW(r_cov)
    risks &amp;lt;-  weight * (r_cov %*% weight)
    g &amp;lt;- rep(risks, times = N) - rep(risks, each = N)
  return(sum(g^2))
  }
  dfn &amp;lt;- function(weight,r_cov){
    out &amp;lt;- weight
    for (i in 0:length(weight)) {
      up &amp;lt;- dn &amp;lt;- weight
      up[i] &amp;lt;- up[i]+.0001
      dn[i] &amp;lt;- dn[i]-.0001
      out[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002
    }
    return(out)
  }
  std &amp;lt;- sqrt(diag(r_cov)) 
  x0 &amp;lt;- 1/std/sum(1/std)
  res &amp;lt;- nloptr::nloptr(x0=x0,
                 eval_f=fn,
                 eval_grad_f=dfn,
                 eval_g_eq=function(weight,r_cov) { sum(weight) - 1 },
                 eval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) },
                 lb=rep(0,length(std)),ub=rep(1,length(std)),
                 opts = list(&amp;quot;algorithm&amp;quot;=&amp;quot;NLOPT_LD_SLSQP&amp;quot;,&amp;quot;print_level&amp;quot; = 0,&amp;quot;xtol_rel&amp;quot;=1.0e-8,&amp;quot;maxeval&amp;quot; = 1000),
                 r_cov = r_cov)
  r_weight &amp;lt;- res$solution
  names(r_weight) &amp;lt;- colnames(r_cov)
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
  }

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;結果はこんな感じ。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)

library(plotly)

# plot allocation weighting
riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))

# plot allocation weighting
d_allocation(riskparity_weight, &amp;quot;Risk Parity Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;どの手法もbenchmarkをアウトパーフォームできているという好ましい結果になりました。
やはり、分散共分散行列の推計がうまくいっているようです。また、DECOのパフォーマンスがよいのは、相関行列に各資産ペアの相関係数の平均値を用いているため、他の手法よりもリスク資産の組み入れが多くなったからだと思われます。ウェイトは以下の通りです。&lt;/p&gt;
&lt;p&gt;とりあえず、今日はここまで。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>カルマンフィルタの実装</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post3/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post3/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;おはこんばんにちは。かなり久しぶりの投稿となってしまいました。決して研究をさぼっていたのではなく、&lt;code&gt;BVAR&lt;/code&gt;のコーディングに手こずっていました。あと少しで完成します。さて、今回は&lt;code&gt;BVAR&lt;/code&gt;やこの前のGiannnone et a (2008)のような分析でも大活躍のカルマンフィルタを実装してしまいたいと思います。このブログではパッケージソフトに頼らず、基本的に自分で一から実装を行い、研究することをポリシーとしていますので、これから頻繁に使用するであろうカルマンフィルタを関数として実装してしまうことは非常に有益であると考えます。今回はRで実装をしましたので、そのご報告をします。&lt;/p&gt;
&lt;div id=&#34;カルマンフィルタとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. カルマンフィルタとは？&lt;/h2&gt;
&lt;p&gt;まず、カルマンフィルタに関する簡単な説明を行います。非常にわかりやすい記事があるので、&lt;a href=&#34;https://qiita.com/MoriKen/items/0c80ef75749977767b43&#34;&gt;こちら&lt;/a&gt;を読んでいただいたほうがより分かりやすいかと思います。&lt;/p&gt;
&lt;p&gt;カルマンフィルタとは、状態空間モデルを解くアルゴリズムの一種です。状態空間モデルとは、手元の観測可能な変数から観測できない変数を推定するモデルであり、以下のような形をしています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} \\
\alpha_{t} = T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(g×1\)&lt;/span&gt;ベクトルの観測可能な変数(観測変数)、&lt;span class=&#34;math inline&#34;&gt;\(Z_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(g×k\)&lt;/span&gt;係数行列、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(k×1\)&lt;/span&gt;ベクトルの観測不可能な変数(状態変数)、&lt;span class=&#34;math inline&#34;&gt;\(T_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(k×k\)&lt;/span&gt;係数行列です。また、&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;は観測変数の誤差項、&lt;span class=&#34;math inline&#34;&gt;\(\eta_{t}\)&lt;/span&gt;は状態変数の誤差項です。これらの誤差項はそれぞれ&lt;span class=&#34;math inline&#34;&gt;\(N(0,H_{t})\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(N(0,Q_{t})\)&lt;/span&gt;に従います（&lt;span class=&#34;math inline&#34;&gt;\(H_{t},Q_{t}\)&lt;/span&gt;は分散共分散行列）。&lt;span class=&#34;math inline&#34;&gt;\(d_{t}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt;は定数項です。1本目の式は観測方程式、2本目の式は遷移方程式と呼ばれます。
状態空間モデルを使用する例として、しばしば池の魚の数を推定する問題が使用されます。今、池の中の魚の全数が知りたいとして、その推定を考えます。観測時点毎に池の中の魚をすべて捕まえてその数を調べるのは現実的に困難なので、一定期間釣りをして釣れた魚をサンプルに全数を推定することを考えます。ここで、釣れた魚は観測変数、池にいる魚の全数は状態変数と考えることができます。今、経験的に釣れた魚の数と全数の間に以下のような関係があるとします。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = 0.01\alpha_{t} + 5 + \epsilon_{t}
\]&lt;/span&gt;
これが観測方程式になります。また、魚の全数は過去の値からそれほど急速には変化しないと考えられるため、以下のようなランダムウォークに従うと仮定します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\alpha_{t} = \alpha_{t-1}  + 500 + \eta_{t}
\]&lt;/span&gt;
これが遷移方程式になります。あとは、これをカルマンフィルタアルゴリズムを用いて計算すれば、観測できない魚の全数を推定することができます。
このように状態空間モデルは非常に便利なモデルであり、また応用範囲も広いです。例えば、販売額から潜在顧客数を推定したり、クレジットスプレッドやトービンのQ等経済モデル上の概念として存在する変数を推定する、&lt;code&gt;BVAR&lt;/code&gt;のように&lt;code&gt;VAR&lt;/code&gt;や回帰式の時変パラメータ推定などにも使用されます。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;カルマンフィルタアルゴリズムの導出&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. カルマンフィルタアルゴリズムの導出&lt;/h2&gt;
&lt;p&gt;さて、非常に便利な状態空間モデルの説明はこれくらいにして、カルマンフィルタの説明に移りたいと思います。カルマンフィルタは状態空間モデルを解くアルゴリズムの一種であると先述しました。つまり、他にも状態空間モデルを解くアルゴリズムは存在します。カルマンフィルタアルゴリズムは一般に誤差項の正規性の仮定を必要としないフィルタリングアルゴリズムであり、観測方程式と遷移方程式の線形性の仮定さえあれば、線形最小分散推定量となります。カルマンフィルタアルゴリズムの導出にはいくつかの方法がありますが、今回はこの線形最小分散推定量としての導出を行います。まず、以下の３つの仮定を置きます。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;初期値&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{0}\)&lt;/span&gt;は正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(a_{0},\Sigma_{0})\)&lt;/span&gt;に従う確率ベクトルである(&lt;span class=&#34;math inline&#34;&gt;\(a_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t}\)&lt;/span&gt;の推定値)。&lt;/li&gt;
&lt;li&gt;誤差項&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(\eta_{s}\)&lt;/span&gt;は全ての&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;で互いに独立で、初期値ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{0}\)&lt;/span&gt;と無相関である（&lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon_{t}\eta_{s})=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon_{t}\alpha_{0})=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E(\eta_{t}\alpha_{0})=0\)&lt;/span&gt;）。&lt;/li&gt;
&lt;li&gt;2より、&lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon_{t}\alpha_{t}&amp;#39;)=0\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(E(\eta_{t}\alpha_{t-1}&amp;#39;)=0\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;まず、&lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;期の情報集合&lt;span class=&#34;math inline&#34;&gt;\(\Omega_{t-1}\)&lt;/span&gt;が既知の状態での&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;の期待値（予測値）を求めてみましょう。上述した状態空間モデルと誤差項の期待値がどちらもゼロである事実を用いると、以下のように計算することができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E(\alpha_{t}|\Omega_{t-1}) = a_{t|t-1} = T_{t}a_{t-1|t-1} + c_{t}
E(Y_{t}|\Omega_{t-1}) = Y_{t|t-1} = Z_{t}a_{t|t-1} + d_{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、次に、これらの分散を求めます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E( (\alpha_{t}-a_{t|t-1})(\alpha_{t}-a_{t|t-1})&amp;#39;|\Omega_{t-1}) &amp;amp;=&amp;amp; E( (T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}-a_{t|t-1})(T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}-a_{t|t-1})&amp;#39;|\Omega_{t-1}) \\ 
&amp;amp;=&amp;amp; E(T_{t}\alpha_{t-1}\alpha_{t-1}&amp;#39;T_{t}&amp;#39; + R_{t}\eta_{t}\eta_{t}&amp;#39;R_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; E(T_{t}\alpha_{t-1}\alpha_{t-1}&amp;#39;T_{t}&amp;#39;|\Omega_{t-1}) + E(R_{t}\eta_{t}\eta_{t}&amp;#39;R_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; T_{t}E(\alpha_{t-1}\alpha_{t-1}&amp;#39;|\Omega_{t-1})T_{t}&amp;#39; + R_{t}E(\eta_{t}\eta_{t}&amp;#39;|\Omega_{t-1})R_{t}&amp;#39; \\
&amp;amp;=&amp;amp; T_{t}\Sigma_{t-1|t-1}T_{t}&amp;#39; + R_{t}Q_{t}R_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E( (Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})&amp;#39;|\Omega_{t-1}) &amp;amp;=&amp;amp; E( (Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t}-Y_{t|t-1})(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t}-Y_{t|t-1})&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; E(Z_{t}\alpha_{t}\alpha_{t}&amp;#39;Z_{t}&amp;#39; + S_{t}\epsilon_{t}\epsilon_{t}&amp;#39;S_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; E(Z_{t}\alpha_{t}\alpha_{t}&amp;#39;Z_{t}&amp;#39;|\Omega_{t-1}) + E(S_{t}\epsilon_{t}\epsilon_{t}&amp;#39;S_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; Z_{t}E(\alpha_{t}\alpha_{t}&amp;#39;|\Omega_{t-1})Z_{t}&amp;#39; + S_{t}E(\epsilon_{t}\epsilon_{t}&amp;#39;|\Omega_{t-1})S_{t}&amp;#39; \\
&amp;amp;=&amp;amp; Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39; \\
&amp;amp;=&amp;amp; F_{t|t-1}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の情報集合&lt;span class=&#34;math inline&#34;&gt;\(\Omega_{t}\)&lt;/span&gt;が得られたとします（つまり、観測値&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;を入手）。カルマンフィルタでは、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の情報である観測値&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;を用いて&lt;span class=&#34;math inline&#34;&gt;\(a_{t|t-1}\)&lt;/span&gt;を以下の方程式で更新します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E(\alpha_{t}|\Omega_{t}) = a_{t|t} = a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1})
\]&lt;/span&gt;
つまり、観測値と&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;の期待値（予測値）の差をあるウェイト&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(k×g\)&lt;/span&gt;行列）でかけたもので補正をかけるわけです。よって、観測値と予測値が完全に一致していた場合は補正は行われないことになります。ここで重要なのは、ウエイト&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;をどのように決めるのかです。&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;は更新後の状態変数の分散&lt;span class=&#34;math inline&#34;&gt;\(E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&amp;#39;)= \Sigma_{t|t}\)&lt;/span&gt;を最小化するよう決定します。これが、カルマンフィルタが線形最小分散推定量である根拠です。最小化にあたっては以下のベクトル微分が必要になりますので、おさらいをしておきましょう。今回使用するのは以下の事実です。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial a&amp;#39;b}{\partial b} = \frac{\partial b&amp;#39;a}{\partial b} = a \\
\displaystyle \frac{\partial b&amp;#39;Ab}{\partial b} = 2Ab
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt;はベクトル（それぞれ&lt;span class=&#34;math inline&#34;&gt;\(n×1\)&lt;/span&gt;ベクトル、&lt;span class=&#34;math inline&#34;&gt;\(1×n\)&lt;/span&gt;ベクトル）、&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(n×n\)&lt;/span&gt;の対称行列です。まず、１つ目から証明していきます。&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y = a&amp;#39;b = b&amp;#39;a = \sum_{i=1}^{n}a_{i}b_{i}\)&lt;/span&gt;とします。
このとき、&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial y}{\partial b_{i}}=a_{i}\)&lt;/span&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial a&amp;#39;b}{\partial b} = \frac{\partial b&amp;#39;a}{\partial b} = a
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;次に２つ目です。&lt;span class=&#34;math inline&#34;&gt;\(y = b&amp;#39;Ab = \sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}b_{i}b_{j}\)&lt;/span&gt;とします。このとき、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial y}{\partial b_{i}} = \sum_{j=1}^{n}a_{ij}b_{j} + \sum_{j=1}^{n}a_{ji}b_{j} = 2\sum_{j=1}^{n}a_{ij}b_{j} = 2a_{i}&amp;#39;b
\]&lt;/span&gt;
よって、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial y}{\partial b} =
\left(
    \begin{array}{cccc}
      \frac{\partial y}{\partial b_{1}} \\
      \vdots \\
      \frac{\partial y}{\partial b_{n}} \\
    \end{array}
  \right) = 2
\left(
    \begin{array}{cccc}
      \sum_{j=1}^{n}a_{1j}b_{j} \\
      \vdots \\
      \sum_{j=1}^{n}a_{nj}b_{j} \\
    \end{array}
  \right) = 2
\left(
    \begin{array}{cccc}
      a_{1}&amp;#39;b \\
      \vdots \\
      a_{n}&amp;#39;b \\
    \end{array}
  \right)
= 2Ab
\]&lt;/span&gt;
さて、準備ができたので、更新後の状態変数の分散&lt;span class=&#34;math inline&#34;&gt;\(E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&amp;#39;)\)&lt;/span&gt;を求めてみましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&amp;#39;) &amp;amp;=&amp;amp; \Sigma_{t|t} \\
&amp;amp;=&amp;amp; E\{ (\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))(\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))&amp;#39;\} \\
&amp;amp;=&amp;amp; E\{ ( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )\} \\
&amp;amp;=&amp;amp; E\{ ( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1}) )( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1}) )&amp;#39;\} \\
&amp;amp;=&amp;amp; E\{ ( (I - k_{t}Z_{t})\alpha_{t} - k_{t}S_{t}\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})( (I - k_{t}Z_{t})\alpha_{t} - k_{t}S_{t}\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})&amp;#39; \} \\
&amp;amp;=&amp;amp; E\{( (I - k_{t}Z_{t})(\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\epsilon_{t})( (I - k_{t}Z_{t})(\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\epsilon_{t})&amp;#39;\} \\
&amp;amp;=&amp;amp; (I - k_{t}Z_{t})\Sigma_{t|t-1}(I - k_{t}Z_{t})&amp;#39; + k_{t}S_{t}H_{t}S_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; (\Sigma_{t|t-1} - k_{t}Z_{t}\Sigma_{t|t-1})(I - k_{t}Z_{t})&amp;#39; + k_{t}S_{t}H_{t}S_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}(k_{t}Z_{t})&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; + k_{t}S_{t}H_{t}S_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)k_{t}&amp;#39; \\
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;１回目の式変形で、&lt;span class=&#34;math inline&#34;&gt;\(a_{t|t}\)&lt;/span&gt;に上述した更新式を代入し、２回目の式変形で観測方程式と上で計算した&lt;span class=&#34;math inline&#34;&gt;\(E(Y_{t}|\Omega_{t-1})\)&lt;/span&gt;を代入しています。さて、更新後の状態変数の分散&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t|t}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;の関数として書き表すことができたので、これを&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;で微分し、0と置き、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t|t}\)&lt;/span&gt;を最小化する&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;を求めます。先述した公式で、&lt;span class=&#34;math inline&#34;&gt;\(a=\Sigma_{t|t-1}Z_{t}&amp;#39;\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(b=k_{t}&amp;#39;\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(A=(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)\)&lt;/span&gt;とすると（&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;は分散共分散行列の和なので対称行列）、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial \Sigma_{t|t}}{\partial k_{t}&amp;#39;} = -2(Z_{t}\Sigma_{t|t-1})&amp;#39; + 2(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)k_{t} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここから、&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;を解きなおすと、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
k_{t} &amp;amp;=&amp;amp; \Sigma_{t|t-1}Z_{t}&amp;#39;(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)^{-1} \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1}Z_{t}&amp;#39;F_{t|t-1}^{-1}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;突然、&lt;span class=&#34;math inline&#34;&gt;\(F_{t|t-1}\)&lt;/span&gt;が出てきました。これは観測変数の予測値の分散&lt;span class=&#34;math inline&#34;&gt;\(E((Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})&amp;#39;|\Omega_{t-1})\)&lt;/span&gt;でした。一方、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t|t-1}Z_{t}\)&lt;/span&gt;は状態変数の予測値の分散を観測変数のスケールに調整したものです（観測空間に写像したもの）。つまり、カルマンゲイン&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;は状態変数と観測変数の予測値の分散比となっているのです。観測変数にはノイズがあり、観測方程式はいつも誤差０で満たされるわけではありません。また、状態方程式にも誤差項が存在します。状態の遷移も100%モデル通りにはいかないということです。&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の観測変数&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;が得られたとして、それをどれほど信頼して状態変数を更新するかは観測変数のノイズが状態変数のノイズに比べてどれほど小さいかによります。つまり、相対的に観測方程式が遷移方程式よりも信頼できる場合には状態変数を大きく更新するのです。このように、カルマンフィルタでは、観測方程式と遷移方程式の相対的な信頼度によって、更新の度合いを毎期調整しています。その度合いが分散比であり、カルマンゲインだというわけです。ちなみに欠損値が発生した場合には、観測変数の分散を無限大にし、状態変数の更新を全く行わないという対処を行います。観測変数に信頼がないので当たり前の処置です。この場合は遷移方程式を100%信頼します。これがカルマンフィルタのコアの考え方になります。
更新後の分散を計算しておきます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\Sigma_{t|t} &amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}F_{t|t-1}k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + (\Sigma_{t|t-1}Z_{t}&amp;#39;F_{t|t-1}^{-1})F_{t|t-1}k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - k_{t}Z_{t}\Sigma_{t|t-1} \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - k_{t}F_{t|t-1}F_{t|t-1}^{-1}Z_{t}\Sigma_{t|t-1} \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;では、最終的に導出されたアルゴリズムをまとめたいと思います。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
a_{t|t-1} &amp;amp;=&amp;amp; T_{t}a_{t-1|t-1} + c_{t} \\
\Sigma_{t|t-1} &amp;amp;=&amp;amp; T_{t}\Sigma_{t-1|t-1}T_{t}&amp;#39; + R_{t}Q_{t}R_{t}&amp;#39; \\
Y_{t|t-1} &amp;amp;=&amp;amp; Z_{t}a_{t|t-1} + d_{t} \\
F_{t|t-1} &amp;amp;=&amp;amp;  Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39; \\
k_{t} &amp;amp;=&amp;amp; \Sigma_{t|t-1}Z_{t}&amp;#39;F_{t|t-1}^{-1} \\
a_{t|t} &amp;amp;=&amp;amp; a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}) \\
\Sigma_{t|t} &amp;amp;=&amp;amp;  \Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;
初期値&lt;span class=&#34;math inline&#34;&gt;\(a_{0},\Sigma_{0}\)&lt;/span&gt;が所与の元で、まず状態変数の予測値&lt;span class=&#34;math inline&#34;&gt;\(a_{1|0},\Sigma_{1|0}\)&lt;/span&gt;を計算します。その結果を用いて、次は観測変数の予測値&lt;span class=&#34;math inline&#34;&gt;\(Y_{t|t-1},F_{t|t-1}\)&lt;/span&gt;を計算し、カルマンゲイン&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;を得ます。&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の観測可能なデータを入手したら、更新方程式を用いて&lt;span class=&#34;math inline&#34;&gt;\(a_{t|t},\Sigma_{t|t}\)&lt;/span&gt;を更新します。これをサンプル期間繰り返していくことになります。ちなみに、遷移方程式の誤差項&lt;span class=&#34;math inline&#34;&gt;\(\eta_{t}\)&lt;/span&gt;と定数項&lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt;がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致します。つまり、新しいサンプルを入手するたびに&lt;code&gt;OLS&lt;/code&gt;をやり直す推計方法ということです（今回はその証明は勘弁してください）。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rで実装する&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. &lt;code&gt;R&lt;/code&gt;で実装する。&lt;/h2&gt;
&lt;p&gt;以下が&lt;code&gt;R&lt;/code&gt;での実装コードです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kalmanfiter &amp;lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){
  #-------------------------------------------------------------------
  # Implemention of Kalman filter
  #   y - observed variable
  #   I - the number of unobserved variable
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   c - constant in state equaion
  #   R - parameter of exogenous variable in state equation
  #   Q - var-cov matrix of exogenous variable in state equation
  #   d - constant in observable equaion
  #   S - parameter of exogenous variable in observable equation
  #   h - var-cov matrix of exogenous variable in observable equation
  #   a_int - initial value of endogenous variable
  #   sig_int - initial value of variance of endogenous variable
  #-------------------------------------------------------------------
  
  library(MASS)
  
  # 1.Define Variable
  if (class(y)!=&amp;quot;matrix&amp;quot;){
    y &amp;lt;- as.matrix(y)
  }
  N &amp;lt;- NROW(y) # sample size
  L &amp;lt;- NCOL(y) # the number of observable variable 
  a_pre &amp;lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable
  a_fil &amp;lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable
  sig_pre &amp;lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable
  sig_fil &amp;lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable
  y_pre &amp;lt;- array(0,dim = c(L,1,N)) # prediction of observed variable
  F_pre &amp;lt;- array(0,dim = c(L,L,N)) # prediction of var-cov mat. of observable variable 
  F_inv &amp;lt;- array(0,dim = c(L,L,N)) # inverse of F_pre
  k &amp;lt;- array(0,dim = c(I,L,N)) # kalman gain
  
  if (any(is.na(a_int))==TRUE){
    a_int &amp;lt;- matrix(0,nrow = I,ncol = 1)
  }
  if (any(is.na(sig_int))==TRUE){
    sig_int &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(R))==TRUE){
    R &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(Q))==TRUE){
    Q &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(S))==TRUE){
    S &amp;lt;- matrix(1,nrow = L,ncol = L)
  }
  if (any(is.na(h))==TRUE){
    H &amp;lt;- array(0,dim = c(L,L,N))
    for(i in 1:N){
      diag(H[,,i]) = 1
    }
  }else if (class(h)!=&amp;quot;array&amp;quot;){
    H &amp;lt;- array(h,dim = c(NROW(h),NCOL(h),N))
  }
  
  # fill infinite if observed data is NA
  for(i in 1:N){
    miss &amp;lt;- is.na(y[i,])
    diag(H[,,i])[miss] &amp;lt;- 1e+32
  }
  y[is.na(y)] &amp;lt;- 0
  
  # 2.Set Initial Value
  a_fil[,,1] &amp;lt;- a_int
  sig_fil[,,1] &amp;lt;- sig_int
  
  # 3.Implement Kalman filter
  for (i in 1:N){
    if(class(z)==&amp;quot;array&amp;quot;){
      Z &amp;lt;- z[,,i]
    }else{
      Z &amp;lt;- z
    }
    a_pre[,,i] &amp;lt;- t%*%a_fil[,,i] + c
    sig_pre[,,i] &amp;lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)
    y_pre[,,i] &amp;lt;- Z%*%a_pre[,,i] + d
    F_pre[,,i] &amp;lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)
    k[,,i] &amp;lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])
    a_fil[,,i+1] &amp;lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])
    sig_fil[,,i+1] &amp;lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])
  }
  
  # 4.Aggregate results
  result &amp;lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)
  names(result) &amp;lt;- c(&amp;quot;state prediction&amp;quot;, &amp;quot;state filtered&amp;quot;, &amp;quot;state var prediction&amp;quot;, 
                     &amp;quot;state var filtered&amp;quot;, &amp;quot;observable prediction&amp;quot;, &amp;quot;kalman gain&amp;quot;,
                     &amp;quot;parameter of state eq&amp;quot;, &amp;quot;parameter of observable eq&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;案外簡単に書けるもんですね。これを使って、Giannone et al (2008)をやり直してみます。データセットは前回記事と変わりません。&lt;/p&gt;
&lt;p&gt;以下、分析用の&lt;code&gt;R&lt;/code&gt;コードです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#------------------------
# Giannone et. al. 2008 
#------------------------

library(MASS)
library(xts)

# ファクターを計算
f &amp;lt;- 3
z &amp;lt;- scale(dataset1)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;S_i &amp;lt;- z[i,]%*%t(z[i,])&amp;quot;,sep = &amp;quot;&amp;quot;)))
  if (i==1){
    S &amp;lt;- S_i
  }else{
    S &amp;lt;- S + S_i
  }
}
S &amp;lt;- (1/nrow(z))*S
gamma &amp;lt;- eigen(S)
D &amp;lt;- diag(gamma$values[1:f])
V &amp;lt;- gamma$vectors[,1:f]
F_t &amp;lt;- matrix(0,nrow(z),f)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;F_t[&amp;quot;,i,&amp;quot;,]&amp;lt;- z[&amp;quot;,i,&amp;quot;,]%*%V&amp;quot;,sep = &amp;quot;&amp;quot;)))
}
lambda_hat &amp;lt;- V
psi &amp;lt;- diag(diag(S-V%*%D%*%t(V)))
R &amp;lt;- diag(diag(cov(z-z%*%V%*%t(V))))

a &amp;lt;- matrix(0,f,f)
b &amp;lt;- matrix(0,f,f)
for(t in 2:nrow(z)){
  a &amp;lt;- a + F_t[t,]%*%t(F_t[t-1,])
  b &amp;lt;- b + F_t[t-1,]%*%t(F_t[t-1,])
}
b_inv &amp;lt;- solve(b)
A_hat &amp;lt;- a%*%b_inv

e &amp;lt;- numeric(f)
for (t in 2:nrow(F_t)){
  e &amp;lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat
}
H &amp;lt;- t(e)%*%e
Q &amp;lt;- diag(1,f,f)
Q[1:f,1:f] &amp;lt;- H

p &amp;lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))

result1 &amp;lt;- kalmanfiter(z,f,A_hat,lambda_hat,c=0,R=NA,Q=Q,d=0,S=NA,h=R,a_int=F_t[1,],sig_int=matrix(p,f,f))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;プロットしてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tidyverse)

ggplot(gather(data.frame(factor1=result1$`state filtered`[1,1,-dim(result1$`state filtered`)[3]],factor2=result1$`state filtered`[2,1,-dim(result1$`state filtered`)[3]],factor3=result1$`state filtered`[3,1,-dim(result1$`state filtered`)[3]],time=as.Date(rownames(dataset1))),key = factor,value = value,-time),aes(x=time,y=value,colour=factor)) + geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;giannoneの記事を書いた際は、元論文の&lt;code&gt;MATLAB&lt;/code&gt;コードを参考にRで書いたのですが、通常のカルマンフィルタとは観測変数の分散共分散行列の逆数の計算方法が違うらしくグラフの形が異なっています。まあでも、概形はほとんど同じですが（なので、ちゃんと動いているはず）。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;カルマンスムージング&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. カルマンスムージング&lt;/h2&gt;
&lt;p&gt;カルマンフィルタの実装は以上で終了なのですが、誤差項の正規性を仮定すれば&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期までの情報集合&lt;span class=&#34;math inline&#34;&gt;\(\Omega_{T}\)&lt;/span&gt;を用いて、&lt;span class=&#34;math inline&#34;&gt;\(a_{i|i}, \Sigma_{i|i}(i = 1:T)\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(a_{i|T}, \Sigma_{i|T}(i = 1:T)\)&lt;/span&gt;へ更新することができます。これをカルマンスムージングと呼びます。これを導出してみましょう。その準備として、以下のような&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t+1|t}\)&lt;/span&gt;の混合分布を計算しておきます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
(\alpha_{t|t},\alpha_{t+1|t}) &amp;amp;=&amp;amp; N(
\left(
    \begin{array}{cccc}
      E(\alpha_{t|t}) \\
      E(\alpha_{t+1|t})
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      Var(\alpha_{t|t}), Cov(\alpha_{t|t},\alpha_{t+1|t}) \\
      Cov(\alpha_{t+1|t},\alpha_{t|t}), Var(\alpha_{t+1|t})
    \end{array}
  \right)
) \\
&amp;amp;=&amp;amp; N(
\left(
    \begin{array}{cccc}
      a_{t|t} \\
      a_{t+1|t}
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      \Sigma_{t|t}, \Sigma_{t|t}T_{t}&amp;#39; \\
      T_{t}\Sigma_{t|t}, \Sigma_{t+1|t} 
    \end{array}
  \right)
)
\end{eqnarray}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(Cov(\alpha_{t|t},\alpha_{t+1|t})\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
Cov(\alpha_{t+1|t},\alpha_{t|t}) &amp;amp;=&amp;amp; Cov(T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}, \alpha_{t|t}) \\
&amp;amp;=&amp;amp; T_{t}Cov(\alpha_{t|t},\alpha_{t|t}) + Cov(c_{t},\alpha_{t|t}) + Cov(R_{t}\eta_{t},\alpha_{t|t}) \\
&amp;amp;=&amp;amp; T_{t}Var(\alpha_{t|t}) = T_{t}\Sigma_{t|t}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、条件付き多変量正規分布は以下のような分布をしていることを思い出しましょう（
&lt;a href=&#34;https://mathwords.net/gaussjoken&#34;&gt;参考&lt;/a&gt;
）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(X_{1},X_{2}) = N(
\left(
    \begin{array}{cccc}
      \mu_{1} \\
      \mu_{2}
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      \Sigma_{11}, \Sigma_{12} \\
      \Sigma_{21}, \Sigma_{22}
    \end{array}
  \right)
) \\
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
(X_{1}|X_{2}=x_{2}) = N(\mu_{1} + \Sigma_{12}\Sigma_{22}^{-1}(x_{2}-\mu_{2}),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;これを用いて、&lt;span class=&#34;math inline&#34;&gt;\((\alpha_{t|t}|\alpha_{t+1|t}=a_{t+1})\)&lt;/span&gt;を計算してみましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
(\alpha_{t|t}|\alpha_{t+1|t}=a_{t+1}) &amp;amp;=&amp;amp; N(a_{t|t} + \Sigma_{t|t}T_{t}&amp;#39;\Sigma_{t+1|t}^{-1}(a_{t+1}-a_{t+1|t}), \Sigma_{t|t}-\Sigma_{t|t}T_{t}&amp;#39;\Sigma_{t+1|t}^{-1}T_{t}\Sigma_{t|t}) \\
&amp;amp;=&amp;amp;N(a_{t|t} + L_{t}(a_{t+1}-a_{t+1|t}), \Sigma_{t|t}-L_{t}\Sigma_{t+1|t}L_{t}&amp;#39;)
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ただし、&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1}\)&lt;/span&gt;の値は観測不可能なので、上式を用いて状態変数を更新することはできません。今、わかるのは&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期における&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1|T}\)&lt;/span&gt;の分布のみです。ということで、&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1|T}\)&lt;/span&gt;で代用し、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|T}\)&lt;/span&gt;の分布を求めてみます。では、計算していきます。&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|T} = N(E(\alpha_{t|T}),Var(\alpha_{t|T}))\)&lt;/span&gt;ですが、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
Var(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(Var(\alpha_{t|t}|\alpha_{t+1|t} = \alpha_{t+1|T})) + Var_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T}))
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;というように、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t+1|T}\)&lt;/span&gt;も確率変数となるので、繰り返し期待値の法則と繰り返し分散の法則を使用します（&lt;a href=&#34;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/video-lectures/lecture-12-iterated-expectations-sum-of-a-random-number-of-random-variables/MIT6_041F10_L12.pdf&#34;&gt;こちら&lt;/a&gt;を参照）。&lt;/p&gt;
&lt;p&gt;*繰り返し期待値の法則
&lt;span class=&#34;math inline&#34;&gt;\(E(x) = E_{Z}(E(X|Y=Z))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;*繰り返し分散の法則
&lt;span class=&#34;math inline&#34;&gt;\(Var(X) = E_{Z}(Var(X|Y=Z))+Var_{Z}(E(X|Y=Z))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
&amp;amp;=&amp;amp; a_{t|t} + L_{t}(a_{t+1|T}-a_{t+1|t}) \\
Var(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(Var(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) + Var_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( (a_{t|t} + L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))(a_{t|t} + L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( (L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))(L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( (L_{t}\alpha_{t+1|T} - L_{t}a_{t+1|T})(L_{t}\alpha_{t+1|T} - L_{t}(a_{t+1|T})&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( L_{t}(\alpha_{t+1|T} - a_{t+1|T})(\alpha_{t+1|T} - a_{t+1|T})&amp;#39;L_{t}&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + L_{t}E( (\alpha_{t+1|T} - a_{t+1|T})(\alpha_{t+1|T} - a_{t+1|T})&amp;#39;)L_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + L_{t}\Sigma_{t+1|T}L_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。カルマンスムージングのアルゴリズムをまとめておきます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
L_{t} &amp;amp;=&amp;amp; \Sigma_{t|t}T_{t}\Sigma_{t+1|t}^{-1} \\
a_{t|T} &amp;amp;=&amp;amp; a_{t|t} + L_{t}(a_{t+1|T} - a_{t+1|t}) \\
\Sigma_{t|T} &amp;amp;=&amp;amp; \Sigma_{t+1|t} + L_{t}(\Sigma_{t+1|T}-\Sigma_{t+1|t})L_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;
カルマンスムージングの特徴的な点は後ろ向きに計算をしていく点です。つまり、&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期から1期に向けて計算を行っていきます。&lt;span class=&#34;math inline&#34;&gt;\(L_{t}\)&lt;/span&gt;に関してはそもそもカルマンフィルタを回した時点で計算可能ですが、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|T}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期までのデータが手元にないと計算できません。今、&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期まで観測可能なデータが入手できたとしましょう。すると、２番目の方程式を用いて、&lt;span class=&#34;math inline&#34;&gt;\(a_{T-1|T}\)&lt;/span&gt;を計算します。ちなみに&lt;span class=&#34;math inline&#34;&gt;\(a_{T|T}\)&lt;/span&gt;はカルマンフィルタを回した時点ですでに手に入っているので、計算する必要はありません。同時に、３番目の式を用いて&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{T-1|T}\)&lt;/span&gt;を計算します。そして、&lt;span class=&#34;math inline&#34;&gt;\(a_{T-1|T},\Sigma_{T-1|T}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(L_{T-1}\)&lt;/span&gt;を用いて&lt;span class=&#34;math inline&#34;&gt;\(a_{T-2|T},\Sigma_{T-2|T}\)&lt;/span&gt;を計算、というように1期に向けて後ろ向きに計算をしていくのです。さきほど、遷移方程式の誤差項&lt;span class=&#34;math inline&#34;&gt;\(\eta_{t}\)&lt;/span&gt;と定数項&lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt;がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致すると書きましたが、カルマンスムージングの場合は&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期までのサンプルで&lt;code&gt;OLS&lt;/code&gt;を行った結果と一致します。
&lt;code&gt;R&lt;/code&gt;で実装してみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kalmansmoothing &amp;lt;- function(filter){
  #-------------------------------------------------------------------
  # Implemention of Kalman smoothing
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   a_pre - prediction of state
  #   a_fil - filtered value of state
  #   sig_pre - prediction of var of state
  #   sig_fil - filtered value of state
  #-------------------------------------------------------------------
  
  library(MASS)
  
  # 1.Define variable
  a_pre &amp;lt;- filter$`state prediction`
  a_fil &amp;lt;- filter$`state filtered`
  sig_pre &amp;lt;- filter$`state var prediction`
  sig_fil &amp;lt;- filter$`state var filtered`
  t &amp;lt;- filter$`parameter of state eq`
  C &amp;lt;- array(0,dim = dim(sig_pre))
  a_sm &amp;lt;- array(0,dim = dim(a_pre))
  sig_sm &amp;lt;- array(0,dim = dim(sig_pre))
  N &amp;lt;- dim(C)[3]
  a_sm[,,N] &amp;lt;- a_fil[,,N]
  sig_sm[,,N] &amp;lt;- sig_fil[,,N]
  
  for (i in N:2){
    C[,,i-1] &amp;lt;- sig_fil[,,i-1]%*%t(t)%*%ginv(sig_pre[,,i])
    a_sm[,,i-1] &amp;lt;- a_fil[,,i-1] + C[,,i-1]%*%(a_sm[,,i]-a_pre[,,i])
    sig_sm[,,i-1] &amp;lt;- sig_fil[,,i-1] + C[,,i-1]%*%(sig_sm[,,i]-sig_pre[,,i])%*%t(C[,,i-1])
  }
  
  
  result &amp;lt;- list(a_sm,sig_sm,C)
  names(result) &amp;lt;- c(&amp;quot;state smoothed&amp;quot;, &amp;quot;state var smoothed&amp;quot;, &amp;quot;c&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;先ほどのコードの続きで&lt;code&gt;R&lt;/code&gt;コードを書いてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result2 &amp;lt;- kalmansmoothing(result1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;かなりシンプルですね。ちなみにグラフにしましたが、１個目とほぼ変わりませんでした。とりあえず、今日はここまで。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ガウス回帰の実装をやってみた</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post1/post1/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post1/post1/</guid>
      <description>
&lt;script src=&#34;https://ayatoashihara.github.io/myblog_jp/myblog_jp/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;　おはこんばんにちは。昨日、&lt;code&gt;Bayesian Vector Autoregression&lt;/code&gt;の記事を書きました。&lt;br /&gt;
　その中でハイパーパラメータのチューニングの話が出てきて、なにか効率的にチューニングを行う方法はないかと探していた際に&lt;code&gt;Bayesian Optimization&lt;/code&gt;を発見しました。日次GDPでも機械学習の手法を利用しようと思っているので、&lt;code&gt;Bayesian Optimization&lt;/code&gt;はかなり使える手法ではないかと思い、昨日徹夜で理解しました。&lt;br /&gt;
　その内容をここで実装しようとは思うのですが、&lt;code&gt;Bayesian Optimization&lt;/code&gt;ではガウス回帰（&lt;code&gt;Gaussian Pocess Regression&lt;/code&gt;,以下&lt;code&gt;GPR&lt;/code&gt;）を使用しており、まずその実装を行おうと持ったのがこのエントリを書いた動機です。&lt;code&gt;Bayesian Optimization&lt;/code&gt;の実装はこのエントリの後にでも書こうかなと思っています。&lt;/p&gt;
&lt;div id=&#34;gprとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. &lt;code&gt;GPR&lt;/code&gt;とは&lt;/h2&gt;
&lt;p&gt;　&lt;code&gt;GRP&lt;/code&gt;とは簡単に言ってしまえば「&lt;strong&gt;ベイズ推定を用いた非線形回帰手法の１種&lt;/strong&gt;」です。モデル自体は線形ですが、&lt;strong&gt;カーネルトリックを用いて入力変数を無限個非線形変換したもの&lt;/strong&gt;を説明変数として推定できるところが特徴です（カーネルになにを選択するかによります）。&lt;br /&gt;
　&lt;code&gt;GPR&lt;/code&gt;が想定しているのは、学習データとして入力データと教師データがそれぞれN個得られており、また入力データに関しては&lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;個目のデータも得られている状況です。この状況から、&lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;個目の教師データを予測します。&lt;br /&gt;
　教師データにはノイズが含まれており、以下のような確率モデルに従います。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
t_{i} = y_{i} + \epsilon_{i}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の観測可能な教師データ（スカラー）、&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;は観測できない出力データ（スカラー）、&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{i}\)&lt;/span&gt;は測定誤差で正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(0,\beta^{-1})\)&lt;/span&gt;に従います。&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;は以下のような確率モデルに従います。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(x_{i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt;はi番目の入力データベクトル、&lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt;は非線形関数、 &lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}^{T}\)&lt;/span&gt;は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\)&lt;/span&gt;を想定しています（&lt;span class=&#34;math inline&#34;&gt;\(x_{1,i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の入力データ&lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt;の１番目の変数）。教師データの確率モデルから、&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の出力データ&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;が得られたうえで&lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt;が得られる条件付確率は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(t_{i}|y_{i}) = N(t_{i}|y_{i},\beta^{-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t} = (t_{1},...,t_{n})^{T}\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y} = (y_{1},...,y_{n})^{T}\)&lt;/span&gt;とすると、上式を拡張することで&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{t}|\textbf{y}) = N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;と書けます。また、事前分布として&lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}\)&lt;/span&gt;の期待値は0、分散は全て&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;と仮定します。&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;はガウス過程に従うと仮定します。ガウス過程とは、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;の同時分布が多変量ガウス分布に従うもののことです。コードで書くと以下のようになります。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define Kernel function
Kernel_Mat &amp;lt;- function(X,sigma,beta){
  N &amp;lt;- NROW(X)
  K &amp;lt;- matrix(0,N,N)
  for (i in 1:N) {
    for (k in 1:N) {
      if(i==k) kdelta = 1 else kdelta = 0
      K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta
    }
  }
  return(K)
}

N &amp;lt;- 10 # max value of X
M &amp;lt;- 1000 # sample size
X &amp;lt;- matrix(seq(1,N,length=M),M,1) # create X
testK &amp;lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix

library(MASS)

P &amp;lt;- 6 # num of sample path
Y &amp;lt;- matrix(0,M,P) # define Y

for(i in 1:P){
  Y[,i] &amp;lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y
}

# Plot
matplot(x=X,y=Y,type = &amp;quot;l&amp;quot;,lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ayatoashihara.github.io/myblog_jp/myblog_jp/post/post1/post1_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;　Kernel_Matについては後述しますが、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;の各要素&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt;の間の共分散行列&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;を入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;からカーネル法を用いて計算しています。そして、この&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;と平均0から、多変量正規乱数を6系列生成し、それをプロットしています。&lt;/p&gt;
&lt;p&gt;　これらの系列は共分散行列から計算されるので、&lt;strong&gt;各要素の共分散が正に大きくなればなるほど同じ値をとりやすくなる&lt;/strong&gt;ようモデリングされていることになります。また、グラフを見ればわかるように非常になめらかなグラフが生成されており、かつ非常に柔軟な関数を表現できていることがわかります。コードでは計算コストの関係上、入力を0から10に限定して1000個の入力点をサンプルし、作図を行っていますが、原理的には&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;は実数空間で定義されるものであるので、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;は無限次元の多変量正規分布に従います。
以上のように、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;はガウス過程に従うと仮定するので同時確率&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;は平均0、分散共分散行列が&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;の多変量正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(\textbf{y}|0,K)\)&lt;/span&gt;に従います。ここで、&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;の各要素&lt;span class=&#34;math inline&#34;&gt;\(K_{i,j}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
K_{i,j} &amp;amp;=&amp;amp; cov[y_{i},y_{j}] = cov[\textbf{w}\phi(x_{i}),\textbf{w}\phi(x_{j})] \\
&amp;amp;=&amp;amp;\phi(x_{i})\phi(x_{j})cov[\textbf{w},\textbf{w}]=\phi(x_{i})\phi(x_{j})\alpha
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt;の&lt;strong&gt;次元が大きくなればなるほど計算量が多く&lt;/strong&gt;なります（つまり、非線形変換をかければかけるほど計算が終わらない）。しかし、カーネル関数&lt;span class=&#34;math inline&#34;&gt;\(k(x,x&amp;#39;)\)&lt;/span&gt;を用いると、計算量は高々入力データ&lt;span class=&#34;math inline&#34;&gt;\(x_{i},x_{j}\)&lt;/span&gt;のサンプルサイズの次元になるので、計算がしやすくなります。カーネル関数を用いて&lt;span class=&#34;math inline&#34;&gt;\(K_{i,j} = k(x_{i},x_{j})\)&lt;/span&gt;となります。カーネル関数としてはいくつか種類がありますが、以下のガウスカーネルがよく使用されます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
k(x,x&amp;#39;) = a \exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;の同時確率が定義できたので、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t}\)&lt;/span&gt;の同時確率を求めることができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\displaystyle p(\textbf{t}) &amp;amp;=&amp;amp; \int p(\textbf{t}|\textbf{y})p(\textbf{y}) d\textbf{y} \\
 \displaystyle &amp;amp;=&amp;amp; \int N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})N(\textbf{y}|0,K)d\textbf{y} \\
 &amp;amp;=&amp;amp; N(\textbf{y}|0,\textbf{C}_{N})
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N} = K + \beta^{-1}\textbf{I}_{N}\)&lt;/span&gt;です。なお、最後の式展開は正規分布の再生性を利用しています（証明は正規分布の積率母関数から容易に導けます）。要は、両者は独立なので共分散は2つの分布の共分散の和となると言っているだけです。個人的には、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;が先ほど説明したガウス過程の事前分布であり、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}|\textbf{y})\)&lt;/span&gt;が尤度関数で、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t})\)&lt;/span&gt;は事後分布をというようなイメージです。事前分布&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;は制約の緩い分布でなめらかであることのみが唯一の制約です。
&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の観測可能な教師データ&lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt;の同時確率は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(\textbf{t},t_{N+1}) = N(\textbf{t},t_{N+1}|0,\textbf{C}_{N+1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N+1}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \textbf{C}_{N+1} = \left(
    \begin{array}{cccc}
      \textbf{C}_{N} &amp;amp; \textbf{k} \\
      \textbf{k}^{T} &amp;amp; c \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(c = k(x_{N+1},x_{N+1})\)&lt;/span&gt;です。&lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt;の同時分布から条件付分布&lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt;を求めることができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(t_{N+1}|\textbf{t}) = N(t_{N+1}|\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{t},c-\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{k})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;条件付分布の計算においては、&lt;a href=&#34;https://qiita.com/kilometer/items/34249479dc2ac3af5706:title&#34;&gt;条件付多変量正規分布の性質&lt;/a&gt;を利用しています。上式を見ればわかるように、条件付分布&lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;個の入力データ、&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の教師データ、カーネル関数のパラメータ&lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt;が既知であれば計算可能となっていますので、任意の点を入力データとして与えてやれば、元のData Generating Processを近似することが可能になります。&lt;code&gt;GPR&lt;/code&gt;の良いところは上で定義した確率モデル&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt;を直接推定しなくても予測値が得られるところです。確率モデルには&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt;があり、非線形変換により入力データを高次元ベクトルへ変換しています。よって、次元が高くなればなるほど&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt;の計算量は大きくなっていきますが、&lt;code&gt;GPR&lt;/code&gt;ではカーネルトリックを用いているので高々入力データベクトルのサンプルサイズの次元の計算量で事足りることになります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gprの実装&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. &lt;code&gt;GPR&lt;/code&gt;の実装&lt;/h2&gt;
&lt;p&gt;　とりあえずここまでを&lt;code&gt;R&lt;/code&gt;で実装してみましょう。PRMLのテストデータで実装しているものがあったので、それをベースにいじってみました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(grid)

# 1.Gaussian Process Regression

# PRML&amp;#39;s synthetic data set
curve_fitting &amp;lt;- data.frame(
  x=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000),
  t=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502))

f &amp;lt;- function(beta, sigma, xmin, xmax, input, train) {
  kernel &amp;lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function
  K &amp;lt;- outer(input, input, kernel); # calc gram matrix
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean 
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))
print(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2))
print(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ayatoashihara.github.io/myblog_jp/myblog_jp/post/post1/post1_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta^{-1}\)&lt;/span&gt;は測定誤差を表しています。&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が大きい（つまり、測定誤差が小さい）とすでに得られているデータとの誤差が少なくなるように予測値をはじき出すので、over fitting しやすくなります。&lt;/strong&gt;上図の左上がそうなっています。左上は&lt;span class=&#34;math inline&#34;&gt;\(\beta=400\)&lt;/span&gt;で、現時点で得られているデータに過度にfitしていることがわかります。逆に&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が小さいと教師データとの誤差を無視するように予測値をはじき出しますが、汎化性能は向上するかもしれません。右上の図がそれです。&lt;span class=&#34;math inline&#34;&gt;\(\beta=4\)&lt;/span&gt;で、得られているデータ点を平均はほとんど通っていません。&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;は現時点で得られているデータが周りに及ぼす影響の広さを表しています。&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;が小さいと、隣接する点が互いに強く影響を及ぼし合うため、精度は下がるが汎化性能は上がるかもしれません。逆に、&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;が大きいと、個々の点にのみフィットする不自然な結果になります。これは右下の図になります（&lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.03},\beta=25\)&lt;/span&gt;）。御覧の通り、&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が大きいのでoverfitting気味であり、なおかつ&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;も大きいので個々の点のみにfitし、無茶苦茶なグラフになっています。左下のグラフが最もよさそうです。&lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.3},\beta=2\)&lt;/span&gt;となっています。試しに、このグラフのx区間を[0,2]へ伸ばしてみましょう。すると、以下のようなグラフがかけます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) 
print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) 
print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ayatoashihara.github.io/myblog_jp/myblog_jp/post/post1/post1_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;これを見ればわかるように、左下以外のグラフはすぐに95%信頼区間のバンドが広がり、データ点がないところではまったく使い物にならないことがわかります。一方、左下のグラフは1.3~1.4ぐらいまではそこそこのバンドがかけており、我々が直感的に理解する関数とも整合的な点を平均値が通っているように思えます。また、観測可能なデータ点から離れすぎるとパラメータに何を与えようと平均０、分散１の正規分布になることもわかるがわかります。
さて、このようにパラメータの値に応じて、アウトサンプルの予測精度が異なることを示したわけですが、ここで問題となるのはこれらハイパーパラメータをどのようにして推計するかです。これは対数尤度関数&lt;span class=&#34;math inline&#34;&gt;\(\ln p(\textbf{t}|a,b)\)&lt;/span&gt;を最大にするハイパーパラメータを勾配法により求めます((&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;は少しタイプが異なるようで、発展的な議論では他のチューニング方法をとる模様。まだ、そのレベルにはいけていないのでここではカリブレートすることにします。))。&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}) = N(\textbf{y}|0,\textbf{C}_{N})\)&lt;/span&gt;なので、対数尤度関数は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \ln p(\textbf{t}|a,b,\beta) = -\frac{1}{2}\ln|\textbf{C}_{N}| - \frac{N}{2}\ln(2\pi) - \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}\textbf{k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。あとは、これをパラメータで微分し、得られた連立方程式を解くことで最尤推定量が得られます。ではまず導関数を導出してみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial}{\partial \theta_{i}} \ln p(\textbf{t}|\theta) = -\frac{1}{2}Tr(\textbf{C}_{N}^{-1}\frac{\partial \textbf{C}_{N}}{\partial \theta_{i}}) + \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}
\frac{\partial\textbf{C}_{N}}{\partial\theta_{i}}\textbf{C}_{N}^{-1}\textbf{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;はパラメータセットで、&lt;span class=&#34;math inline&#34;&gt;\(\theta_{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目のパラメータを表しています。この導関数が理解できない方は&lt;a href=&#34;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf:title=PRML&#34;&gt;こちら&lt;/a&gt;の補論にある(C.21)式と(C.22)式をご覧になると良いと思います。今回はガウスカーネルを用いているため、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial a} = \exp(-b(x-x&amp;#39;)^{2}) \\
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial b} = -a(x-x&amp;#39;)^{2}\exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;を上式に代入すれば良いだけです。ただ、今回は勾配法により最適なパラメータを求めます。以下、実装のコードです（かなり迷走しています）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- function(xmin, xmax, input, train){
  # initial value
  beta = 100
  b = 1
  a = 1
  learning_rate = 0.1
  itermax &amp;lt;- 1000
  if (class(input) == &amp;quot;numeric&amp;quot;){
    N &amp;lt;- length(input)
  } else
  {
    N &amp;lt;- NROW(input)
  }
  kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel
  derivative_a &amp;lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2)
  derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
  dloglik_a &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y 
  }
  dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
  }
  # loglikelihood function
  likelihood &amp;lt;- function(b,a,x,y){
    kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2)
    K &amp;lt;- outer(x, x, kernel)
    C_N &amp;lt;- K + diag(N)/beta
    itermax &amp;lt;- 1000
    l &amp;lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y
    return(l)
  }
  K &amp;lt;- outer(input, input, kernel) 
  C_N &amp;lt;- K + diag(N)/beta
  for (i in 1:itermax){
    kernel &amp;lt;- function(x1, x2) a*exp(-b*(x1-x2)^2)
    derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
    dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
      -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
    }
    K &amp;lt;- outer(input, input, kernel) # calc gram matrix
    C_N &amp;lt;- K + diag(N)/beta
    l &amp;lt;- 0
    if(abs(l-likelihood(b,a,input,train))&amp;lt;0.0001&amp;amp;i&amp;gt;2){
      break
    }else{
      a &amp;lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input))
      b &amp;lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input))
    }
    l &amp;lt;- likelihood(b,a,input,train)
  }
  K &amp;lt;- outer(input, input, kernel)
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train)  
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel))))
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

print(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ayatoashihara.github.io/myblog_jp/myblog_jp/post/post1/post1_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;たしかに、良さそうな感じがします（笑）
とりあえず、今日はここまで。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BVARについて</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post5/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post5/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;　おはこんばんにちは。日次GDP推計を休日に進めているのですが、今日は少し勉強編で&lt;code&gt;BVAR&lt;/code&gt;についての記事を書きたいと思います。この&lt;code&gt;BVAR&lt;/code&gt;はFRBアトランタ連銀の&lt;code&gt;GDPNow&lt;/code&gt;でも使用されていることから、日次GDP推計との親和性も高いと思われます。そもそも、時系列でアウトサンプルの予測精度を上げたいということになると真っ先に思いつくのが&lt;code&gt;BVAR&lt;/code&gt;です。Doan, Litterman and Sims(1984)で提案されたこのモデルは予測精度が良いので、非常に有効な手段になると思われます。&lt;code&gt;BVAR&lt;/code&gt;は&lt;code&gt;Bayesian Vector Autoregression&lt;/code&gt;の略で、ベクトル自己回帰モデル（&lt;code&gt;VAR&lt;/code&gt;）の派生版です。&lt;code&gt;VAR&lt;/code&gt;とネットで調べるとまず&lt;code&gt;Value at Risk&lt;/code&gt;（&lt;code&gt;VaR&lt;/code&gt;）が出てくると思いますが、それとは違います。よく見るとaが小文字になっていることに気づくかと思います。
　さて、&lt;code&gt;BVAR&lt;/code&gt;の説明をこれから行おうとするのですが、その前にまず基本的な&lt;code&gt;VAR&lt;/code&gt;の説明からしたいと思います。ただし、歴史的な背景（大型マクロ計量モデルからの経緯など）には触れません。あくまで、&lt;code&gt;BVAR&lt;/code&gt;を説明するうえで必要な知識について触れたいと思います。&lt;/p&gt;
&lt;div id=&#34;unrestricted-varについて&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Unrestricted VARについて&lt;/h2&gt;
&lt;p&gt;　まず、注意点を一点。この投稿では、もっとも基本的な&lt;code&gt;VAR&lt;/code&gt;のことをUnrestricted VAR（UVAR）と呼ぶことにします。UVARはSims(1980)の論文が有名です。&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;このモデルには、理論的な基礎づけは原則ありません。あくまで実証的なモデルです。UVARは一般系は以下のような形をしています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = A_{0} + A_{1}Y_{t-1} + ... + A_{K}Y_{t-K} + U_{t}, ~~ U_{t} ～ N(0,\Omega)
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = \left(
    \begin{array}{cccc}
      y_{1,t} \\
      y_{2,t} \\
      \vdots  \\
      y_{J,t} \\
    \end{array}
  \right),
       A_{0} = \left(
    \begin{array}{cccc}
      a_{10} \\
      a_{20} \\
      \vdots  \\
      a_{J0} \\
    \end{array}
  \right),
      A_{k} = \left(
    \begin{array}{cccc}
      a_{11,k} &amp;amp; a_{12,k} &amp;amp; \ldots &amp;amp; a_{1J,k} \\
      a_{21,k} &amp;amp; a_{22,k} &amp;amp; \ldots &amp;amp; a_{2J,k} \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      a_{J1,k} &amp;amp; a_{J2,k} &amp;amp; \ldots &amp;amp; a_{JJ,k}
    \end{array}
  \right),
    U_{t} = \left(
    \begin{array}{cccc}
      u_{1,t} \\
      u_{2,t} \\
      \vdots  \\
      u_{J,t} \\
    \end{array}
  \right)
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;は時点、&lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;は変数の数、&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;はラグ数を表しています。上式を見ると、UVARは自己回帰＋他変数のラグでt期の変数&lt;span class=&#34;math inline&#34;&gt;\(y_{j,t}\)&lt;/span&gt;を説明しようとするモデルであると言えます。しばしば、経済の実証分析で使用され、インサンプルの当てはまりが良いことも知られています（GDP、消費、投資、金利、マネーサプライの５変数&lt;code&gt;VAR&lt;/code&gt;で金融政策の波及経路を分析したり･･･）。推定するパラメータの個数は、回帰式1本だけでJK+1個（定数項込み）の係数を含むので、J本になればJ(JK+1)個になります。また、&lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;がJ(J+1)/2個のパラメータを持っているので、合計J(JK+1)+J(J+1)/2個のパラメータを推定することになり、かなりパラメータ数が多い印象です（これは後々重要になってきます）。具体的な推計方法ですが、UVARは同時方程式体系ではないのでそこまで面倒ではありません。UVAR自体は&lt;code&gt;Seemingly Unrestricted Regression Equation&lt;/code&gt;（SUR）の一種でそれぞれの方程式は誤差項の相関を通じて関係してはいますが（&lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;の部分）、全ての回帰式が同じ説明変数を持つため、各方程式を最小二乗法（&lt;code&gt;OLS&lt;/code&gt;）によって推定するだけで良いことが知られています。
　この事実を説明してみましょう（&lt;code&gt;BVAR&lt;/code&gt;が気になる方は読み飛ばしてもらって構いません）。説明のために今、UVARをSURの一般系に書き直します。上式はt期の&lt;code&gt;VAR&lt;/code&gt;(K)システムですが、UVARを推定する際はこれらJ本の方程式がサンプル数Tセット分存在するので、実際のシステム体系は以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
   \left(
    \begin{array}{cccc}
      Y_{1} \\
      Y_{2} \\
      \vdots  \\
      Y_{J} \\
    \end{array}
  \right) = 
 \left(
    \begin{array}{cccc}
      X_{1} &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; X_{2} &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; X_{J}
    \end{array}
  \right) 
  \left(
    \begin{array}{cccc}
      A_{1} \\
      A_{2} \\
      \vdots  \\
      A_{J} \\
    \end{array}
  \right) +
  \left(
    \begin{array}{cccc}
      U_{1} \\
      U_{2} \\
      \vdots  \\
      U_{J} \\
    \end{array}
  \right) = \overline{X}A + U (1式)
\]&lt;/span&gt;
ここで、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{j} = \left(
    \begin{array}{cccc}
      y_{t,j} \\
      y_{t+1,j} \\
      \vdots  \\
      y_{T,j} \\
    \end{array}
  \right) ,
X_{j} = X = \left(
    \begin{array}{cccc}
      1 &amp;amp; y_{t-1,1} &amp;amp; \ldots &amp;amp; y_{t-K,1} &amp;amp; \ldots &amp;amp; y_{t-K,J} \\
      1 &amp;amp; y_{t,1} &amp;amp; \ldots &amp;amp; y_{t-K+1,1} &amp;amp; \ldots &amp;amp; y_{t-K+1,J} \\
      \vdots &amp;amp; \vdots &amp;amp; \ldots &amp;amp; \ldots &amp;amp; \ddots &amp;amp; \vdots \\
      1 &amp;amp; y_{T-1,1} &amp;amp; \ldots &amp;amp; y_{T-K,1} &amp;amp; \ldots &amp;amp; y_{T-K,J} \\
    \end{array}
  \right),
 A_{j} = \left(
    \begin{array}{cccc}
      a_{00,j} \\
      a_{11,j} \\
      \vdots  \\
      a_{JK,j} \\
    \end{array}
  \right)
\]&lt;/span&gt;
です。上式とは違い、変数順で並べられていることに注意してください（つまり、各方程式を並べる優先順位は１番目にj、２番目にtとなっている）。また、&lt;span class=&#34;math inline&#34;&gt;\(X_{j}\)&lt;/span&gt;が全ての変数&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;について等しいことに注目してください（jに依存していません、&lt;code&gt;VAR&lt;/code&gt;なので当たり前ですが）。これが後々非常に重要になってきます。&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;の分散共分散行列は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[UU&amp;#39;|X_{1}, X_{2},..,X_{J}] = \Omega = 
\left(
    \begin{array}{cccc}
      \sigma_{11}I &amp;amp; \sigma_{12}I &amp;amp; \ldots &amp;amp; \sigma_{1J}I \\
      \sigma_{21}I &amp;amp; \sigma_{22}I &amp;amp; \ldots &amp;amp; \sigma_{2J}I \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      \sigma_{J1}I &amp;amp; \sigma_{J2}I &amp;amp; \ldots &amp;amp; \sigma_{JJ}I \\
    \end{array}
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となっており、それぞれの変数は誤差項の相関を通じて関係しています（ただし、同じ変数内の異なる時点間の相関はないと仮定します）。このような場合、一般化最小二乗法（&lt;code&gt;GLS&lt;/code&gt;）を用いて推計を行うことになりますが、これら方程式体系において説明変数が同じであるならば、&lt;code&gt;GLS&lt;/code&gt;推定量と&lt;code&gt;OLS&lt;/code&gt;推定量は同値になります。それを確かめてみましょう。上述した方程式体系(1)の&lt;code&gt;GLS&lt;/code&gt;推定量は以下になります（&lt;a href=&#34;http://user.keio.ac.jp/~nagakura/zemi/GLS.pdf&#34;&gt;参考&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A_{GLS} = (\overline{X}&amp;#39;\Omega^{-1}\overline{X})^{-1}\overline{X}&amp;#39;\Omega^{-1}Y = (\overline{X}&amp;#39;(\Sigma^{-1}\otimes I)\overline{X})^{-1}\overline{X}&amp;#39;(\Sigma^{-1}\otimes I)Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;は誤差項の分散共分散行列のうち、スカラーである分散、共分散を取り出した行列です。先ほど確認したように、&lt;span class=&#34;math inline&#34;&gt;\(X_{1}=X_{2}=...=X_{J}=X\)&lt;/span&gt;なので&lt;span class=&#34;math inline&#34;&gt;\(\overline{X}=I \otimes X\)&lt;/span&gt;であり、その転置も&lt;span class=&#34;math inline&#34;&gt;\(\overline{X}&amp;#39;=I \otimes X\)&lt;/span&gt;となります。よって、&lt;a href=&#34;https://mathwords.net/kuronekaseki&#34;&gt;この&lt;/a&gt;の性質を用いると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
A_{GLS} &amp;amp;=&amp;amp; [(I \otimes X&amp;#39;)(\Sigma^{-1}\otimes I)(I \otimes X)]^{-1}(I \otimes X&amp;#39;)(\Sigma^{-1}\otimes I)Y \\
&amp;amp;=&amp;amp; [(\Sigma^{-1}\otimes X&amp;#39;)(I \otimes X)]^{-1}(\Sigma^{-1}\otimes X&amp;#39;)y \\
&amp;amp;=&amp;amp; (\Sigma^{-1}\otimes(X&amp;#39;X))^{-1}(\Sigma^{-1}\otimes X&amp;#39;)y \\
&amp;amp;=&amp;amp; (I \otimes (X&amp;#39;X)^{-1}X&amp;#39;)y \\
&amp;amp;=&amp;amp; (\overline{X}&amp;#39;\overline{X})^{-1}\overline{X}&amp;#39;Y \\
&amp;amp;=&amp;amp;  \left(
    \begin{array}{cccc}
      (X&amp;#39;X)^{-1}X&amp;#39; &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; (X&amp;#39;X)^{-1}X&amp;#39; &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; (X&amp;#39;X)^{-1}X&amp;#39;
    \end{array}
  \right) 
\left(
    \begin{array}{cccc}
      Y_{1} \\
      Y_{2} \\
      \vdots  \\
      Y_{J} \\
    \end{array}
  \right)\\ 
&amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      (X&amp;#39;X)^{-1}X&amp;#39;Y_{1} \\
      (X&amp;#39;X)^{-1}X&amp;#39;Y_{2} \\
      \vdots  \\
      (X&amp;#39;X)^{-1}X&amp;#39;Y_{J} \\
    \end{array}
  \right) \\
&amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      A_{1,OLS} \\
      A_{2,OLS} \\
      \vdots  \\
      A_{J,OLS} \\
    \end{array}
  \right)
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となり、各経済変数の方程式を別個に&lt;code&gt;OLS&lt;/code&gt;推計していけばよいことがわかります。
　&lt;code&gt;OLS&lt;/code&gt;推定に際し、&lt;code&gt;VAR&lt;/code&gt;の次数Kの選択を選択する必要がありますが、次数Kは&lt;code&gt;AIC&lt;/code&gt;（&lt;code&gt;BIC&lt;/code&gt;）を評価軸に探索的に決定します。つまり、いろいろな値をKに設定し、&lt;code&gt;OLS&lt;/code&gt;推計を行い、計算された&lt;code&gt;VAR&lt;/code&gt;(K)の&lt;code&gt;AIC&lt;/code&gt;(&lt;code&gt;BIC&lt;/code&gt;)のうちで最も値が大きいモデルの次数を真のモデルの次数Kとして採用するということです。&lt;code&gt;R&lt;/code&gt;にも&lt;code&gt;VARselect()&lt;/code&gt;という関数があり、引数にラグの探索最大数とデータを渡すことで最適な次数を計算してくれます（便利）。
　このようにUVARは&lt;code&gt;OLS&lt;/code&gt;推計で各変数間の相互依存関係をデータから推計できる手軽な手法です。私が知っている分野ですと財政政策乗数の推計に使用されていました。GDP、消費、投資、政府支出の４変数で&lt;code&gt;VAR&lt;/code&gt;を推定し、推定した&lt;code&gt;VAR&lt;/code&gt;でインパルス応答を見ることで１単位の財政支出の増加がGDP等に与える影響を定量的にシミュレートすることができたりします（指導教官が論文を書いてました）。そもそも私の専門の&lt;code&gt;DSGE&lt;/code&gt;も誘導系に書き直せば&lt;code&gt;VAR&lt;/code&gt;形式になり、インパルス応答などは基本的に一緒です。また、経済変数は慣性が強いので（特に我が国の場合）、インサンプルのモデルの当てはまりもいいです。ただし、あくまでインサンプルです。&lt;b&gt;アウトサンプルの当てはまりはそれほど良い印象はありません。&lt;/b&gt;なぜなら、推定パラメータが多すぎるからです。UVARの予測に関する問題点は&lt;code&gt;over-parametrization&lt;/code&gt;です。例えば、先ほどの４変数UVARでラグが６期だったとすると、推定すべきパラメータは３１個になります。よって、データ数にもよりますが、パラメータ数がデータ数に近づくとインサンプルの補間に近づき、過学習を引き起こす危険性があります。日次GDP推計は大量の変数を使用するのでこの問題は非常に致命的になります。&lt;code&gt;BVAR&lt;/code&gt;はこの問題を解決することに主眼を置いています。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bvarについて&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. BVARについて&lt;/h2&gt;
&lt;p&gt;　UVARの問題点は&lt;code&gt;over-parametrization&lt;/code&gt;であると述べました。&lt;code&gt;BVAR&lt;/code&gt;はこの問題を防ぐために不必要な説明変数（のパラメータ）をそぎ落とそうとします。ただ、不要なパラメータを推定する前に０と仮置き（カリブレート）するのではなく、１階の自己に関わるパラメータは１周り、その他変数のパラメータは０周りに正規分布するという形の制約を与えます。このモデルの最大の仮定は「各経済変数は多かれ少なかれドリフト付き１階のランダムウォークに従う」というものであり、上述したような事前分布を先験的に与えた上で推定を行うのです。砕けた言い方をすると、&lt;code&gt;BVAR&lt;/code&gt;の考え方は「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」というものなんです。さて、前置きが長くなりましたが、具体的な説明に移りたいと思います。&lt;/p&gt;
&lt;div id=&#34;具体的な推定方法カルマンフィルタ&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;具体的な推定方法（カルマンフィルタ）&lt;/h3&gt;
&lt;p&gt;　上述した事前分布に加えて、&lt;code&gt;BVAR&lt;/code&gt;がUVARと異なる点はパラメータが&lt;code&gt;time-varying&lt;/code&gt;であるということです。なんとなく、パラメータがずっと固定よりもサンプルが増えるたびにその値が更新されるほうが予測精度が上がりそうですよね（笑）。推定手法としてはUVARの時のように&lt;code&gt;OLS&lt;/code&gt;をそれぞれにかけることはせず、&lt;a href=&#34;https://qiita.com/MoriKen/items/0c80ef75749977767b43&#34;&gt;カルマンフィルタ&lt;/a&gt; と呼ばれるアルゴリズムを用いて推定を行います。&lt;code&gt;BVAR&lt;/code&gt;は以下のような状態空間モデルとして定義されます（各ベクトル、行列の次元はUVAR時と同じです）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = X_{t}B_{t} + u_{t} \\
B_{t} = \Phi B_{t-1} + \epsilon_{t} \\
B_{t} = \left(
    \begin{array}{cccc}
      \beta_{00,t} \\
      \beta_{11,t} \\
      \vdots  \\
      \beta_{JK,t} \\
    \end{array}
  \right), 
\Phi = \left(
    \begin{array}{cccc}
      \phi_{00} &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; \phi_{11} &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; \phi_{JK}
    \end{array}
  \right),
u_{t} = \left(
    \begin{array}{cccc}
      u_{1,t} \\
      u_{2,t} \\
      \vdots  \\
      u_{J,t} \\
    \end{array}
  \right),
\epsilon_{t} = \left(
    \begin{array}{cccc}
      \epsilon_{00,t} \\
      \epsilon_{11,t} \\
      \vdots  \\
      \epsilon_{JK,t} \\
    \end{array}
  \right)
\]&lt;/span&gt;
状態空間モデルは観測可能なデータ（ex.経済統計）を用いて、観測不可能なデータ（ex.リスクプレミアムや限界消費性向等）を推定します。観測可能なデータと不可能なデータを関連付ける方程式を観測方程式、観測不可能なデータの挙動をモデル化した方程式を遷移方程式と呼びます。ここでは、1本目が観測方程式、2本目が遷移方程式となります。御覧の通り、観測方程式は通常の&lt;code&gt;VAR&lt;/code&gt;の形をしている一方、遷移方程式は&lt;code&gt;AR(1)&lt;/code&gt;となっており、これによってパラメータ&lt;span class=&#34;math inline&#34;&gt;\(B_{t}\)&lt;/span&gt;は過去の値を引きずりながら&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;によって確率的に変動します。&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;は自己回帰係数です。誤差項&lt;span class=&#34;math inline&#34;&gt;\(u_{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;は平均０の正規分布に従い、その分散は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
Var(u_{t}) &amp;amp;=&amp;amp; \sigma_{u}^{2} \\
Var(\epsilon_{t}) &amp;amp;=&amp;amp; \sigma_{\epsilon}^{2}R
\end{eqnarray}
\]&lt;/span&gt;
で与えられます。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\Phi,R,\sigma_{u}^{2},\sigma_{\epsilon}^{2}\)&lt;/span&gt;は既知であるとします。今、t-1期までのデータが入手可能であるとすると、そのデータをカルマンフィルタアルゴリズムで推定した&lt;span class=&#34;math inline&#34;&gt;\(B_{t-1|t-1}\)&lt;/span&gt;とその分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(P_{t-1|t-1}\)&lt;/span&gt;を用いて、t期の予想値を以下のように計算します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
B_{t|t-1} &amp;amp;=&amp;amp; \Phi B_{t-1|t-1} \\
P_{t|t-1} &amp;amp;=&amp;amp; \Phi P_{t|t-1} \Phi&amp;#39; + \sigma_{\epsilon}^{2}R
\end{eqnarray}
\]&lt;/span&gt;
観測可能な変数の予測値はこの値を用いて計算します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{Y_{t}} = X_{t} B_{t|t-1}
\]&lt;/span&gt;
次にt期の観測値が得られると次の更新方程式を用いて&lt;span class=&#34;math inline&#34;&gt;\(B_{t|t}, P_{t|t}\)&lt;/span&gt;を計算します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
B_{t|t} &amp;amp;=&amp;amp; B_{t|t-1} + P_{t|t-1}X_{t}&amp;#39;(X_{t}P_{t|t-1}X_{t}&amp;#39; + \sigma_{u}^{2})^{-1}(Y_{t}-X_{t}B_{t|t-1}) \\
P_{t|t} &amp;amp;=&amp;amp; P_{t|t-1} + P_{t|t-1}X_{t}&amp;#39;(X_{t}P_{t|t-1}X_{t}&amp;#39; + \sigma_{u}^{2})^{-1}X_{t}P_{t|t-1}
\end{eqnarray}
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\epsilon}^{2}R = 0\)&lt;/span&gt;かつ&lt;span class=&#34;math inline&#34;&gt;\(\Phi = I\)&lt;/span&gt;の場合は&lt;code&gt;逐次最小二乗法&lt;/code&gt;に一致します。要は入手できるサンプル増えるたびに&lt;code&gt;OLS&lt;/code&gt;をやり直していくことと同値だということです。こうして推計を行うのが&lt;code&gt;BVAR&lt;/code&gt;なのですが、カルマンフィルタは漸化式なので初期値&lt;span class=&#34;math inline&#34;&gt;\(B_{0|0}, P_{0|0}\)&lt;/span&gt;を決めてやる必要があります。&lt;code&gt;BVAR&lt;/code&gt;の２つ目の特徴は初期値の計算に混合推定法を用いているところであり、ここに前述した事前分布が関係してきます。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;カルマンフィルタの初期値をどのようにきめるか&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;カルマンフィルタの初期値をどのようにきめるか&lt;/h3&gt;
&lt;p&gt;初期値&lt;span class=&#34;math inline&#34;&gt;\(B_{0|0}, P_{0|0}\)&lt;/span&gt;をどうやって計算するのかを考えた際にすぐ思いつく方法としては、カルマンフィルタのスタート地点tの前に、初期値推計期間をある程度用意し、&lt;span class=&#34;math inline&#34;&gt;\(Y = XB + \epsilon\)&lt;/span&gt;で&lt;span class=&#34;math inline&#34;&gt;\(B_{0|0}, P_{0|0}\)&lt;/span&gt;を推定する方法があります。つまり、観測方程式を&lt;code&gt;GLS&lt;/code&gt;推計し、そのパラメータを初期値とする方法です。その際の&lt;code&gt;GLS&lt;/code&gt;推定量は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{B} = (X&amp;#39;V^{-1}X)^{-1}X&amp;#39;V^{-1}Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。これでもいいんですが、これではただ時変パラメータを推計しているだけで先ほど述べた&lt;code&gt;over-parametrization&lt;/code&gt;の問題にはアプローチできていません。そこで、ここではパラメータ&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;に対してなにか先験的な情報が得られているとしましょう。つまり、先述した「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」という予想です。これを定式化してみましょう。つまり、パラメータ&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;に対して以下の制約式を課します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r = RB + \nu
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(E(\nu) = 0,Var(\nu) = V_{0}\)&lt;/span&gt;です。また、&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(RB\)&lt;/span&gt;の予想値であり、&lt;span class=&#34;math inline&#34;&gt;\(V_{0}\)&lt;/span&gt;はその予想値の周りでのばらつきを表しています。取っ付きにくいかもしれませんが、&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;は１階自己回帰係数に関わる部分は1、それ以外は0となるベクトルで、&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;は単位行列&lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;だと思ってもらえばいいです。つまり、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\left(
    \begin{array}{cccc}
      0 \\
      1 \\
      0 \\
      \vdots  \\
      1 \\
      \vdots  \\
      0 \\
    \end{array}
  \right)
 &amp;amp;=&amp;amp; 
\left(
    \begin{array}{cccc}
　  \beta_{0,1}^{1} \\
      \beta_{1,1}^{1} \\
      \beta_{2,1}^{1} \\
      \vdots  \\
      \beta_{j,1}^{j} \\
      \vdots  \\
      \beta_{J,K}^{J} \\
    \end{array}
  \right)
+
\left(
    \begin{array}{cccc}
      \nu_{0,1}^{1} \\
      \nu_{1,1}^{1} \\
      \nu_{2,1}^{1}  \\
\vdots  \\
      \nu_{j,1}^{j}  \\
\vdots  \\
      \nu_{J,K}^{J} \\
    \end{array}
  \right)
\end{eqnarray}
\]&lt;/span&gt;
みたいな感じです。ここで&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{i}\)&lt;/span&gt;はi番目の方程式のj番目の変数のk次ラグにかかるパラメータを表しています。混合推定では、正規分布に従う観測値とこの事前分布が独立であるという仮定の下で観測方程式&lt;span class=&#34;math inline&#34;&gt;\(Y = XB + \epsilon\)&lt;/span&gt;と$ r = RB + $を以下のように組み合わせます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left(
    \begin{array}{cccc}
      Y \\
      r 
    \end{array}
  \right) = \left(
    \begin{array}{cccc}
      X \\
      R 
    \end{array}
  \right)B + 
\left(
    \begin{array}{cccc}
      \epsilon \\
      \nu 
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E\left(
    \begin{array}{cccc}
      \epsilon \\
      \nu 
    \end{array}
  \right) = 0 \\
Var\left(
    \begin{array}{cccc}
      \epsilon \\
      \nu 
    \end{array}
  \right) = \left(
    \begin{array}{cccc}
      \sigma^{2}V &amp;amp; 0 \\
      0 &amp;amp; V_{0} \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;そして、このシステム体系を&lt;code&gt;GLS&lt;/code&gt;で推計するのです。こうすることで、事前分布を考慮した初期値の推定を行うことができます。&lt;code&gt;GLS&lt;/code&gt;推定量は以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \hat{B}_{M} = (\frac{1}{\sigma^{2}}X&amp;#39;V^{-1}X + R&amp;#39;V_{0}^{-1}R)^{-1}(\frac{1}{\sigma^{2}}X&amp;#39;V^{-1}y + R&amp;#39;V_{0}^{-1}r)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ご覧になればわかるように、&lt;code&gt;GLS&lt;/code&gt;推定量は上記2本の連立方程式(実際には行列なので何本もありますが)それぞれのGLS推定量を按分したような推定量になります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}\)&lt;/span&gt;は既知ではないので、いったん&lt;code&gt;OLS&lt;/code&gt;で推計しその推定量を用います。問題は&lt;span class=&#34;math inline&#34;&gt;\(V_{0}\)&lt;/span&gt;の置き方です。&lt;span class=&#34;math inline&#34;&gt;\(V_{0}\)&lt;/span&gt;の各要素を小さくとれば、事前分布に整合的な推定量が得られます（つまり、ランダムウォーク）。逆に、大きくとれば通常の&lt;code&gt;GLS&lt;/code&gt;推定量に近づいていきます。Doan, Litternam and Sims (1984)では以下のように分散を置いています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\displaystyle Var(\beta_{j,k}^{j}) &amp;amp;=&amp;amp; \frac{\pi_{5}・\pi_{1}}{k・exp(\pi_{4}w_{i}^{i})} \\
\displaystyle Var(\beta_{j,k}^{i}) &amp;amp;=&amp;amp; \frac{\pi_{5}・\pi_{2}・\sigma_{i}^{2}}{k・exp(\pi_{4}w_{j}^{i})・\sigma_{j}^{2}} \\
Var(c^{i}) &amp;amp;=&amp;amp; \pi_{5}・\pi_{3}・\sigma_{i}^{2}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の方程式の自己回帰パラメータ、&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目方程式の他変数ラグ項にかかるパラメータ、&lt;span class=&#34;math inline&#34;&gt;\(c^{i}\)&lt;/span&gt;は定数項です。そして、&lt;span class=&#34;math inline&#34;&gt;\(\pi_{1},\pi_{2},\pi_{3},\pi_{4},\pi_{5}\)&lt;/span&gt;はハイパーパラメータと呼ばれるもので、カルマンフィルタにかけるパラメータの初期値の事前分布の分布の広がりを決定するパラメータとなっています。これらは初期値の推定を行う前に値を指定する必要があります。各ハイパーパラメータの具体的な特徴は以下の通りです。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{1}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{j}\)&lt;/span&gt;の分散にのみ出現することから自己回帰係数に影響を与えるパラメータとなっています。具体的には、&lt;span class=&#34;math inline&#34;&gt;\(\pi_{1}\)&lt;/span&gt;が大きくなればなるほど自己回帰係数は事前分布から大きく離れた辺りを取りうることになります。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{2}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{i}\)&lt;/span&gt;の分散にのみ出現することから他変数のラグ項に影響を与えるパラメータとなっています。こちらも値が大きくなればなるほど係数は事前分布から大きく離れた辺りを取りうることになります。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{3}\)&lt;/span&gt;は定数項の事前分布に影響を与えるパラメータでこちらも考え方は同じです。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{4}\)&lt;/span&gt;はラグ項の分散(つまり定数項以外)に影響を与えるパラメータで、値が大きくなるにつれ、初期値は事前分布に近づいていきます。最後に&lt;span class=&#34;math inline&#34;&gt;\(\pi_{5}\)&lt;/span&gt;ですが、こちらは全体にかかるパラメータで、値が大きくなるにつれ、初期値は事前分布から遠ざかります。&lt;/p&gt;
&lt;p&gt;　上式には、ハイパーパラメータ以外にもパラメータや変数が存在します。&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{i}, \sigma_{j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(y_{j,t}, y_{i,t}\)&lt;/span&gt;をそれぞれ&lt;code&gt;AR(m)&lt;/code&gt;でフィッティングをかけた時の残差の標準偏差の推定値です。変数間のスケーリングの違いを考慮するために、&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{i}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{j}\)&lt;/span&gt;で割ったものを使用しています。本来ならば、&lt;code&gt;VAR&lt;/code&gt;の残差の標準偏差を使用すべきなのですが、推定する前にわかるわけもないので、それぞれ&lt;code&gt;AR(m)&lt;/code&gt;で推定をかけ、その標準偏差を使用しています((ランダムウォークが先験情報なので整合性は取れているような気がします))。&lt;span class=&#34;math inline&#34;&gt;\(w_{j}^{i}\)&lt;/span&gt;は完全に恣意的なパラメータで、DLSではrelative weightsと呼ばれているものです。i番目の方程式のj番目の変数のラグ項にかかるパラメータが０であるかどうかについて、分析者の先験情報を反映するためのパラメータです。分散の式を見ればわかるように、relative weightが大きくなれば分散は小さくなり、推定値は事前分布に近づいていきます。DLSでは、ほとんどの変数は&lt;span class=&#34;math inline&#34;&gt;\(w_{i}^{i}=0,w_{j}^{i}=1\)&lt;/span&gt;でよいと主張されています。つまり、自己ラグにかかる事前分布の分散に関しては確信をもってランダムウォークであるといえる一方、他変数ラグについては予測力向上に役立つもののあることを考え、値を１と置いているのです。一方、為替レートや株価はランダムウォーク色が強いということから大きい値を使用しています。最後に、&lt;span class=&#34;math inline&#34;&gt;\(Var(\beta_{j,k}^{j})\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(Var(\beta_{j,k}^{i})\)&lt;/span&gt;には分母にkがついています。つまり、ラグ次数kが大きくなればなるほど、その係数は０に近づいていくことを先験的情報として仮定していることになります((このおかげで&lt;code&gt;VAR&lt;/code&gt;の次数をこちらで指定する必要がなくなります。適当に大きい次数を指定しておけば、必要のない次数の大きいパラメータに関しては事前と値が０になるので。))。
　
　これらがいわゆる&lt;code&gt;Minnesota Prior&lt;/code&gt;の正体です((実はこれに加えて自己回帰パラメータの総和が１、他変数のラグ項にかかるパラメータの総和が０となる制約を課すのですが、話が複雑になりすぎるので今回は割愛しました))。初期値が事前分布に近づけば&lt;code&gt;BVAR&lt;/code&gt;はランダムウォークに近づきますし、離れるとUVARに近づきます。現実はその間となるのですが、なにを評価尺度としてハイパーパラメータの値を決めるかというと、それは当てはまりの良さということになります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ハイパーパラメータの決定方法とその評価尺度&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ハイパーパラメータの決定方法とその評価尺度&lt;/h3&gt;
&lt;p&gt;当てはまりの良さと言ってもいろいろありますが、DLSはその時点で観測可能なデータから予測できるk期先の予測値の当てはまりの良さを基準としています。DLSでは以下の予測誤差ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}_{t+k|t}\)&lt;/span&gt;のクロス積和を最小化することを目的関数として、ハイパーパラメータのチューニングを行っています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\epsilon}_{t+k|t} = \hat{Y}_{t+k|t}-Y_{t+k}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}_{t+k|t}\)&lt;/span&gt;はカルマンフィルタによるk期先の予測値です（kをいくつ先にすれば良いかは不明）。このクロス積は&lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}_{t+k|t}\hat{\epsilon}&amp;#39;_{t+k|t}\)&lt;/span&gt;であり、これをフィルタリングをかけるt=1期からサンプル期間であるt=T期まで計算していくので、最終的にはT個の予測誤差ベクトルを得ることになり、以下のようなこれらの総和を最小化します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \sum_{t=1}^{T-k}\hat{\epsilon}_{t+k|t}\hat{\epsilon}&amp;#39;_{t+k|t}
\]&lt;/span&gt;
より厳密にはこのクロス積和の対数値を最小化するハイパーパラメータの値をグリッドサーチやランダムサーチで探索していくことになります((ここは機械学習等で用いられているベイズ最適化を利用するとより高速に収束させることが可能かもです。))。
とまあ、&lt;code&gt;BVAR&lt;/code&gt;の推定方法はこんな感じです。他の&lt;code&gt;VAR&lt;/code&gt;と違い、恣意的であり、また推定方法が機械学習に近い点が特徴ではないかと思います。そもそも、&lt;code&gt;BVAR&lt;/code&gt;は予測に特化した&lt;code&gt;VAR&lt;/code&gt;ですから、他の&lt;code&gt;VAR&lt;/code&gt;とは別物と考える方が良いかもです。&lt;/p&gt;
&lt;p&gt;（追記　2019/4/29）&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rでの実装&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Rでの実装&lt;/h2&gt;
&lt;p&gt;ここまでを&lt;code&gt;R&lt;/code&gt;で実装したいと思います。まずは、&lt;code&gt;vars&lt;/code&gt;パッケージに準備されているCanadaデータを使用して、通常の&lt;code&gt;VAR&lt;/code&gt;の推定をやってみます。これはカナダの1980～2000年の労働生産性、雇用、失業率、実質賃金をまとめたものになります。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vars)

data(Canada)
Canada.var &amp;lt;- VAR(Canada,p=VARselect(Canada,lag.max = 4)$selection[1])
summary(Canada.var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## VAR Estimation Results:
## ========================= 
## Endogenous variables: e, prod, rw, U 
## Deterministic variables: const 
## Sample size: 81 
## Log Likelihood: -150.609 
## Roots of the characteristic polynomial:
## 1.004 0.9283 0.9283 0.7437 0.7437 0.6043 0.6043 0.5355 0.5355 0.2258 0.2258 0.1607
## Call:
## VAR(y = Canada, p = VARselect(Canada, lag.max = 4)$selection[1])
## 
## 
## Estimation results for equation e: 
## ================================== 
## e = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1       1.75274    0.15082  11.622  &amp;lt; 2e-16 ***
## prod.l1    0.16962    0.06228   2.723 0.008204 ** 
## rw.l1     -0.08260    0.05277  -1.565 0.122180    
## U.l1       0.09952    0.19747   0.504 0.615915    
## e.l2      -1.18385    0.23517  -5.034 3.75e-06 ***
## prod.l2   -0.10574    0.09425  -1.122 0.265858    
## rw.l2     -0.02439    0.06957  -0.351 0.727032    
## U.l2      -0.05077    0.24534  -0.207 0.836667    
## e.l3       0.58725    0.16431   3.574 0.000652 ***
## prod.l3    0.01054    0.06384   0.165 0.869371    
## rw.l3      0.03824    0.05365   0.713 0.478450    
## U.l3       0.34139    0.20530   1.663 0.100938    
## const   -150.68737   61.00889  -2.470 0.016029 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.3399 on 68 degrees of freedom
## Multiple R-Squared: 0.9988,  Adjusted R-squared: 0.9985 
## F-statistic:  4554 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## Estimation results for equation prod: 
## ===================================== 
## prod = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1      -0.14880    0.28913  -0.515   0.6085    
## prod.l1    1.14799    0.11940   9.615 2.65e-14 ***
## rw.l1      0.02359    0.10117   0.233   0.8163    
## U.l1      -0.65814    0.37857  -1.739   0.0866 .  
## e.l2      -0.18165    0.45083  -0.403   0.6883    
## prod.l2   -0.19627    0.18069  -1.086   0.2812    
## rw.l2     -0.20337    0.13337  -1.525   0.1319    
## U.l2       0.82237    0.47034   1.748   0.0849 .  
## e.l3       0.57495    0.31499   1.825   0.0723 .  
## prod.l3    0.04415    0.12239   0.361   0.7194    
## rw.l3      0.09337    0.10285   0.908   0.3672    
## U.l3       0.40078    0.39357   1.018   0.3121    
## const   -195.86985  116.95813  -1.675   0.0986 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.6515 on 68 degrees of freedom
## Multiple R-Squared:  0.98,   Adjusted R-squared: 0.9765 
## F-statistic: 277.5 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## Estimation results for equation rw: 
## =================================== 
## rw = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1    -4.716e-01  3.373e-01  -1.398    0.167    
## prod.l1 -6.500e-02  1.393e-01  -0.467    0.642    
## rw.l1    9.091e-01  1.180e-01   7.702 7.63e-11 ***
## U.l1    -7.941e-04  4.417e-01  -0.002    0.999    
## e.l2     6.667e-01  5.260e-01   1.268    0.209    
## prod.l2 -2.164e-01  2.108e-01  -1.027    0.308    
## rw.l2   -1.457e-01  1.556e-01  -0.936    0.353    
## U.l2    -3.014e-01  5.487e-01  -0.549    0.585    
## e.l3    -1.289e-01  3.675e-01  -0.351    0.727    
## prod.l3  2.140e-01  1.428e-01   1.498    0.139    
## rw.l3    1.902e-01  1.200e-01   1.585    0.118    
## U.l3     1.506e-01  4.592e-01   0.328    0.744    
## const   -1.167e+01  1.365e+02  -0.086    0.932    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.7601 on 68 degrees of freedom
## Multiple R-Squared: 0.9989,  Adjusted R-squared: 0.9987 
## F-statistic:  5239 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## Estimation results for equation U: 
## ================================== 
## U = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##          Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1     -0.61773    0.12508  -4.939 5.39e-06 ***
## prod.l1  -0.09778    0.05165  -1.893 0.062614 .  
## rw.l1     0.01455    0.04377   0.332 0.740601    
## U.l1      0.65976    0.16378   4.028 0.000144 ***
## e.l2      0.51811    0.19504   2.656 0.009830 ** 
## prod.l2   0.08799    0.07817   1.126 0.264279    
## rw.l2     0.06993    0.05770   1.212 0.229700    
## U.l2     -0.08099    0.20348  -0.398 0.691865    
## e.l3     -0.03006    0.13627  -0.221 0.826069    
## prod.l3  -0.01092    0.05295  -0.206 0.837180    
## rw.l3    -0.03909    0.04450  -0.879 0.382733    
## U.l3      0.06684    0.17027   0.393 0.695858    
## const   114.36732   50.59802   2.260 0.027008 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.2819 on 68 degrees of freedom
## Multiple R-Squared: 0.9736,  Adjusted R-squared: 0.969 
## F-statistic: 209.2 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## 
## Covariance matrix of residuals:
##             e     prod       rw        U
## e     0.11550 -0.03161 -0.03681 -0.07034
## prod -0.03161  0.42449  0.05589  0.01494
## rw   -0.03681  0.05589  0.57780  0.03660
## U    -0.07034  0.01494  0.03660  0.07945
## 
## Correlation matrix of residuals:
##            e     prod      rw        U
## e     1.0000 -0.14276 -0.1425 -0.73426
## prod -0.1428  1.00000  0.1129  0.08136
## rw   -0.1425  0.11286  1.0000  0.17084
## U    -0.7343  0.08136  0.1708  1.00000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(predict(Canada.var,n.ahead=20,ci=0.95))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/r-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;パッケージが整備されているので簡単に実行できました。上で見たようにやっていることは別々の&lt;code&gt;OLS&lt;/code&gt;を4本推定しているだけです。
次に&lt;code&gt;BVAR&lt;/code&gt;です。まず、カルマンフィルタと事前分布を定義します。カルマンフィルタは以前記事でご紹介したものと同じです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(seasonal)

kalmanfiter &amp;lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){
  #-------------------------------------------------------------------
  # Implemention of Kalman filter
  #   y - observed variable
  #   I - the number of unobserved variable
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   c - constant in state equaion
  #   R - parameter of exogenous variable in state equation
  #   Q - var-cov matrix of exogenous variable in state equation
  #   d - constant in observable equaion
  #   S - parameter of exogenous variable in observable equation
  #   h - var-cov matrix of exogenous variable in observable equation
  #   a_int - initial value of endogenous variable
  #   sig_int - initial value of variance of endogenous variable
  #-------------------------------------------------------------------
  
  library(MASS)
    
  # 1.Define Variable
  if (class(y)!=&amp;quot;matrix&amp;quot;){
    y &amp;lt;- as.matrix(y)
  }
  N &amp;lt;- NROW(y) # sample size
  L &amp;lt;- NCOL(y) # the number of observable variable 
  a_pre &amp;lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable
  a_fil &amp;lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable
  sig_pre &amp;lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable
  sig_fil &amp;lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable
  y_pre &amp;lt;- array(0,dim = c(L,1,N)) # prediction of observed variable
  F_pre &amp;lt;- array(0,dim = c(L,L,N)) # auxiliary variable 
  k &amp;lt;- array(0,dim = c(I,L,N)) # kalman gain
  
  if (any(is.na(a_int))){
    a_int &amp;lt;- matrix(0,nrow = I,ncol = 1)
  }
  if (any(is.na(sig_int))){
    sig_int &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(R))){
    R &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(Q))){
    Q &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if(any(is.na(S))){
    S &amp;lt;- matrix(1,nrow = L,ncol = L)
  }
  if (any(is.na(h))){
    H &amp;lt;- array(0,dim = c(L,L,N))
    for(i in 1:N){
      diag(H[,,i]) = 1
    }
  }else if (class(h)!=&amp;quot;array&amp;quot;){
    H &amp;lt;- array(h,dim = c(NROW(h),NCOL(h),N))
  }
  
  # fill infinite if observed data is NA
  for(i in 1:N){
    miss &amp;lt;- is.na(y[i,])
    diag(H[,,i])[miss] &amp;lt;- 1e+32
  }
  y[is.na(y)] &amp;lt;- 0
  
  # 2.Set Initial Value
  a_fil[,,1] &amp;lt;- a_int
  sig_fil[,,1] &amp;lt;- sig_int
  
  # 3.Implement Kalman filter
  for (i in 1:N){
    if(class(z)==&amp;quot;array&amp;quot;){
      Z &amp;lt;- z[,,i]
    }else{
      Z &amp;lt;- z
    }
    a_pre[,,i] &amp;lt;- t%*%a_fil[,,i] + c
    sig_pre[,,i] &amp;lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)
    y_pre[,,i] &amp;lt;- Z%*%a_pre[,,i] + d
    F_pre[,,i] &amp;lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)
    k[,,i] &amp;lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])
    a_fil[,,i+1] &amp;lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])
    sig_fil[,,i+1] &amp;lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])
  }
  
  # 4.Aggregate results
  result &amp;lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)
  names(result) &amp;lt;- c(&amp;quot;state prediction&amp;quot;, &amp;quot;state filtered&amp;quot;, &amp;quot;state var prediction&amp;quot;, 
                     &amp;quot;state var filtered&amp;quot;, &amp;quot;observable prediction&amp;quot;, &amp;quot;kalman gain&amp;quot;,
                     &amp;quot;parameter of state eq&amp;quot;, &amp;quot;parameter of observable eq&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;事前分布を計算する関数を定義します。まず、引数を計算しやすい形に変換しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # set pi
  pi &amp;lt;- abs(rnorm(5)) # hyperparameter
  x &amp;lt;- Canada # dataset
  order &amp;lt;- 4 # order
  
  library(MASS)
    
  # 1. Process row data
  for (i in 1:5){
    eval(parse(text = paste0(&amp;quot;pi&amp;quot;,i,&amp;quot;=pi[[&amp;quot;,i,&amp;quot;]]&amp;quot;)))
  }
  if (class(x)!=&amp;quot;matrix&amp;quot;){
    x &amp;lt;- as.matrix(x)
  }
  dependent &amp;lt;- as.matrix(x[(order+1):NROW(x),1])
  for (i in 2:NCOL(x)){
    dependent &amp;lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))
  }
  explanatory &amp;lt;- embed(cbind(1,x),order)[,-((order*NCOL(x)+order-NCOL(x)):(order*NCOL(x)+order))]
  explanatory &amp;lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)
  npara &amp;lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;dependentは非説明変数、explanatoryは説明変数です。ちょうど1式を再現した形になります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
   \left(
    \begin{array}{cccc}
      Y_{1} \\
      Y_{2} \\
      \vdots  \\
      Y_{J} \\
    \end{array}
  \right) = 
 \left(
    \begin{array}{cccc}
      X_{1} &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; X_{2} &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; X_{J}
    \end{array}
  \right) 
  \left(
    \begin{array}{cccc}
      A_{1} \\
      A_{2} \\
      \vdots  \\
      A_{J} \\
    \end{array}
  \right) +
  \left(
    \begin{array}{cccc}
      U_{1} \\
      U_{2} \\
      \vdots  \\
      U_{J} \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2. Make Prior matrixes
  r &amp;lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation
  iter1 &amp;lt;- numeric(NCOL(x))
  n &amp;lt;- 2
  for (i in 1:length(iter1)){
    iter1[i] &amp;lt;- n
    n &amp;lt;- n + order*NCOL(x) + 1
  }
  for (i in iter1){
    r[i] &amp;lt;- 1
  }
  R &amp;lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation
  V &amp;lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation
  
  V0 &amp;lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix
  
  # PRIOR FOR VAR OF CONSTANT
  sigi &amp;lt;- numeric(NCOL(x)) # var of AR(m)
  for (i in 1:NCOL(x)){
    AR &amp;lt;- ar(x[,i],order.max = order)
    sigi[i] &amp;lt;- AR$var.pred
  }
  sigma_AR &amp;lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))
  iter2 &amp;lt;- seq(1,npara,(NCOL(x)*order+1))
  n &amp;lt;- 1
  for (i in iter2){
    V0[i,i] &amp;lt;- pi5*pi3*sigi[n]
    n &amp;lt;- n + 1
  }
  
  # PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER
  wi &amp;lt;- 0 # prior weight on s.d. of autoregressive parameter
  k &amp;lt;- 0 # decay parameter
  iter3 &amp;lt;- numeric((NCOL(x)*order))
  n &amp;lt;- 2;iter3[1] &amp;lt;- n
  for (i in 2:length(iter3)){
    if ((i-1)%%order == 0){
      n &amp;lt;- n + NCOL(x) + 2 # const
      iter3[i] &amp;lt;- n 
    }else{
      n &amp;lt;- n + NCOL(x)
      iter3[i] &amp;lt;- n
    }
  }
  for (i in iter3) {
    k &amp;lt;- k + 1
    V0[i,i] &amp;lt;- pi5*pi1/(k*exp(pi4*wi))
    if (k == order){
      k = 0
    }
  }
  
  # PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER
  wj &amp;lt;- 1 # prior weight on s.d. of distributed lag parameter
  k &amp;lt;- 1 # decay parameter
  count &amp;lt;- 0
  n &amp;lt;- 1
  ndis &amp;lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation
  
  l &amp;lt;- rep(1:NCOL(x),length=npara)
  temp &amp;lt;- numeric(length(iter3))
  j &amp;lt;- 1;temp[1] &amp;lt;- j
  for (i in 2:length(temp)){
    if ((i-1)%%order == 0){
      j &amp;lt;- j + NCOL(x) + 1 # const
      temp[i] &amp;lt;- j 
    }else{
      j &amp;lt;- j + NCOL(x)
      temp[i] &amp;lt;- j
    }
  }
  l &amp;lt;- l[-c(temp)]; l &amp;lt;- l[1:(ndis)]
  
  iter4 &amp;lt;- 1:npara; iter4 &amp;lt;- iter4[-c(iter2,iter3)]
  for (i in iter4){
    count &amp;lt;- count + 1
    V0[i,i] &amp;lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])
    if(count%%(order-1) == 0){
      k &amp;lt;- k + 1
    }
    if (count%%(ndis/NCOL(x)) == 0){
      n = n + 1
      k = 1
    }
  }

  # 3. Estimate OLS
  beta &amp;lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent
  sig &amp;lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*
                      t(dependent-explanatory%*%beta)%*%ginv(V)%*%
                      (dependent-explanatory%*%beta))
  
  # 4. Implement Mixed Estimation for initial values
  beta_M &amp;lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%
    ((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)
  sig_M &amp;lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)
  
  # for kalman filter
  observable &amp;lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable
  coefficient &amp;lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable
  for(i in 1:(NROW(x)-order)){
    for (k in 1:NCOL(x)){
      coefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] &amp;lt;- explanatory[i,1:(npara/NCOL(x))]
    }
  }
  
  result &amp;lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)
  names(result) &amp;lt;- c(&amp;quot;observable&amp;quot;, &amp;quot;coefficient&amp;quot;,&amp;quot;beta_M&amp;quot;, &amp;quot;sig_M&amp;quot;,&amp;quot;sigma_AR&amp;quot;, &amp;quot;V0&amp;quot;,&amp;quot;order&amp;quot;,&amp;quot;npara&amp;quot;)
  
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MPrior &amp;lt;- function(x,order,pi){
  #-------------------------------------------------------------------
  # Make Minnesota Prior
  #   x - dataset
  #   order - max lag of autoregression
  #   pi - a set of hyper-parameter of prior matrix
  #-------------------------------------------------------------------

  library(MASS)
    
  # 1. Process row data
  for (i in 1:5){
    eval(parse(text = paste0(&amp;quot;pi&amp;quot;,i,&amp;quot;=pi[[&amp;quot;,i,&amp;quot;]]&amp;quot;)))
  }
  if (class(x)!=&amp;quot;matrix&amp;quot;){
    x &amp;lt;- as.matrix(x)
  }
  dependent &amp;lt;- as.matrix(x[1:(NROW(x)-order),1])
  for (i in 2:NCOL(x)){
    dependent &amp;lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))
  }
  explanatory &amp;lt;- cbind(1,embed(x,order)[-1,])
  explanatory &amp;lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)
  npara &amp;lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))
  
  # 2. Make Prior matrixes
  r &amp;lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation
  iter1 &amp;lt;- numeric(NCOL(x))
  n &amp;lt;- 2
  for (i in 1:length(iter1)){
    iter1[i] &amp;lt;- n
    n &amp;lt;- n + order*NCOL(x) + 1
  }
  for (i in iter1){
    r[i] &amp;lt;- 1
  }
  R &amp;lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation
  V &amp;lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation
  
  V0 &amp;lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix
  
  # PRIOR FOR VAR OF CONSTANT
  sigi &amp;lt;- numeric(NCOL(x)) # var of AR(m)
  for (i in 1:NCOL(x)){
    AR &amp;lt;- ar(x[,i],order.max = order)
    sigi[i] &amp;lt;- AR$var.pred
  }
  sigma_AR &amp;lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))
  iter2 &amp;lt;- seq(1,npara,(NCOL(x)*order+1))
  n &amp;lt;- 1
  for (i in iter2){
    V0[i,i] &amp;lt;- pi5*pi3*sigi[n]
    n &amp;lt;- n + 1
  }
  
  # PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER
  wi &amp;lt;- 0 # prior weight on s.d. of autoregressive parameter
  k &amp;lt;- 0 # decay parameter
  iter3 &amp;lt;- numeric((NCOL(x)*order))
  n &amp;lt;- 2;iter3[1] &amp;lt;- n
  for (i in 2:length(iter3)){
    if ((i-1)%%order == 0){
      n &amp;lt;- n + NCOL(x) + 2 # const
      iter3[i] &amp;lt;- n 
    }else{
      n &amp;lt;- n + NCOL(x)
      iter3[i] &amp;lt;- n
    }
  }
  for (i in iter3) {
    k &amp;lt;- k + 1
    V0[i,i] &amp;lt;- pi5*pi1/(k*exp(pi4*wi))
    if (k == order){
      k = 0
    }
  }
  
  # PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER
  wj &amp;lt;- 1 # prior weight on s.d. of distributed lag parameter
  k &amp;lt;- 1 # decay parameter
  count &amp;lt;- 0
  n &amp;lt;- 1
  ndis &amp;lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation
  
  l &amp;lt;- rep(1:NCOL(x),length=npara)
  temp &amp;lt;- numeric(length(iter3))
  j &amp;lt;- 1;temp[1] &amp;lt;- j
  for (i in 2:length(temp)){
    if ((i-1)%%order == 0){
      j &amp;lt;- j + NCOL(x) + 1 # const
      temp[i] &amp;lt;- j 
    }else{
      j &amp;lt;- j + NCOL(x)
      temp[i] &amp;lt;- j
    }
  }
  l &amp;lt;- l[-c(temp)]; l &amp;lt;- l[1:(ndis)]
  
  iter4 &amp;lt;- 1:npara; iter4 &amp;lt;- iter4[-c(iter2,iter3)]
  for (i in iter4){
    count &amp;lt;- count + 1
    V0[i,i] &amp;lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])
    if(count%%(order-1) == 0){
      k &amp;lt;- k + 1
    }
    if (count%%(ndis/NCOL(x)) == 0){
      n = n + 1
      k = 1
    }
  }

  # 3. Estimate OLS
  beta &amp;lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent
  sig &amp;lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*
                      t(dependent-explanatory%*%beta)%*%ginv(V)%*%
                      (dependent-explanatory%*%beta))
  
  # 4. Implement Mixed Estimation for initial values
  beta_M &amp;lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%
    ((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)
  sig_M &amp;lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)
  
  # for kalman filter
  observable &amp;lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable
  coefficient &amp;lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable
  for(i in 1:(NROW(x)-order)){
    for (k in 1:NCOL(x)){
      coefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] &amp;lt;- explanatory[i,1:(npara/NCOL(x))]
    }
  }
  
  result &amp;lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)
  names(result) &amp;lt;- c(&amp;quot;observable&amp;quot;, &amp;quot;coefficient&amp;quot;,&amp;quot;beta_M&amp;quot;, &amp;quot;sig_M&amp;quot;,&amp;quot;sigma_AR&amp;quot;, &amp;quot;V0&amp;quot;,&amp;quot;order&amp;quot;,&amp;quot;npara&amp;quot;)
  
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BVAR &amp;lt;- function(m){
  #-------------------------------------------------------------------
  # Estimation of Bayesian Vector Auto-Regression
  #   m - dataset
  #-------------------------------------------------------------------
  
  # 1. Process data
  observable &amp;lt;- m$observable
  coefficient &amp;lt;- m$coefficient
  beta_M &amp;lt;- m$beta_M
  sig_M &amp;lt;- m$sig_M
  sigma_AR &amp;lt;- m$sigma_AR
  V0 &amp;lt;- m$V0
  order &amp;lt;- m$order
  npara &amp;lt;- m$npara
  
  # 2. Run Kalmanfilter
  results &amp;lt;- kalmanfiter(observable,npara,diag(1,nrow = npara,ncol = npara),
                         coefficient,0,NA,V0,0,NA,sigma_AR,beta_M,sig_M)
  return(results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- MPrior(Canada,3,abs(rnorm(5)))
results &amp;lt;- BVAR(m)

BVAR_Prediction &amp;lt;- function(results,horizon){
  
  state_pred &amp;lt;- results$`state prediction`
  state_fil &amp;lt;- results$`state filtered`
  obs_pred &amp;lt;- results$`observable prediction`
  A &amp;lt;- results$`parameter of state eq`
    
  nparaall &amp;lt;- dim(state_pred)[1]
  num &amp;lt;- dim(obs_pred)[1]
  npara &amp;lt;- order*num+1
  order &amp;lt;- (nparaall-num)/(num^2)
  samplesize &amp;lt;- dim(state_pred)[3]
  
  y &amp;lt;- matrix(0,horizon,num)
  B &amp;lt;- matrix(0,horizon,nparaall)
  x &amp;lt;- results$`parameter of observable eq`[1,1:npara,samplesize]
  
  B[1,] &amp;lt;- state_fil[,1,samplesize]
  
  for (i in 1:(horizon-1)) {
    B[i+1,] &amp;lt;- A%*%B[i,]
    y[i+1,] &amp;lt;- B[i+1,]%*%(diag(1,num)%x%x)
    x &amp;lt;- c(1,y[i+1,],x[-c(1,seq(from=(npara-order),to=npara))])
  }
  
  result &amp;lt;- list(B,y)
  names(result) &amp;lt;- c(&amp;quot;B&amp;quot;,&amp;quot;y&amp;quot;)
  
  return(result)
  
}

pre &amp;lt;- BVAR_Prediction(results,20)

BVAR_OPT &amp;lt;- function(x){
  int_pi &amp;lt;- rnorm(5)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Sims, Christopher A, 1980. “Macroeconomics and Reality,” Econometrica, Econometric Society, vol. 48(1), pages 1-48, January.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gianonne et. al. (2008)のマルチファクターモデルで四半期GDPを予想してみた</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post6/</link>
      <pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post6/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;おはこんばんにちは。
前回、統計ダッシュボードからAPI接続で統計データを落とすという記事を投稿しました。
今回はそのデータを、Gianonne et. al. (2008)のマルチファクターモデルにかけ、四半期GDPの予測を行いたいと思います。&lt;/p&gt;
&lt;div id=&#34;gianonne-et.-al.-2008版マルチファクターモデル&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Gianonne et. al. (2008)版マルチファクターモデル&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dept.ku.edu/~empirics/Courses/Econ844/papers/Nowcasting%20GDP.pdf&#34;&gt;元論文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前回の投稿でも書きましたが、この論文はGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標を2つのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しています。
まず、このモデルについてご紹介します。このモデルでは2段階推計を行います。まず主成分分析により経済統計を統計間の相関が0となるファクターへ変換します（&lt;a href=&#34;https://datachemeng.com/principalcomponentanalysis/&#34;&gt;参考&lt;/a&gt;）。そして、その後の状態空間モデルでの推計で必要になるパラメータを&lt;code&gt;OLS&lt;/code&gt;推計し、そのパラメータを使用してカルマンフィルタ＆カルマンスムーザーを回し、ファクターを推計しています。では、具体的な説明に移ります。
統計データを&lt;span class=&#34;math inline&#34;&gt;\(x_{i,t|v_j}\)&lt;/span&gt;と定義します。ここで、&lt;span class=&#34;math inline&#34;&gt;\(i=1,...,n\)&lt;/span&gt;は経済統計を表し（つまり&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;が全統計数）、&lt;span class=&#34;math inline&#34;&gt;\(t=1,...,T_{iv_j}\)&lt;/span&gt;は統計&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;のサンプル期間の時点を表しています（つまり、&lt;span class=&#34;math inline&#34;&gt;\(T_{iv_j}\)&lt;/span&gt;は統計&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;のその時点での最新データ日付を表す）。また、&lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt;はある時点&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;（2005年など）で得られる情報集合（vintage）を表しています。統計データ&lt;span class=&#34;math inline&#34;&gt;\(x_{i,t|v_j}\)&lt;/span&gt;は以下のようにファクター&lt;span class=&#34;math inline&#34;&gt;\(f_{r,t}\)&lt;/span&gt;の線形結合で表すことができます（ここで&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;はファクターの数を表す）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_{i,t|v\_j} = \mu_i + \lambda_{i1}f_{1,t} + ... + \lambda_{ir}f_{r,t} + \xi_{i,t|v_j} \tag{1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu\_i\)&lt;/span&gt;は定数項、&lt;span class=&#34;math inline&#34;&gt;\(\lambda\_{ir}\)&lt;/span&gt;はファクターローディング、&lt;span class=&#34;math inline&#34;&gt;\(\xi\_{i,t|v\_j}\)&lt;/span&gt;はホワイトノイズの誤差項を表しています。これを行列形式で書くと以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_{t|v_j}  = \mu + \Lambda F_t + \xi_{t|v_j} = \mu + \chi_t + \xi_{t|v_j} \tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(x_{t|v_j} = (x_{1,t|v_j}, ..., x_{n,t|v_j} )^{\mathrm{T}}\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\xi_{t|v_j}=(\xi_{1,t|v_j}, ..., \xi_{n,t|v_j})^{\mathrm{T}}\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(F_t = (f_{1,t}, ..., f_{r,t})^{\mathrm{T}}\)&lt;/span&gt;であり、&lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;は各要素が$ _{ij}&lt;span class=&#34;math inline&#34;&gt;\(の\)&lt;/span&gt;nr&lt;span class=&#34;math inline&#34;&gt;\(行列のファクターローディングを表しています。また、\)&lt;/span&gt;&lt;em&gt;t = F_t&lt;span class=&#34;math inline&#34;&gt;\(です。よって、ファクター\)&lt;/span&gt; F_t&lt;span class=&#34;math inline&#34;&gt;\(を推定するためには、データ\)&lt;/span&gt;x&lt;/em&gt;{i,t|v_j}$を以下のように基準化したうえで、分散共分散行列を計算し、その固有値問題を解けばよいという事になります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle z_{it} = \frac{1}{\hat{\sigma}_i}(x_{it} - \hat{\mu}_{it}) \tag{3}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \hat{\mu}_{it} = 1/T \sum_{t=1}^T x_{it}\)&lt;/span&gt;であり、&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}_i = \sqrt{1/T \sum_{t=1}^T (x_{it}-\hat{\mu_{it}})^2}\)&lt;/span&gt;です（ここで&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;はサンプル期間）。分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;を以下のように定義します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle S = \frac{1}{T} \sum_{t=1}^T z_t z_t^{\mathrm{T}} \tag{4}
\]&lt;/span&gt;
次に、&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;のうち、固有値を大きい順に&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;個取り出し、それを要素にした$ r r&lt;span class=&#34;math inline&#34;&gt;\(対角行列を\)&lt;/span&gt; D&lt;span class=&#34;math inline&#34;&gt;\(、それに対応する固有ベクトルを\)&lt;/span&gt;n r&lt;span class=&#34;math inline&#34;&gt;\(行列にしたものを\)&lt;/span&gt; V&lt;span class=&#34;math inline&#34;&gt;\(と定義します。ファクター\)&lt;/span&gt; _t$は以下のように推計できます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{F}_t = V^{\mathrm{T}} z_t \tag{5}
\]&lt;/span&gt;
ファクターローディング&lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;と誤差項の共分散行列&lt;span class=&#34;math inline&#34;&gt;\(\Psi = \mathop{\mathbb{E}} [\xi_t\xi^{\mathrm{T}}_t]\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}_t\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(z_t\)&lt;/span&gt;に回帰することで推計します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \hat{\Lambda} = \sum_{t=1}^T z_t \tilde{F}^{\mathrm{T}}_t (\sum_{t=1}^T\tilde{F}_t\tilde{F}^{\mathrm{T}}_t)^{-1} = V \tag{6}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\Psi} = diag(S - VDV) \tag{7}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意して頂きたいのは、ここで推計した&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}_t\)&lt;/span&gt;は、以下の状態空間モデルでの推計に必要なパラメータを計算するための一時的な推計値であるという事です（２段階推計の１段階目という事）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_{t|v_j}  = \mu + \Lambda F\_t + \xi_{t|v_j} = \mu + \chi_t + \xi_{t|v_j} \tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F\_t = AF\_{t-1} + u\_t \tag{8}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt;は平均0、分散&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;のホワイトノイズです。再掲している(2)式が観測方程式、(8)式が遷移方程式となっています。推定すべきパラメータは&lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;以外に&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;があります（&lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;としています）。&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;は主成分分析により計算した&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}_t\)&lt;/span&gt;を&lt;code&gt;VAR(1)&lt;/code&gt;にかけることで推定します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{A} = \sum_{t=2}^T\tilde{F}_t\tilde{F}_{t-1}^{\mathrm{T}} (\sum_{t=2}^T\tilde{F}_{t-1}\tilde{F}_{t-1}^{\mathrm{T}})^{-1} \tag{9}
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;は今推計した&lt;code&gt;VAR(1)&lt;/code&gt;の誤差項の共分散行列から計算します。これで必要なパラメータの推定が終わりました。次にカルマンフィルタを回します。カルマンフィルタに関しては&lt;a href=&#34;https://qiita.com/MoriKen/items/0c80ef75749977767b43&#34;&gt;こちら&lt;/a&gt;を参考にしてください。わかりやすいです。これで最終的に&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{t|v_j}\)&lt;/span&gt;の推計ができるわけです。
GDPがこれらのファクターで説明可能であり（つまり固有の変動がない）、GDPと月次経済指標がjointly normalであれば以下のような単純なOLS推計でGDPを予測することができます。もちろん月次経済指標の方が早く公表されるので、内生性の問題はないと考えられます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}_{3k|v_j} = \alpha + \beta^{\mathrm{T}} \hat{F}_{3k|v_j} \tag{10}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(3k\)&lt;/span&gt;は四半期の最終月を示しています（3月、6月など）&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_{3k|v_j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;時点で得られる情報集合&lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt;での四半期GDPを表しており、&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{3k|v_j}\)&lt;/span&gt;はその時点で推定したファクターを表しています（四半期最終月の値だけを使用している点に注意）。これで推計方法の説明は終わりです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rで実装する&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Rで実装する&lt;/h2&gt;
&lt;p&gt;では実装します。前回記事で得られたデータ（dataset）が読み込まれている状態からスタートします。まず、主成分分析でファクターを計算します。なお、前回の記事で3ファクターの累積寄与度が80%を超えたため、今回もファクター数は3にしています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#------------------------
# Giannone et. al. 2008 
#------------------------

library(xts)
library(MASS)
library(tidyverse)

# 主成分分析でファクターを計算
f &amp;lt;- 3 # ファクター数を定義
a &amp;lt;- which(dataset1$publication == &amp;quot;2012-04-01&amp;quot;) # サンプル開始期間を2012年に設定。
dataset2 &amp;lt;- dataset1[a:nrow(dataset1),]
rownames(dataset2) &amp;lt;- dataset2$publication
dataset2 &amp;lt;- dataset2[,-2]
z &amp;lt;- scale(dataset2) # zは基準化されたサンプルデータ
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;S_i &amp;lt;- z[i,]%*%t(z[i,])&amp;quot;,sep = &amp;quot;&amp;quot;)))
  if (i==1){
    S &amp;lt;- S_i
  }else{
    S &amp;lt;- S + S_i
  }
}
S &amp;lt;- (1/nrow(z))*S # 分散共分散行列を計算 (4)式
gamma &amp;lt;- eigen(S) 
D &amp;lt;- diag(gamma$values[1:f])
V &amp;lt;- gamma$vectors[,1:f]
F_t &amp;lt;- matrix(0,nrow(z),f)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;F_t[&amp;quot;,i,&amp;quot;,]&amp;lt;- z[&amp;quot;,i,&amp;quot;,]%*%V&amp;quot;,sep = &amp;quot;&amp;quot;))) # (5)式を実行
}
F_t.xts &amp;lt;- xts(F_t,order.by = as.Date(row.names(z)))
plot.zoo(F_t.xts,col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;yellow&amp;quot;,&amp;quot;purple&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda_hat &amp;lt;- V
psi &amp;lt;- diag(S-V%*%D%*%t(V)) # (7)式
R &amp;lt;- diag(diag(cov(z-z%*%V%*%t(V)))) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;推計したファクター&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}\_t\)&lt;/span&gt;の時系列プロットは以下のようになり、前回&lt;code&gt;princomp&lt;/code&gt;関数で計算したファクターと完全一致します（じゃあ&lt;code&gt;princomp&lt;/code&gt;でいいやんと思われるかもしれませんが実装しないと勉強になりませんので）。&lt;/p&gt;
&lt;p&gt;次に、&lt;code&gt;VAR(1)&lt;/code&gt;を推計し、パラメータを取り出します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# VAR(1)モデルを推計
a &amp;lt;- matrix(0,f,f)
b &amp;lt;- matrix(0,f,f)
for(t in 2:nrow(z)){
  a &amp;lt;- a + F_t[t,]%*%t(F_t[t-1,])
  b &amp;lt;- b + F_t[t-1,]%*%t(F_t[t-1,])
}
b_inv &amp;lt;- solve(b)
A_hat &amp;lt;- a%*%b_inv # (9)式

e &amp;lt;- numeric(f)
for (t in 2:nrow(F_t)){
  e &amp;lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat
}
H &amp;lt;- t(e)%*%e
Q &amp;lt;- diag(1,f,f)
Q[1:f,1:f] &amp;lt;- H&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;VAR(1)&lt;/code&gt;に関しても&lt;code&gt;var&lt;/code&gt;関数とパラメータの数値が一致することを確認済みです。いよいよカルマンフィルタを実行します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# カルマンフィルタを実行
RR &amp;lt;- array(0,dim = c(ncol(z),ncol(z),nrow(z))) # RRは観測値の分散行列（相関はないと仮定）
for(i in 1:nrow(z)){
  miss &amp;lt;- is.na(z[i,])
  R_temp &amp;lt;- diag(R)
  R_temp[miss] &amp;lt;- 1e+32 # 欠損値の分散は無限大にする
  RR[,,i] &amp;lt;- diag(R_temp)
}
zz &amp;lt;- z; zz[is.na(z)] &amp;lt;- 0 # 欠損値（NA）に0を代入（計算結果にはほとんど影響しない）。
a_t &amp;lt;- matrix(0,nrow(zz),f) # a_tは状態変数の予測値
a_tt &amp;lt;- matrix(0,nrow(zz),f) # a_ttは状態変数の更新後の値
a_tt[1,] &amp;lt;- F_t[1,] # 状態変数の初期値には主成分分析で推計したファクターを使用
sigma_t &amp;lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の予測値
sigma_tt &amp;lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の更新値
p &amp;lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))
sigma_tt[,,1] &amp;lt;- matrix(p,3,3) # 状態変数の分散の初期値はVAR(1)の推計値から計算
y_t &amp;lt;- matrix(0,nrow(zz),ncol(zz)) # y_tは観測値の予測値
K_t &amp;lt;- array(0,dim = c(f,ncol(zz),nrow(zz))) # K_tはカルマンゲイン
data.m &amp;lt;- as.matrix(dataset2)
# カルマンフィルタを実行
for (t in 2:nrow(zz)){
  a_t[t,] &amp;lt;- A_hat%*%a_tt[t-1,]
  sigma_t[,,t] &amp;lt;- A_hat%*%sigma_tt[,,t-1]%*%t(A_hat) + Q
  y_t[t,] &amp;lt;- as.vector(V%*%a_t[t,])
  S_t &amp;lt;- V%*%sigma_tt[,,t-1]%*%t(V)+RR[,,t]
  GG &amp;lt;- t(V)%*%diag(1/diag(RR[,,t]))%*%V
  Sinv &amp;lt;- diag(1/diag(RR[,,t])) - diag(1/diag(RR[,,t]))%*%V%*%ginv(diag(nrow(A_hat))+sigma_t[,,t]%*%GG)%*%sigma_t[,,t]%*%t(V)%*%diag(1/diag(RR[,,t]))
  K_t[,,t] &amp;lt;- sigma_t[,,t]%*%t(V)%*%Sinv
  a_tt[t,] &amp;lt;- a_t[t,] + K_t[,,t]%*%(zz[t,]-y_t[t,])
  sigma_tt[,,t] &amp;lt;- sigma_t[,,t] - K_t[,,t]%*%V%*%sigma_tt[,,t-1]%*%t(V)%*%t(K_t[,,t])
  }

F.xts &amp;lt;- xts(a_tt,order.by = as.Date(rownames(data.m)))
plot.zoo(F.xts, col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;yellow&amp;quot;,&amp;quot;purple&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 得られた推計値を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;カルマンフィルタにより推計したファクターの時系列プロットが以下です。遷移方程式がAR(1)だったからかかなり平準化された値となっています。&lt;/p&gt;
&lt;p&gt;では、この得られたファクターをOLSにかけます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 得られたファクターとGDPをOLSにかける
F_q &amp;lt;- as.data.frame(a_tt[seq(3,nrow(a_tt),3),]) # 四半期の終わり月の値だけを引っ張ってくる 
colnames(F_q) &amp;lt;- c(&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;)
colnames(GDP) &amp;lt;- c(&amp;quot;publication&amp;quot;,&amp;quot;GDP&amp;quot;)
t &amp;lt;- which(GDP$publication==&amp;quot;2012-04-01&amp;quot;)
t2 &amp;lt;- which(GDP$publication==&amp;quot;2015-01-01&amp;quot;) # 2012-2q~2015-1qまでのデータが学習データ、それ以降がテストデータ
GDP_q &amp;lt;- GDP[t:nrow(GDP),]
dataset.q &amp;lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])
test &amp;lt;- lm(GDP~factor1 + factor2 + factor3,data=dataset.q)
summary(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GDP ~ factor1 + factor2 + factor3, data = dataset.q)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1350.34  -361.67   -31.63   375.61  1105.07 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 125729.4     4134.2  30.412 1.07e-08 ***
## factor1       -199.0     1651.3  -0.121    0.907    
## factor2      -1699.7      960.2  -1.770    0.120    
## factor3      -2097.6     3882.5  -0.540    0.606    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 897.5 on 7 degrees of freedom
## Multiple R-squared:  0.783,  Adjusted R-squared:   0.69 
## F-statistic: 8.419 on 3 and 7 DF,  p-value: 0.0101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out_of_sample &amp;lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成
test.pred &amp;lt;-  predict(test, out_of_sample, interval=&amp;quot;prediction&amp;quot;)
pred.GDP.xts &amp;lt;- xts(cbind(test.pred[,1],out_of_sample$GDP),order.by = out_of_sample$publication)
plot.zoo(pred.GDP.xts,col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 予測値と実績値を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;OLSの推計結果はfactor1（赤）とfactor2（青）が有意との結果。前回の投稿でも言及したように、factor1（赤）はリスクセンチメントを表していそうなので、係数の符号が負であることは頷ける。ただし、factor2（青）も符号が負なのではなぜなのか…。このファクターは生産年齢人口など経済の潜在能力を表していると思っていたのに。かなり謎。まあとりあえず予測に移りましょう。このモデルを使用したGDPの予測値と実績値の推移はいかのようになりました。直近の精度は悪くない？&lt;/p&gt;
&lt;p&gt;というか、これ完全に単位根の問題を無視してOLSしてしまっているな。ファクターもGDPも完全に単位根を持つけど念のため単位根検定をかけてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tseries)

adf.test(F_q$factor1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$factor1
## Dickey-Fuller = -2.8191, Lag order = 2, p-value = 0.2603
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$factor2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$factor2
## Dickey-Fuller = -2.6749, Lag order = 2, p-value = 0.3153
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$factor3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$factor3
## Dickey-Fuller = -2.8928, Lag order = 2, p-value = 0.2323
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(GDP_q$GDP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  GDP_q$GDP
## Dickey-Fuller = 1.5034, Lag order = 3, p-value = 0.99
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;はい。全部単位根もってました…。階差をとったのち、単位根検定を行います。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GDP_q &amp;lt;- GDP_q %&amp;gt;% mutate(growth.rate=(GDP/lag(GDP)-1)*100)
F_q &amp;lt;- F_q %&amp;gt;% mutate(f1.growth.rate=(factor1/lag(factor1)-1)*100,
                      f2.growth.rate=(factor2/lag(factor2)-1)*100,
                      f3.growth.rate=(factor3/lag(factor3)-1)*100)

adf.test(GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)]
## Dickey-Fuller = -0.31545, Lag order = 3, p-value = 0.9838
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)]
## Dickey-Fuller = -2.7762, Lag order = 2, p-value = 0.2767
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)]
## Dickey-Fuller = -2.6156, Lag order = 2, p-value = 0.3379
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)]
## Dickey-Fuller = -2.9893, Lag order = 2, p-value = 0.1955
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;factor1だけは5%有意水準で帰無仮説を棄却できない…。困りました。有意水準を10%ということにして、とりあえず階差で&lt;code&gt;OLS&lt;/code&gt;してみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset.q &amp;lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])
colnames(dataset.q) &amp;lt;- c(&amp;quot;publication&amp;quot;,&amp;quot;GDP&amp;quot;,&amp;quot;growth.rate&amp;quot;,&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;,&amp;quot;f1.growth.rate&amp;quot;,&amp;quot;f2.growth.rate&amp;quot;,&amp;quot;f3.growth.rate&amp;quot;)
test1 &amp;lt;- lm(growth.rate~f1.growth.rate + f2.growth.rate + f3.growth.rate,data=dataset.q)
summary(test1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = growth.rate ~ f1.growth.rate + f2.growth.rate + 
##     f3.growth.rate, data = dataset.q)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.6940 -0.2411  0.2041  0.4274  1.0904 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)     8.978e-02  4.588e-01   0.196    0.851
## f1.growth.rate -6.353e-03  1.375e-02  -0.462    0.660
## f2.growth.rate  6.155e-04  6.026e-03   0.102    0.922
## f3.growth.rate -9.249e-05  5.152e-04  -0.180    0.863
## 
## Residual standard error: 0.956 on 6 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.04055,    Adjusted R-squared:  -0.4392 
## F-statistic: 0.08452 on 3 and 6 DF,  p-value: 0.966&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;推計結果がわるくなりました…。予測値を計算し、実績値とプロットしてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out_of_sample1 &amp;lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成
test1.pred &amp;lt;- predict(test1, out_of_sample1, interval=&amp;quot;prediction&amp;quot;)
pred1.GDP.xts &amp;lt;- xts(cbind(test1.pred[,1],out_of_sample1$growth.rate),order.by = out_of_sample1$publication)
plot.zoo(pred1.GDP.xts,col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 予測値と実績値を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ん～、これはやり直しですね。今日はここまでで勘弁してください…。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>日次GDP推計に使用する経済統計を統計ダッシュボードから集めてみた</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post7/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post7/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;おはこんばんちわ。&lt;/p&gt;
&lt;p&gt;最近、競馬ばっかりやってましたが、そろそろ本業のマクロの方もやらないとなということで今回は日次GDP推計に使用するデータを総務省が公開している統計ダッシュボードから取ってきました。
そもそも、前の記事では四半期GDP速報の精度が低いことをモチベーションに高頻度データを用いてより精度の高い予測値をはじき出すモデルを作れないかというテーマで研究を進めていました。しかし、先行研究を進めていくうちに、どうやら大規模な経済指標を利用することで日次で四半期GDPの予測値を計算することが可能であることが判明しました。しかも、精度も良い(米国ですが)ということで、なんとかこの方向で研究を進めていけないかということになりました。&lt;/p&gt;
&lt;div id=&#34;先行研究と具体的にやりたいこと&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. 先行研究と具体的にやりたいこと&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dept.ku.edu/~empirics/Courses/Econ844/papers/Nowcasting%20GDP.pdf&#34;&gt;先行研究&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Giannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標をいくつかのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しました。なお、ファクターの計算にはカルマンスムージングを用いています(詳しい推計方法は論文&amp;amp;Technical Appendixを参照)。理論的な定式化は無いのですが、なかなか当たります。そもそも私がこの研究に興味を持ったのは、以下の本を立ち読みした際に参考文献として出てきたからで、いよいよ運用機関などでも使用され始めるのかと思い、やっておこうと思った次第です。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.amazon.co.jp/exec/obidos/ASIN/4532134811/hatena-blog-22/&#34;&gt;実践 金融データサイエンス 隠れた構造をあぶり出す6つのアプローチ&lt;/a&gt;
&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51KIE5GeV+L._SX350_BO1,204,203,200_.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;とりあえずはGiannoneの日本版をやろうかなと思っています。実はこの後に、ファクターモデルとDSGEを組み合わせたモデルがありましてそこまで発展させたいなーなんて思っておりますが。とにかく、ファクターを計算するための経済統計が必要ですので、今回はそれを集めてきたというのがこの記事の趣旨です。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;統計ダッシュボードからのデータの収集&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 統計ダッシュボードからのデータの収集&lt;/h2&gt;
&lt;p&gt;政府や日銀が公表しているデータの一部は統計ダッシュボードから落とすことができます。これは総務省統計局が提供しているもので、これまで利用しにくかった経済統計をより身近に使用してもらおうというのが一応のコンセプトとなっています。似たものに総務省統計局が提供しているestatがありますが、日銀の公表データがなかったり、メールアドレスの登録が必要だったりと非常に使い勝手が悪いです(個人的感想)。ただ、estatにはestatapiというRパッケージがあり、データを整形するのは比較的容易であると言えます。今回、統計ダッシュボードを選択した理由はそうは言っても日銀のデータがないのはダメだろうという理由で、データの整形に関しては関数を組みました。
そもそも統計ダッシュボードは経済統計をグラフなどで見て楽しむ？ものですが、私のような研究をしたい者を対象にAPIを提供してくれています。取得できるデータは大きく分けて6つあります。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;20180714160029.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;やり方は簡単で、ベースのurlと欲しい統計のIDをGET関数で渡すことによって、データを取得することができます。公式にも以下のように書かれています。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;基本的な使い方としては、まず①「統計メタ情報（系列）取得」で取得したいデータの[系列コード]を検索し、 その後⑥「統計データ取得」で[系列コード]を検索条件に指定し、その系列の情報を取得します。
（②③④⑤は補助的な情報として独立して取得できるようにしています。データのみ必要な場合は当該機能は不要です。）
具体的な使い方は、以下の「WebAPIの詳細仕様」に記載する[ベースURL]に検索条件となる[パラメータ]を“&amp;amp;”で連結し、HTTPリクエスト（GET）を送信することで目的のデータを取得できます。
各パラメータは「パラメータ名=値」のように名称と値を’=‘で結合し、複数のパラメータを指定する場合は「パラメータ名=値&amp;amp;パラメータ名=値&amp;amp;…」のようにそれぞれのパラメータ指定を’&amp;amp;’で結合してください。
また、パラメータ値は必ずURLエンコード(文字コードUTF-8)してから結合してください。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今回も以下の文献を参考にデータを取ってきたいと思います。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/486354216X&#34;&gt;Rによるスクレイピング入門&lt;/a&gt;
&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51ZBnu8oSvL._SX350_BO1,204,203,200_.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;まず、最初にこのAPIからデータを取得し、得られた結果を分析しやすいように整形する関数を定義したいと思います。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(httr)
library(estatapi)
library(dplyr)
library(XML)
library(stringr)
library(xts)
library(GGally)
library(ggplot2)
library(seasonal)
library(dlm)
library(vars)
library(MASS)

# 関数を定義
get_dashboard &amp;lt;- function(ID){
  base_url &amp;lt;- &amp;quot;https://dashboard.e-stat.go.jp/api/1.0/JsonStat/getData?&amp;quot;
  res &amp;lt;- GET(
    url = base_url,
    query = list(
      IndicatorCode=ID
    )
  )
  result &amp;lt;- content(res)
  x &amp;lt;- result$link$item[[1]]$value
  x &amp;lt;- t(do.call(&amp;quot;data.frame&amp;quot;,x))
  date_x &amp;lt;- result$link$item[[1]]$dimension$Time$category$label
  date_x &amp;lt;- t(do.call(&amp;quot;data.frame&amp;quot;,date_x))
  date_x &amp;lt;- str_replace_all(date_x, pattern=&amp;quot;年&amp;quot;, replacement=&amp;quot;/&amp;quot;)
  date_x &amp;lt;- str_replace_all(date_x, pattern=&amp;quot;月&amp;quot;, replacement=&amp;quot;&amp;quot;)
  date_x &amp;lt;- as.Date(gsub(&amp;quot;([0-9]+)/([0-9]+)&amp;quot;, &amp;quot;\\1/\\2/1&amp;quot;, date_x))
  date_x &amp;lt;- as.Date(date_x, format = &amp;quot;%m/%d/%Y&amp;quot;)
  date_x &amp;lt;- as.numeric(date_x)
  date_x &amp;lt;- as.Date(date_x, origin=&amp;quot;1970-01-01&amp;quot;)
  #x &amp;lt;- cbind(x,date_x)
  x &amp;lt;- data.frame(x)
  x[,1] &amp;lt;- as.character(x[,1])%&amp;gt;%as.numeric(x[,1])
  colnames(x) &amp;lt;- c(result$link$item[[1]]$label)
  x &amp;lt;- x %&amp;gt;% mutate(&amp;quot;publication&amp;quot; = date_x)
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;まずベースのurlを定義しています。今回はデータが欲しいので⑥統計データのベースurlを使用します（
&lt;a href=&#34;https://dashboard.e-stat.go.jp/static/api&#34;&gt;参考&lt;/a&gt;）。次にベースurlと統計ID（IndicatorCode）を&lt;code&gt;GET&lt;/code&gt;関数で渡し、結果を取得しています。統計IDについてはエクセルファイルで公開されています。得られた結果の中身（リスト形式）をresultに格納し、リストの深層にある原数値データ（value）をxに格納します。原数値データもリスト形式なので、それを&lt;code&gt;do.call&lt;/code&gt;でデータフレームに変換しています。次に、データ日付を取得します。resultの中を深くたどるとTime→category→labelというデータがあり、そこに日付データが保存されているので、それをdate_xに格納し、同じようにデータフレームへ変換します。データの仕様上、日付は「yyyy年mm月」になっていますが、これだと&lt;code&gt;R&lt;/code&gt;は日付データとして読み取ってくれないので、&lt;code&gt;str_replace_all&lt;/code&gt;等で変換したのち、&lt;code&gt;Date&lt;/code&gt;型に変換しています。列名にデータ名（result→link→item[[1]]→label）をつけ、データ日付をxに追加したら完成です。
そのほか、&lt;code&gt;data_connect&lt;/code&gt;という関数も定義しています。これはデータ系列によれば、たとえば推計方法の変更などで1980年～2005年の系列と2003年～2018年までの系列の2系列があるようなデータも存在し、この2系列を接続するための関数です。これは単純に接続しているだけなので、説明は省略します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_connect &amp;lt;- function(x){
  a &amp;lt;- min(which(x[,ncol(x)] != &amp;quot;NA&amp;quot;))
  b &amp;lt;- x[a,ncol(x)]/x[a,1]
  c &amp;lt;- x[1:a-1,1]*b
  return(c)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、実際にデータを取得していきます。今回取得するデータは月次データとなっています。これは統計dashboardが月次以下のデータがとれないからです。なので、例えば日経平均などは月末の終値を引っ張っています。ただし、GDPは四半期データとなっています。さきほど定義したget_dashboardの使用方法は簡単で、引数に統計ダッシュボードで公開されている統計IDを入力するだけでデータが取れます。今回使用するデータを以下の表にまとめました。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;20180714212330.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# データを取得
Nikkei &amp;lt;- get_dashboard(&amp;quot;0702020501000010010&amp;quot;)
callrate &amp;lt;- get_dashboard(&amp;quot;0702020300000010010&amp;quot;)
TOPIX &amp;lt;- get_dashboard(&amp;quot;0702020590000090010&amp;quot;)
kikai &amp;lt;- get_dashboard(&amp;quot;0701030000000010010&amp;quot;)
kigyo.bukka &amp;lt;- get_dashboard(&amp;quot;0703040300000090010&amp;quot;)
money.stock1 &amp;lt;- get_dashboard(&amp;quot;0702010201000010030&amp;quot;)
money.stock2 &amp;lt;- get_dashboard(&amp;quot;0702010202000010030&amp;quot;)
money.stock &amp;lt;- dplyr::full_join(money.stock1,money.stock2,by=&amp;quot;publication&amp;quot;)
c &amp;lt;- data_connect(money.stock)
a &amp;lt;- min(which(money.stock[,ncol(money.stock)] != &amp;quot;NA&amp;quot;))
money.stock[1:a-1,ncol(money.stock)] &amp;lt;- c
money.stock &amp;lt;- money.stock[,c(2,3)]
cpi &amp;lt;- get_dashboard(&amp;quot;0703010401010090010&amp;quot;)
export.price &amp;lt;- get_dashboard(&amp;quot;0703050301000090010&amp;quot;)
import.price &amp;lt;- get_dashboard(&amp;quot;0703060301000090010&amp;quot;)
import.price$`輸出物価指数（総平均）（円ベース）2015年基準` &amp;lt;- NULL
public.expenditure1 &amp;lt;- get_dashboard(&amp;quot;0802020200000010010&amp;quot;)
public.expenditure2 &amp;lt;- get_dashboard(&amp;quot;0802020201000010010&amp;quot;)
public.expenditure &amp;lt;- dplyr::full_join(public.expenditure1,public.expenditure2,by=&amp;quot;publication&amp;quot;)
c &amp;lt;- data_connect(public.expenditure)
a &amp;lt;- min(which(public.expenditure[,ncol(public.expenditure)] != &amp;quot;NA&amp;quot;))
public.expenditure[1:a-1,ncol(public.expenditure)] &amp;lt;- c
public.expenditure &amp;lt;- public.expenditure[,c(2,3)]
export.service &amp;lt;- get_dashboard(&amp;quot;1601010101000010010&amp;quot;)
working.population &amp;lt;- get_dashboard(&amp;quot;0201010010000010020&amp;quot;)
yukoukyuujinn &amp;lt;- get_dashboard(&amp;quot;0301020001000010010&amp;quot;)
hours_worked &amp;lt;- get_dashboard(&amp;quot;0302010000000010000&amp;quot;)
nominal.wage &amp;lt;- get_dashboard(&amp;quot;0302020000000010000&amp;quot;) 
iip &amp;lt;- get_dashboard(&amp;quot;0502070101000090010&amp;quot;)
shukka.shisu &amp;lt;- get_dashboard(&amp;quot;0502070102000090010&amp;quot;)
zaiko.shisu &amp;lt;- get_dashboard(&amp;quot;0502070103000090010&amp;quot;)
sanji.sangyo &amp;lt;- get_dashboard(&amp;quot;0603100100000090010&amp;quot;)
retail.sells &amp;lt;- get_dashboard(&amp;quot;0601010201010010000&amp;quot;)
GDP1 &amp;lt;- get_dashboard(&amp;quot;0705020101000010000&amp;quot;)
GDP2 &amp;lt;- get_dashboard(&amp;quot;0705020301000010000&amp;quot;)
GDP &amp;lt;- dplyr::full_join(GDP1,GDP2,by=&amp;quot;publication&amp;quot;)
c &amp;lt;- data_connect(GDP)
a &amp;lt;- min(which(GDP[,ncol(GDP)] != &amp;quot;NA&amp;quot;))
GDP[1:a-1,ncol(GDP)] &amp;lt;- c
GDP &amp;lt;- GDP[,c(2,3)]
yen &amp;lt;- get_dashboard(&amp;quot;0702020401000010010&amp;quot;)
household.consumption &amp;lt;- get_dashboard(&amp;quot;0704010101000010001&amp;quot;)
JGB10y &amp;lt;- get_dashboard(&amp;quot;0702020300000010020&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;今取得したデータは原数値系列のデータが多いので、それらは季節調整をかけます。なぜ季節調整済みのデータを取得しないのかというとそれらのデータは何故か極端にサンプル期間が短くなってしまうからです。ここらへんは使い勝手が悪いです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 季節調整をかける
Sys.setenv(X13_PATH = &amp;quot;C:\\Program Files\\WinX13\\x13as&amp;quot;)
checkX13()
seasoning &amp;lt;- function(data,i,start.y,start.m){
  timeseries &amp;lt;- ts(data[,i],frequency = 12,start=c(start.y,start.m))
  m &amp;lt;- seas(timeseries)
  summary(m$data)
  return(m$series$s11)
}
k &amp;lt;- seasoning(kikai,1,2005,4)
kikai$`機械受注額（船舶・電力を除く民需）` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(kigyo.bukka,1,1960,1)
kigyo.bukka$`国内企業物価指数（総平均）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(cpi,1,1970,1)
cpi$`消費者物価指数（生鮮食品を除く総合）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(export.price,1,1960,1)
export.price$`輸出物価指数（総平均）（円ベース）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(import.price,1,1960,1)
import.price$`輸入物価指数（総平均）（円ベース）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(public.expenditure,2,2004,4)
public.expenditure$公共工事受注額 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(export.service,1,1996,1)
export.service$`貿易・サービス収支` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(yukoukyuujinn,1,1963,1)
yukoukyuujinn$有効求人倍率 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(hours_worked,1,1990,1)
hours_worked$総実労働時間 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(nominal.wage,1,1990,1)
nominal.wage$現金給与総額 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(iip,1,1978,1)
iip$`鉱工業生産指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(shukka.shisu,1,1990,1)
shukka.shisu$`鉱工業出荷指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(zaiko.shisu,1,1990,1)
zaiko.shisu$`鉱工業在庫指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(sanji.sangyo,1,1988,1)
sanji.sangyo$`第３次産業活動指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(retail.sells,1,1980,1)
retail.sells$`小売業販売額（名目）` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(household.consumption,1,2010,1)
household.consumption$`二人以上の世帯　消費支出（除く住居等）` &amp;lt;- as.numeric(k)
GDP.ts &amp;lt;- ts(GDP[,2],frequency = 4,start=c(1980,1))
m &amp;lt;- seas(GDP.ts)
GDP$`国内総生産（支出側）（実質）2011年基準` &amp;lt;- m$series$s11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ここでは詳しく季節調整のかけ方は説明しません。x13arimaを使用しています。上述のコードを回す際はx13arimaがインストールされている必要があります。以下の記事を参考にしてください。&lt;/p&gt;
&lt;p&gt;[&lt;a href=&#34;http://sinhrks.hatenablog.com/entry/2014/11/09/003632&#34; class=&#34;uri&#34;&gt;http://sinhrks.hatenablog.com/entry/2014/11/09/003632&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;では、データ日付を基準に落としてきたデータを結合し、データセットを作成します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# データセットに結合
dataset &amp;lt;- dplyr::full_join(kigyo.bukka,callrate,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,kikai,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,Nikkei,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,money.stock,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,cpi,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,export.price,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,import.price,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,public.expenditure,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,export.service,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,working.population,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,yukoukyuujinn,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,hours_worked,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,nominal.wage,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,iip,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,shukka.shisu,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,zaiko.shisu,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,sanji.sangyo,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,retail.sells,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,yen,by=&amp;quot;publication&amp;quot;)
colnames(dataset) &amp;lt;- c(&amp;quot;DCGPI&amp;quot;,&amp;quot;publication&amp;quot;,&amp;quot;callrate&amp;quot;,&amp;quot;Machinery_Orders&amp;quot;,
                       &amp;quot;Nikkei225&amp;quot;,&amp;quot;money_stock&amp;quot;,&amp;quot;CPI&amp;quot;,&amp;quot;export_price&amp;quot;,
                       &amp;quot;import_price&amp;quot;,&amp;quot;public_works_order&amp;quot;,
                       &amp;quot;trade_service&amp;quot;,&amp;quot;working_population&amp;quot;,
                       &amp;quot;active_opening_ratio&amp;quot;,&amp;quot;hours_worked&amp;quot;,
                       &amp;quot;wage&amp;quot;,&amp;quot;iip_production&amp;quot;,&amp;quot;iip_shipment&amp;quot;,&amp;quot;iip_inventory&amp;quot;,
                       &amp;quot;ITIA&amp;quot;,&amp;quot;retail_sales&amp;quot;,&amp;quot;yen&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最後に列名をつけています。datasetはそれぞれのデータの公表開始時期が異なるために大量の&lt;code&gt;NA&lt;/code&gt;を含むデータフレームとなっているので、&lt;code&gt;NA&lt;/code&gt;を削除するために最もデータの開始時期が遅い機械受注統計に合わせてデータセットを再構築します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- min(which(dataset$Machinery_Orders != &amp;quot;NA&amp;quot;))
dataset1 &amp;lt;- dataset[a:nrow(dataset),]
dataset1 &amp;lt;- na.omit(dataset1)
rownames(dataset1) &amp;lt;- dataset1$publication
dataset1 &amp;lt;- dataset1[,-2]
dataset1.xts &amp;lt;- xts(dataset1,order.by = as.Date(rownames(dataset1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これでとりあえずデータの収集は終わりました。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;得られたデータを主成分分析にかけてみる&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. 得られたデータを主成分分析にかけてみる&lt;/h2&gt;
&lt;p&gt;本格的な分析はまた今後にしたいのですが、データを集めるだけでは面白くないので、Gianonneらのように主成分分析を行いたいと思います。主成分分析をこれまでに学んだことのない方は以下を参考にしてください。個人的にはわかりやすいと思っています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://datachemeng.com/principalcomponentanalysis/&#34;&gt;主成分分析(Principal Component Analysis, PCA)～データセットの見える化・可視化といったらまずはこれ！～&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では主成分分析を実行してみます。&lt;code&gt;R&lt;/code&gt;では&lt;code&gt;princomp&lt;/code&gt;関数を使用することで非常に簡単に主成分分析を行うことができます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 主成分分析を実行
factor.pca &amp;lt;- princomp(~.,cor = TRUE,data = dataset1) # cor = TRUEでデータの基準化を自動で行ってくれる。
summary(factor.pca)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Importance of components:
##                           Comp.1    Comp.2    Comp.3     Comp.4     Comp.5
## Standard deviation     3.2538097 1.8606047 1.5142917 1.05061250 0.88155352
## Proportion of Variance 0.5293639 0.1730925 0.1146540 0.05518933 0.03885683
## Cumulative Proportion  0.5293639 0.7024563 0.8171103 0.87229965 0.91115648
##                           Comp.6     Comp.7     Comp.8      Comp.9     Comp.10
## Standard deviation     0.7504599 0.63476742 0.48794520 0.413374289 0.358909911
## Proportion of Variance 0.0281595 0.02014648 0.01190453 0.008543915 0.006440816
## Cumulative Proportion  0.9393160 0.95946246 0.97136699 0.979910904 0.986351721
##                            Comp.11     Comp.12     Comp.13      Comp.14
## Standard deviation     0.296125594 0.254294037 0.233119328 0.1394055697
## Proportion of Variance 0.004384518 0.003233273 0.002717231 0.0009716956
## Cumulative Proportion  0.990736239 0.993969512 0.996686743 0.9976584386
##                             Comp.15      Comp.16      Comp.17     Comp.18
## Standard deviation     0.1356026290 0.1104356585 0.0861468897 0.070612034
## Proportion of Variance 0.0009194036 0.0006098017 0.0003710643 0.000249303
## Cumulative Proportion  0.9985778423 0.9991876440 0.9995587083 0.999808011
##                            Comp.19      Comp.20
## Standard deviation     0.053565104 3.115371e-02
## Proportion of Variance 0.000143461 4.852767e-05
## Cumulative Proportion  0.999951472 1.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;screeplot(factor.pca)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc &amp;lt;- predict(factor.pca,dataset1)[,1:3] # 主成分を計算
pc.xts &amp;lt;- xts(pc,order.by = as.Date(rownames(dataset1)))
plot.zoo(pc.xts,col=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;purple&amp;quot;,&amp;quot;yellow&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 主成分を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;第3主成分まででデータの約80％が説明できる結果を得たので、第3主成分までのプロットをお見せします。第1主成分（赤）はリーマンショックや東日本大震災、消費税増税のあたりで急上昇しています。ゆえに経済全体のリスクセンチメントを表しているのではないかと思っています。第2主成分（青）と第3主成分（緑）はリーマンショックのあたりで大きく落ち込んでいることは共通していますが2015年～現在の動きが大きく異なっています。また、第2主成分（青）はサンプル期間を通して過去トレンドを持つことから日本経済の潜在能力のようなものを表しているのではないでしょうか（そうするとリーマンショックまで上昇傾向にあることが疑問なのですが）。第3主成分（緑）はいまだ解読不能です（物価＆為替動向を表しているのではないかと思っています）。とりあえず今日はこれまで。次回はGianonne et. al.(2008)の日本版の再現を行いたいと思います。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>はじめまして</title>
      <link>https://ayatoashihara.github.io/myblog_jp/post/post4/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      <guid>https://ayatoashihara.github.io/myblog_jp/post/post4/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;index_files/accessible-code-block/empty-anchor.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;どうもはじめまして。
東京にある資産運用会社に勤める新卒1年目の葦原彩人と申します。&lt;/p&gt;
&lt;p&gt;簡単に自己紹介をしたいと思います。
大学院卒の24歳です。
専攻はマクロ経済学で特にDSGEモデル、状態空間モデル、カルマンフィルタ、ベイズ推定、MCMCなんかをやっていました。
指導教官の研究サポートとして自然言語処理をやったこともあります。
もともと学者志望でしたが金銭的な問題で就職することになりました。
今は営業サポートの下働きをしています。&lt;/p&gt;
&lt;p&gt;このブログは研究への興味関心を捨てることができない私が趣味として研究を進めていく際の備忘録という位置付けになるのだと思います。&lt;/p&gt;
&lt;p&gt;現在進めている研究は以下の２つです。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;馬券版ファクターモデル&lt;/li&gt;
&lt;li&gt;四半期GDP予測モデル&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1は、馬券市場が株式市場よりも投機的に魅力のある市場であるという観点から、回収率100%超えを目指す馬券ポートフォリオを構築するモデルを作成できないかというものです。株式にはファーマフレンチのようなファクターモデルがありますが、それを応用して馬券版ファクターモデルなるものを作れないかと思っています。&lt;/p&gt;
&lt;p&gt;2は最近の四半期GDP速報の精度が低いという問題意識から、精度の高い予測モデルを新たに構築できないかというものです。特にこれまでのようなマクロ経済理論に基づいたモデルではなく、機械学習を用い、通常マクロ経済学の研究で使用しないようなデータを織り込んだモデルを作成する方向で研究を進めていきたいと思っています。&lt;/p&gt;
&lt;p&gt;まだ、研究は始めたばかりなのですが、
興味を持ってもらえるように頑張りたいと思います…
では、最初はこの辺で。&lt;/p&gt;
&lt;p&gt;よろしくお願いします。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
